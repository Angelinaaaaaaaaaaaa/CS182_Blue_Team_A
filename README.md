```md
# CS182 Blue Team â€” Special Participation A Archive

A web application for exploring, analyzing, and visualizing CS182 **Special Participation A** submissions (students test LLMs on homework and report what worked / what failed). I focus on (1) **evidence-based comparisons** grounded in actual student text, (2) **deterministic analytics** that are fully reproducible, and (3) a **clean, mobile-friendly UI** that makes the archive usable.

---

## ğŸ“Š Project Overview

This archive collects and analyzes **169 Special Participation A posts** from Ed Discussion. Students tested many AI models (ChatGPT, Claude, Gemini, DeepSeek, etc.) on homework assignments and documented performance, failure modes, and one-shot capability.

### Key Metrics
- **169 posts** from unique student contributors  
- **50+ AI models** (canonical naming + typo handling)  
- **13 homeworks** covered (HW0â€“HW12)  
- **Deterministic analytics** by default (same input â†’ same output)  
- **Mobile-optimized** responsive design  

---

## ğŸŒŸ What I did beyond the baseline

### 1) Canonical Model Detection (50+ patterns)
I normalized messy, real-world model names into canonical labels (with a â€œmost-specific-firstâ€ ordering) so analytics donâ€™t split the same model across variants.

- Added newer model variants: `GPT-5-Thinking`, `GPT-5.1-Thinking`, `GPT-5.1`, `GPT-5`, `Grok`, `Kimi-K2`, `Perplexity-Sonar`, `DeepSeek-v3.2`, `DeepSeek-v3`, `Gemma`, `GPT-OSS`, etc.
- Typo handling:
  - `HWK 8` â†’ `HW8`
  - `HW02` â†’ `HW2` (leading zeros removed)

### 2) Evidence-based Model Comparison (real quotes â†’ structured pros/cons)
I redesigned the dashboard so **Model Comparison appears first** and is grounded in **actual evidence extracted from posts**, not generic â€œModel X is goodâ€ claims.

Each model card includes:
- Separate **Strengths vs. Weaknesses** sections (clear visual separation)
- **Homework tags** showing where the model was tested
- Focus on **most-tested** models to keep the view informative, not noisy

### 3) Deterministic Analytics as the default
By default, all analytics are reproducible:
- TF-IDF-based topic signals (with enhanced stopword filtering)
- Heuristic extraction of strength/weakness evidence from student text
- HWÃ—Model coverage heatmap + drill-down
- Representative post selection for each HWÃ—Model

No AI is required for the baseline pipeline.

### 4) Optional AI â€œone-linersâ€ (OpenAI API key, strictly additive)
I added an **optional** feature where the UI can show a **single-sentence pro/con per model** using an OpenAI API key.

Important constraints:
- Default analytics remain deterministic.
- AI is used only to summarize **already-extracted evidence** into short one-liners (and never replaces the evidence itself).
- If no evidence is available, the UI falls back to â€œInsufficient evidence in posts.â€

### 5) Mobile Optimization
I improved responsive design for all screen sizes:
- Single-column layouts on mobile
- Reduced padding / font sizes
- Horizontal scrolling for dense tables (heatmap)
- Stacked nav tabs and mobile-friendly search

---

## âœ… Key Fix: content-rich dataset (so evidence extraction works)

Early versions of the dataset used settled metadata but had `content=""`, so evidence extraction returned empty strengths/weaknesses.

The current pipeline uses:
- A manually settled CSV (`special_participation_a_settled.csv`) for **clean model/HW labels**
- A merge step that fetches **full thread content** from Ed by thread id and attaches it to each row

This is critical: strengths/weaknesses + TF-IDF topics require the real post text.

---

## ğŸ“ Project Structure

```

cs182_blue_team/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ scraper.py
â”‚   â”œâ”€â”€ analytics.py
â”‚   â”œâ”€â”€ advanced_analytics.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ special_participation_a_settled.csv     # Manual settled metadata (169 posts)
â”‚   â”‚   â”œâ”€â”€ merge_settled_with_content.py           # Merge settled CSV + fetch full content from Ed
â”‚   â”‚   â”œâ”€â”€ special_participation_a.json            # Final JSON (includes content)
â”‚   â”‚   â”œâ”€â”€ special_participation_a.csv
â”‚   â”‚   â”œâ”€â”€ analytics.json
â”‚   â”‚   â””â”€â”€ advanced_analytics.json
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx        # Model comparison at top + charts
â”‚   â”‚   â”‚   â”œâ”€â”€ Browse.jsx           # Search + filters
â”‚   â”‚   â”‚   â”œâ”€â”€ Analytics.jsx        # Timeline + coverage matrix
â”‚   â”‚   â”‚   â”œâ”€â”€ Insights.jsx         # HWÃ—Model heatmap drill-down + representative posts
â”‚   â”‚   â”‚   â””â”€â”€ Compare.jsx          # Pairwise comparisons
â”‚   â”‚   â”œâ”€â”€ styles/
â”‚   â”‚   â”‚   â”œâ”€â”€ App.css
â”‚   â”‚   â”‚   â””â”€â”€ index.css
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ public/data/                 # Data files copied from backend outputs
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_pipeline.sh
â”œâ”€â”€ FINAL_IMPROVEMENTS.md
â”œâ”€â”€ MODEL_INSIGHTS_UPDATE.md
â”œâ”€â”€ README.md
â””â”€â”€ .env.example

````

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 18+
- **Ed API token** (required if you want content fetching)
- **OpenAI API key** (optional; for one-line pro/con summaries)

### Environment variables

Create `.env` in the project root:

```env
ED_API_TOKEN=your_ed_api_token_here
OPENAI_API_KEY=your_openai_api_key_here   # optional
````

Note: `.env` is gitignored.

---

## ğŸ§ª Option A (Recommended): settled metadata + fetch full post content

This produces a dataset that includes real post text (required for evidence extraction).

```bash
# 1) Install backend deps
cd backend
pip install -r requirements.txt

# 2) Merge settled CSV with full content from Ed
python3 data/merge_settled_with_content.py

# 3) Run analytics
python3 advanced_analytics.py
python3 analytics.py

# 4) Copy outputs to frontend
cd ..
mkdir -p frontend/public/data
cp backend/data/special_participation_a.json frontend/public/data/
cp backend/data/special_participation_a.csv frontend/public/data/
cp backend/data/analytics.json frontend/public/data/
cp backend/data/advanced_analytics.json frontend/public/data/

# 5) Start frontend
cd frontend
npm install
npm run dev
```

Visit: `http://localhost:5173`

---

## ğŸ§¾ Option B: run the full pipeline script

```bash
cp .env.example .env
# fill in ED_API_TOKEN (and optionally OPENAI_API_KEY)
./scripts/run_pipeline.sh

cd frontend
npm install
npm run dev
```

---

## ğŸ“Š Data Pipeline

### 1) Settled metadata (manual)

`backend/data/special_participation_a_settled.csv`

* Ensures no â€œUnknownâ€ model labels
* Normalizes homework labels
* Fixes edge cases / typos

### 2) Merge with Ed content

`backend/data/merge_settled_with_content.py`

* Reads settled CSV
* Fetches thread detail by id from Ed
* Attaches full post text (and optionally replies)
* Outputs `special_participation_a.json` with non-empty `content`

### 3) Advanced analytics

`backend/advanced_analytics.py`

* TF-IDF topic extraction
* Strength/weakness evidence extraction
* HWÃ—Model heatmap
* Representative post selection

### 4) Basic analytics

`backend/analytics.py`

* Builds model-level summaries + dashboard-ready stats
* Filters out noise / stopwords
* Produces `analytics.json`

---

## ğŸ¨ Frontend Pages

### Dashboard

* Stats grid: posts, contributors, models, homeworks
* Model comparison at the top (evidence-based pros/cons + HW tags)
* Charts: top models, distribution, posts by homework
* Key findings at the bottom

### Insights

* HWÃ—Model heatmap with drill-down
* For a selected cell: top terms, strengths, weaknesses, representative posts

### Browse

* Fuzzy search across title/author/content
* Filters: model, homework, sort options
* Post cards with metadata

### Compare

* Side-by-side model comparisons (designed to reduce â€œinfo overloadâ€)

---

## ğŸ¤– Optional OpenAI one-liner summaries

If `OPENAI_API_KEY` is set, the UI can display a short one-line pro/con for each model.

Design principles:

* AI is additive: it summarizes already-extracted evidence.
* Evidence is always shown and remains the source of truth.
* Strict JSON parsing + fallback behavior (â€œInsufficient evidenceâ€).

---

## ğŸš¢ Deployment

### Vercel

```bash
cd frontend
npm run build
vercel
```

---

## ğŸ“„ License

Created for CS182 coursework.

---

Built for CS182 | 169 posts | 50+ models | Evidence-based comparisons | Deterministic by default | Mobile-optimized

```
::contentReference[oaicite:0]{index=0}
```
