{
  "generated_at": "2025-12-21T19:07:03.604620",
  "hw_model_analysis": {
    "HW4": {
      "ChatGPT": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "used",
            "score": 0.0488
          },
          {
            "term": "pro",
            "score": 0.0488
          },
          {
            "term": "one",
            "score": 0.0488
          },
          {
            "term": "problem",
            "score": 0.0488
          },
          {
            "term": "chatgpt",
            "score": 0.0244
          },
          {
            "term": "non",
            "score": 0.0244
          },
          {
            "term": "coding",
            "score": 0.0244
          },
          {
            "term": "parts",
            "score": 0.0244
          },
          {
            "term": "summary",
            "score": 0.0244
          },
          {
            "term": "quite",
            "score": 0.0244
          },
          {
            "term": "good",
            "score": 0.0244
          },
          {
            "term": "shotting",
            "score": 0.0244
          },
          {
            "term": "problems",
            "score": 0.0244
          },
          {
            "term": "even",
            "score": 0.0244
          },
          {
            "term": "prompt",
            "score": 0.0244
          }
        ],
        "strengths": [
          "Summary: It was quite good at one-shotting all problems, even with just one prompt - except a numerical problem, for which it (incorrectly) used python code to generate a matrix"
        ],
        "weaknesses": [
          "Summary: It was quite good at one-shotting all problems, even with just one prompt - except a numerical problem, for which it (incorrectly) used python code to generate a matrix",
          "Further, another small issue was the reasoning time - it took 20+ minutes to get a response from the Pro model on this problem set"
        ],
        "representative_posts": [
          {
            "title": "Special participation A: ChatGPT 5.1 Thinking extended on HW 4",
            "author": "Abdelaziz Mohamed",
            "url": "https://edstem.org/us/courses/84647/discussion/7429445",
            "snippet": "..."
          },
          {
            "title": "Special Participation A: ChatGPT-5.1 Pro on HW4 Non-coding",
            "author": "Neel Kolhe",
            "url": "https://edstem.org/us/courses/84647/discussion/7449252",
            "snippet": "I used ChatGPT 5 - Pro on HW 4(all non-coding parts). \n\nSummary: It was quite good at one-shotting all problems, even with just one prompt - except a numerical problem, for which it (incorrectly) used..."
          }
        ]
      },
      "Claude Sonnet": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "model",
            "score": 0.037
          },
          {
            "term": "part",
            "score": 0.0267
          },
          {
            "term": "problem",
            "score": 0.0206
          },
          {
            "term": "convolution",
            "score": 0.0165
          },
          {
            "term": "error",
            "score": 0.0144
          },
          {
            "term": "claude",
            "score": 0.0123
          },
          {
            "term": "answer",
            "score": 0.0123
          },
          {
            "term": "correlation",
            "score": 0.0123
          },
          {
            "term": "edge",
            "score": 0.0123
          },
          {
            "term": "one",
            "score": 0.0103
          },
          {
            "term": "chat",
            "score": 0.0082
          },
          {
            "term": "multiple",
            "score": 0.0082
          },
          {
            "term": "format",
            "score": 0.0082
          },
          {
            "term": "attempt",
            "score": 0.0082
          },
          {
            "term": "flip",
            "score": 0.0082
          }
        ],
        "strengths": [
          "The model has strong conceptual understanding but struggles with notation conventions, sign errors in signal processing, and tracking how multiple scaling factors interact",
          "Detailed Findings by Problem\n\nProblem 1: Newton-Schulz Runtime (with minor prompting)\n\nWhat happened:\n\nThe model correctly identified the two dominant matrix multiplications and their complexity\n\nIssu",
          "Initial answer for part (c): Matrix of all +40\n\nCorrect answer: Matrix of all -40\n\nWhen I pointed out the sign error, the model realized it needed to flip the kernel h to h_flipped before computing",
          "8, 0]^T\n\nCorrect answer: g3 = [0, 0",
          "All parts were correct on the first attempt"
        ],
        "weaknesses": [
          "This post documents my observations and an analysis of where the model succeeded and failed",
          "Detailed Findings by Problem\n\nProblem 1: Newton-Schulz Runtime (with minor prompting)\n\nWhat happened:\n\nThe model correctly identified the two dominant matrix multiplications and their complexity\n\nIssu",
          "Problem 2: MuP at the Unit Scale\n\nThe model one-shotted most parts of this problem (a, b, c, f)",
          "Part d required a minor fix, while part e was a major struggle",
          "Part g was initially wrong, but was fixed correctly after fixing part e"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Sonnet 4.5 on Homework 4 (Written Problems)",
            "author": "Elizabeth Weaver",
            "url": "https://edstem.org/us/courses/84647/discussion/7445493",
            "snippet": "I engaged Claude Sonnet 4.5 on all written portions of Homework 4 (Problems 1, 2, 3, 4, and 7) to evaluate its ability to solve deep learning theory problems. I provided screenshots of the questions f..."
          }
        ]
      },
      "Unknown": {
        "post_count": 5,
        "top_terms": [
          {
            "term": "problem",
            "score": 0.0226
          },
          {
            "term": "questions",
            "score": 0.02
          },
          {
            "term": "question",
            "score": 0.0146
          },
          {
            "term": "one",
            "score": 0.0133
          },
          {
            "term": "grok",
            "score": 0.0107
          },
          {
            "term": "part",
            "score": 0.0107
          },
          {
            "term": "problems",
            "score": 0.0107
          },
          {
            "term": "deepseek",
            "score": 0.0107
          },
          {
            "term": "homework",
            "score": 0.0093
          },
          {
            "term": "solution",
            "score": 0.0093
          },
          {
            "term": "answer",
            "score": 0.0093
          },
          {
            "term": "qwen",
            "score": 0.0093
          },
          {
            "term": "model",
            "score": 0.008
          },
          {
            "term": "its",
            "score": 0.008
          },
          {
            "term": "context",
            "score": 0.008
          }
        ],
        "strengths": [
          "Executive Summary:\n\nI used Grok to complete the written portion of Homework #4",
          "Grok was able to one-shot many parts of the problem, but sometimes required corrections to arrive at the correct solution",
          "Grok got the same answer as indicated by previous Special Participation A posts focusing on this problem set, and when I asked the model to evaluate whether the key’s current solution is reasonable, i",
          "5V)”) appeared to be accurate throughout",
          "In this experiment:\n\nDeepSeek solved four out of five questions correctly in a single attempt"
        ],
        "weaknesses": [
          "Grok was able to one-shot many parts of the problem, but sometimes required corrections to arrive at the correct solution",
          "As noted in previous posts, Problem #2 part e on the answer key potentially contains a mistake",
          "Grok got the same answer as indicated by previous Special Participation A posts focusing on this problem set, and when I asked the model to evaluate whether the key’s current solution is reasonable, i",
          "For some problems, such as Problem #3 part d and Problem #4 part f, Grok presents work towards one solution and then indicates that it has changed its mind, says the previous work is incorrect, and su",
          "I inputted all the problems, including problem 3 which contains a figure and Problems #3 and #4 which contain example matrices, into the model, and its ability to “read” these screenshots (using “Grok"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: GPT-5 HW4",
            "author": "Nyx Iskandar",
            "url": "https://edstem.org/us/courses/84647/discussion/7353572",
            "snippet": "Generally. GPT-5 generates accurate answers for conceptual and computation questions. There are some conventions that it chooses to use that we don't use in class, like Xavier initialization using 1/s..."
          },
          {
            "title": "Special Participation A: Qwen on HW4",
            "author": "Zach Pricz",
            "url": "https://edstem.org/us/courses/84647/discussion/7400839",
            "snippet": "For special participation A on HW4, I used Qwen and its Qwen3-Max model with thinking to solve the non coding problems on the homework (problems 1, 2, 3, 4, 7). \n\nI attempted this homework with Qwen 3..."
          },
          {
            "title": "Special Participation A: Grok on HW4",
            "author": "Elizabeth Polito",
            "url": "https://edstem.org/us/courses/84647/discussion/7405554",
            "snippet": "Executive Summary:\n\nI used Grok to complete the written portion of Homework #4. Since I do not have the paid tier, I used Grok fast. While this is not the top model in the Grok line, it is interesting..."
          }
        ]
      },
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0494
          },
          {
            "term": "one",
            "score": 0.0329
          },
          {
            "term": "pro",
            "score": 0.0247
          },
          {
            "term": "answer",
            "score": 0.0247
          },
          {
            "term": "problems",
            "score": 0.0206
          },
          {
            "term": "made",
            "score": 0.0206
          },
          {
            "term": "mistake",
            "score": 0.0206
          },
          {
            "term": "correct",
            "score": 0.0206
          },
          {
            "term": "gave",
            "score": 0.0206
          },
          {
            "term": "solve",
            "score": 0.0165
          },
          {
            "term": "problem",
            "score": 0.0165
          },
          {
            "term": "its",
            "score": 0.0165
          },
          {
            "term": "solutions",
            "score": 0.0165
          },
          {
            "term": "used",
            "score": 0.0123
          },
          {
            "term": "homework",
            "score": 0.0123
          }
        ],
        "strengths": [
          "I began by giving Gemini the assignment and telling it that I was trying to evaluate how well it could solve the problems, and then went through the problems with it one by one",
          "Overall, it did a good job of solving the problems",
          "It only made one reasoning mistake, that wasn’t due to it misreading the question, in part 2g, but was able to correct itself after being told what step in the derivation it made a mistake on",
          "One notable part of the interaction was when it solved problem 2e",
          "Its answer differed from what was given in the solutions, even though it was correct as there seems to be a mistake in the solutions, so I tried prompting it to fix its answer, at which point it arriv"
        ],
        "weaknesses": [
          "All the mistakes it made, except one, were due to the fact that it read the problems in the homework wrong",
          "Oftentimes, these misreadings were very blatant, like when it just misread the entries of the matrices for problem 3",
          "One notable part of the interaction was when it solved problem 2e",
          "I split the homework pdf into a few pdfs, one with each problem, and gave those to Gemini sequentially",
          "I have no significant complaints about how Gemini did for any problem"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini on Homework 4",
            "author": "Jason Guo",
            "url": "https://edstem.org/us/courses/84647/discussion/7265693",
            "snippet": "Annotated Transcript:\n\nhttps://drive.google.com/file/d/1ZOIMXval6EtWYyoBE6H13fS0I7d58Fmd/view?usp=sharing\n\nFor this special participation, I used Gemini Pro 2.5 to solve the written portions of homewo..."
          },
          {
            "title": "Special Participation A: Gemini 3.0 Pro (Thinking) on HW4",
            "author": "Tiger Zhang",
            "url": "https://edstem.org/us/courses/84647/discussion/7428749",
            "snippet": "Executive summary:\n\nFollowing the release of Gemini 3.0 Pro, I wanted to use it to solve HW4 and see if there is a significant improvement from when Jason Guo used Gemini 2.5 Pro to solve it.\n\nI split..."
          }
        ]
      }
    },
    "HW1": {
      "ChatGPT": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "question",
            "score": 0.0416
          },
          {
            "term": "llm",
            "score": 0.016
          },
          {
            "term": "chatgpt",
            "score": 0.0128
          },
          {
            "term": "one",
            "score": 0.0128
          },
          {
            "term": "problem",
            "score": 0.0128
          },
          {
            "term": "answers",
            "score": 0.0112
          },
          {
            "term": "sometimes",
            "score": 0.0112
          },
          {
            "term": "correct",
            "score": 0.0112
          },
          {
            "term": "questions",
            "score": 0.0096
          },
          {
            "term": "though",
            "score": 0.0096
          },
          {
            "term": "its",
            "score": 0.0096
          },
          {
            "term": "answer",
            "score": 0.0096
          },
          {
            "term": "however",
            "score": 0.0096
          },
          {
            "term": "reasoning",
            "score": 0.0096
          },
          {
            "term": "homework",
            "score": 0.008
          }
        ],
        "strengths": [
          "\" and the LLM responded \"I'm glad you shared the problem set, but I can't directly solve every question for you or give a complete set of worked solutions, since this is a real course homework and tha",
          "The forms it decided to write the answers in were sometimes overly rewritten/simplified to a point that didn't provide any additional simplicity or information, but the answers were still correct",
          "Also for 2h it gave a different solution than the course homework solutions; however, I believe the LLM solution to be correct and the course solution might have a type; the question setup already con",
          "in 3f the takeaway of the question wasn't immediately clear to me, though the computations were correct",
          "Identities were correct"
        ],
        "weaknesses": [
          "It gave me some generic answers, showing it was inferring what could be in the notebook based on the setup of the question, but upon further probing it admitted to me that it cannot see its contents",
          "I thought this was slightly concerning, because I can imagine it would be a problem in a higher-stakes context if it makes up data as though it is analyzing real data",
          "3) When I first submitted the pdf, my prompt was \"Attached is a homework problem set from a deep learning class",
          "\" and the LLM responded \"I'm glad you shared the problem set, but I can't directly solve every question for you or give a complete set of worked solutions, since this is a real course homework and tha",
          "Here is a breakdown of my interactions with each question:\n\nQuestion 1: Added some additional logic referring to the homogeneous error dynamic"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT-5.1 Thinking on Homework 1",
            "author": "Jacqueline Thibault",
            "url": "https://edstem.org/us/courses/84647/discussion/7428374",
            "snippet": "I engaged `ChatGPT-5.1: Thinking` on Homework 1's non-coding parts. \n\nExecutive summary:\n\nThe LLM was able to one-shot all of the questions. I was thoroughly impressed by this, though it makes sense g..."
          },
          {
            "title": "Special Participation A: ChatGPT on HW1",
            "author": "Junya Tsuneishi",
            "url": "https://edstem.org/us/courses/84647/discussion/7219478",
            "snippet": "I used ChatGPT on HW1 no-cording parts(Special Participation A).\nI posted the results, my findings about them, and my summary on the attached pdf.\n\nThis is summary from the pdf.\nOverall, ChatGPT achie..."
          }
        ]
      },
      "Unknown": {
        "post_count": 5,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0186
          },
          {
            "term": "answer",
            "score": 0.0174
          },
          {
            "term": "problem",
            "score": 0.0162
          },
          {
            "term": "question",
            "score": 0.0151
          },
          {
            "term": "deepseek",
            "score": 0.0139
          },
          {
            "term": "problems",
            "score": 0.0116
          },
          {
            "term": "solution",
            "score": 0.0104
          },
          {
            "term": "homework",
            "score": 0.0081
          },
          {
            "term": "solve",
            "score": 0.0081
          },
          {
            "term": "kimi",
            "score": 0.007
          },
          {
            "term": "non",
            "score": 0.007
          },
          {
            "term": "step",
            "score": 0.007
          },
          {
            "term": "thinking",
            "score": 0.007
          },
          {
            "term": "however",
            "score": 0.007
          },
          {
            "term": "mistral",
            "score": 0.007
          }
        ],
        "strengths": [
          "Mathematical Rigor & Reasoning:\n\nUnlike some LLMs that skip steps or hallucinate intermediate lines to reach a \"known\" answer, Kimi k2 provided complete, step-by-step derivations",
          "It seems like the mathematical reasoning is good since the non-matrix computations are all working very well without any errors",
          "I think it also got better when I pointed out that there are some computational errors because after that there has not been any mistakes",
          "In fact, it was clear that for around 4 parts in the homework, the model does not have a sufficient \"understanding\" of linear algebra to reach the solution without essentially giving the model the ans",
          "In particular, a massive pitfall of the model is that it appears to not be able to parse PDF files with math very well, and the model repeatedly got the wrong mapping from problem numbers/letters to t"
        ],
        "weaknesses": [
          ", using the SVD to diagonalize the loss landscape) immediately upon being presented with the problem statement",
          "Mathematical Rigor & Reasoning:\n\nUnlike some LLMs that skip steps or hallucinate intermediate lines to reach a \"known\" answer, Kimi k2 provided complete, step-by-step derivations",
          "In instances where the problem allowed for multiple interpretations (e",
          "For example, it was able to correctly reason 3a however it failed to get 3b even though, from 3a to 3b, it is just simple matrix calculations",
          "I tried to ask Mistral to fix some of the mistakes  that it has made on the previous answer, but it seems to misunderstand my request and always go with the same incorrect question"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Deepseek v3.2 on HW1",
            "author": "Yubo Fan",
            "url": "https://edstem.org/us/courses/84647/discussion/7451410",
            "snippet": "Special Participation A: Deepseek v3.2 on HW1\nFor the Type A participation option, I interactively engaged with DeepSeek v3.2 to solve the written (non-coding) portions of Homework 1. Attached is the ..."
          },
          {
            "title": "Special Participation A: Mistral on HW 1",
            "author": "Minjune Kim",
            "url": "https://edstem.org/us/courses/84647/discussion/7386904",
            "snippet": "I have used Mistral to test on Hw 1. \n\nLink: https://chat.mistral.ai/chat/6ff004cd-66c9-49ef-92fb-19476f51402b\n\nSummary:\n\nIn general, it was able to get most of the answers without any mistakes. A lot..."
          },
          {
            "title": "Special Participation A: Gemma 3 on Homework 1",
            "author": "Siva Tanikonda",
            "url": "https://edstem.org/us/courses/84647/discussion/7451722",
            "snippet": "Hi,\n\nI tried to get the Gemma 3 (12 billion parameter) model to solve the non-coding portion of Homework 1. The transcript of my interactions are outlined in the PDF:\n\n(Note that a stylized export of ..."
          }
        ]
      },
      "Claude Sonnet": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0275
          },
          {
            "term": "overall",
            "score": 0.0183
          },
          {
            "term": "solutions",
            "score": 0.0183
          },
          {
            "term": "often",
            "score": 0.0183
          },
          {
            "term": "correct",
            "score": 0.0183
          },
          {
            "term": "conditions",
            "score": 0.0183
          },
          {
            "term": "convergence",
            "score": 0.0183
          },
          {
            "term": "made",
            "score": 0.0183
          },
          {
            "term": "incorrect",
            "score": 0.0183
          },
          {
            "term": "derivations",
            "score": 0.0183
          },
          {
            "term": "proofs",
            "score": 0.0183
          },
          {
            "term": "special",
            "score": 0.0092
          },
          {
            "term": "participation",
            "score": 0.0092
          },
          {
            "term": "used",
            "score": 0.0092
          },
          {
            "term": "claude",
            "score": 0.0092
          }
        ],
        "strengths": [
          "Overall, the model produced solutions that were often structurally correct, but it was not reliable",
          "The model occasionally solved subproblems correctly on the first try, especially when the math followed familiar patterns (e",
          "Overall, interacting with the LLM is useful for brainstorming structures of proofs or confirming intuition, but it is not capable of producing fully correct, rigorous solutions on its own"
        ],
        "weaknesses": [
          "However, it frequently made subtle mathematical mistakes like missing constants, incorrect simplifications, unjustified assumptions, or skipped derivations",
          "A recurring pattern was that the model sounded confident even when the reasoning was incomplete or incorrect"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Sonnet 4.5 on HW 1 Written Problems",
            "author": "Arjun Kohli",
            "url": "https://edstem.org/us/courses/84647/discussion/7450203",
            "snippet": "For this Special Participation A, I used Claude Sonnet 4.5 to work through all the non-coding parts of HW1. Overall, the model produced solutions that were often structurally correct, but it was not r..."
          }
        ]
      },
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0476
          },
          {
            "term": "problems",
            "score": 0.0265
          },
          {
            "term": "its",
            "score": 0.0212
          },
          {
            "term": "problem",
            "score": 0.0159
          },
          {
            "term": "step",
            "score": 0.0159
          },
          {
            "term": "llms",
            "score": 0.0159
          },
          {
            "term": "answer",
            "score": 0.0106
          },
          {
            "term": "https",
            "score": 0.0106
          },
          {
            "term": "google",
            "score": 0.0106
          },
          {
            "term": "com",
            "score": 0.0106
          },
          {
            "term": "able",
            "score": 0.0106
          },
          {
            "term": "one",
            "score": 0.0106
          },
          {
            "term": "sometimes",
            "score": 0.0106
          },
          {
            "term": "solve",
            "score": 0.0106
          },
          {
            "term": "previous",
            "score": 0.0106
          }
        ],
        "strengths": [
          "Gemini did a good job of justifying each step in problems that required many sequential equivalences",
          "At the end, I tried to see if Gemini would be able to add clarifications to its previous responses given the answer key, but it just confirmed that its answers were correct, and didn't yield any impro",
          "In previous experiences, I typically had to re-prompt multiple times before getting a coherent explanation of a notation or concept, but Gemini delivered these interpretations clearly on the first try"
        ],
        "weaknesses": [
          "The only issues were minor misinterpretations of the problem statement (notably, the interpretation of the error factor in 1b)",
          "One issue: Gemini tended to forget that it was solving problems from the provided document, sometimes coming up with its own problem to solve",
          "Based on past interactions with LLMs, my experience was that LLMs lack the ability to provide insight/ intuition into mathematical problems and tend to focus on just deriving answers (that are even fr",
          "This time, I explicitly prompted it to interpret each problem, produce the full solution, and show all step-by-step derivations"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 2.5 Flash on Homework 1",
            "author": "Diana Kohr",
            "url": "https://edstem.org/us/courses/84647/discussion/7427837",
            "snippet": "I used Gemini 2.5 Flash to answer HW 1 written problems. \n\nConversation: https://gemini.google.com/share/11b5f1b89778\n\nAnnotated: https://drive.google.com/file/d/1vGRWvLGliMGdQhvNDdYq0SKYC575tPdd/view..."
          },
          {
            "title": "Special Participation A: Gemini 3 Pro(Thinking) Homework 1",
            "author": "Yuri Lee",
            "url": "https://edstem.org/us/courses/84647/discussion/7450682",
            "snippet": "In this assignment, I attempted to use Gemini 3 Pro (in Thinking mode) to solve all the non-coding portions of HW1. Based on past interactions with LLMs, my experience was that LLMs lack the ability t..."
          }
        ]
      },
      "Gemini Pro": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0678
          },
          {
            "term": "pro",
            "score": 0.0339
          },
          {
            "term": "answer",
            "score": 0.0339
          },
          {
            "term": "homework",
            "score": 0.0339
          },
          {
            "term": "pdf",
            "score": 0.0339
          },
          {
            "term": "questions",
            "score": 0.0339
          },
          {
            "term": "asking",
            "score": 0.0339
          },
          {
            "term": "started",
            "score": 0.0339
          },
          {
            "term": "used",
            "score": 0.0169
          },
          {
            "term": "written",
            "score": 0.0169
          },
          {
            "term": "problems",
            "score": 0.0169
          },
          {
            "term": "conversation",
            "score": 0.0169
          },
          {
            "term": "https",
            "score": 0.0169
          },
          {
            "term": "google",
            "score": 0.0169
          },
          {
            "term": "com",
            "score": 0.0169
          }
        ],
        "strengths": [
          "com/share/f3019ef7b48e\n\nAnnotated: \n\n\nSummary: Gemini Pro initially had issues when asked to complete the entire homework when given a pdf file of the questions"
        ],
        "weaknesses": [
          "However, the real breakthrough came when I started copying and pasting problem text into gemini directly, rather than asking it to reference the Homework PDF"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini Pro on HW1 (Non-coding)",
            "author": "Garv Goswami",
            "url": "https://edstem.org/us/courses/84647/discussion/7428581",
            "snippet": "I used Gemini 3 Pro to answer HW 1 written problems.\n\nConversation: https://gemini.google.com/share/f3019ef7b48e\n\nAnnotated: \n\n\nSummary: Gemini Pro initially had issues when asked to complete the enti..."
          }
        ]
      }
    },
    "HW2": {
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0301
          },
          {
            "term": "gemini",
            "score": 0.0251
          },
          {
            "term": "answer",
            "score": 0.015
          },
          {
            "term": "answers",
            "score": 0.0125
          },
          {
            "term": "reasoning",
            "score": 0.0125
          },
          {
            "term": "step",
            "score": 0.0125
          },
          {
            "term": "one",
            "score": 0.01
          },
          {
            "term": "correct",
            "score": 0.01
          },
          {
            "term": "first",
            "score": 0.01
          },
          {
            "term": "interaction",
            "score": 0.0075
          },
          {
            "term": "deep",
            "score": 0.0075
          },
          {
            "term": "learning",
            "score": 0.0075
          },
          {
            "term": "distributed",
            "score": 0.0075
          },
          {
            "term": "shot",
            "score": 0.0075
          },
          {
            "term": "problems",
            "score": 0.0075
          }
        ],
        "strengths": [
          "Executive Summary: Interaction with Gemini on Deep Learning Theory\n\nModel Tested: Gemini 3 Pro\n\nDomain: Deep Learning Optimization & Distributed Training (CS 182/282 Context)\n\nOverall Performance: 100",
          "In every instance, the model provided the correct analytical solution and numerical answers on the first attempt (one-shot)",
          "This behavior mimics a cautious human student double-checking their work to ensure logical consistency, rather than a machine simply outputting a retrieved token sequence",
          "Conclusion\n\nGemini demonstrated graduate-level competency in deep learning theory, capable of handling multimodal inputs (LaTeX screenshots) and complex analytical derivations with perfect accuracy",
          "The interaction suggests that for well-defined theoretical problems, modern reasoning models can serve as reliable verification engines, provided the user monitors the \"reasoning track\" for the model'"
        ],
        "weaknesses": [
          "Optimizer Convergence: Analyzing convergence points for Vanilla SGD, Simplified Adam, and Feature Rescaling on a constrained linear regression problem",
          "Hallucinations & Accuracy\n\nHallucination Rate: 0%",
          "Then, I do the following steps:\n\nStep 1: Provide Gemini with one homework problem",
          "Step 3: If the answer was incorrect, provide hints to guide Gemini to fix the original answer, and repeat until the response was correct or it seems to have no chance of fixing it",
          "Although Gemini was able to correctly solve most of the questions in the first answer, it occasionally made logical or mathematical errors that led to incorrect final answers, even when the reasoning "
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 3 Pro on HW 2 Written Questions",
            "author": "Ijin Yu",
            "url": "https://edstem.org/us/courses/84647/discussion/7397166",
            "snippet": "Executive Summary: Interaction with Gemini on Deep Learning Theory\n\nModel Tested: Gemini 3 Pro\n\nDomain: Deep Learning Optimization & Distributed Training (CS 182/282 Context)\n\nOverall Performance: 100..."
          },
          {
            "title": "Special Participation A: Gemini 2.5 Flash on HW2",
            "author": "Ruizhe Song",
            "url": "https://edstem.org/us/courses/84647/discussion/7244375",
            "snippet": "I interactively engaged Gemini 2.5 Flash on the non-coding parts of Homework 2. Overall, the model was able to arrive at the correct answers in most cases, though several notable issues were observed...."
          }
        ]
      },
      "Unknown": {
        "post_count": 4,
        "top_terms": [
          {
            "term": "question",
            "score": 0.0191
          },
          {
            "term": "problem",
            "score": 0.0174
          },
          {
            "term": "one",
            "score": 0.0174
          },
          {
            "term": "homework",
            "score": 0.0122
          },
          {
            "term": "model",
            "score": 0.0104
          },
          {
            "term": "reasoning",
            "score": 0.0104
          },
          {
            "term": "step",
            "score": 0.0104
          },
          {
            "term": "kimi",
            "score": 0.0104
          },
          {
            "term": "able",
            "score": 0.0104
          },
          {
            "term": "deepseek",
            "score": 0.0087
          },
          {
            "term": "answer",
            "score": 0.0087
          },
          {
            "term": "well",
            "score": 0.0087
          },
          {
            "term": "mistral",
            "score": 0.0087
          },
          {
            "term": "non",
            "score": 0.0069
          },
          {
            "term": "coding",
            "score": 0.0069
          }
        ],
        "strengths": [
          "It arguably fails to notice one small detail in one of the subparts (see Q1 for more), but apart from that, all perfect",
          "Files & Links\n\nQuestion 1:\n\nQuestion 2:\n\nQuestion 5: Here, I looked at how well Kimi K2 could solve the written questions on Homework 2",
          "Overall, Kimi was able to handle the questions well with minimal nudges or corrections",
          "For my approach, I provided the model with the homework pdf, indicated to solve a problem step-by-step, and included that it was well-versed in deep learning and optimization",
          "However, once this was indicated, the model was able to revise and arrive at the correct solution immediately"
        ],
        "weaknesses": [
          "For every problem, my process consisted of highlighting the relevant subpart in its entirety on the homework PDF, then copy pasting into the text box",
          "No context was given except for those present in the problem originally, and no prompt optimization was performed (i",
          "For my approach, I provided the model with the homework pdf, indicated to solve a problem step-by-step, and included that it was well-versed in deep learning and optimization",
          "Analysis: Kimi was mostly able to one-shot the sub-parts for each of the problems with minimal hallucinations, aside from problem 1b",
          "In this scenario, the model committed to a hallucinated version of the problem where the L-infinity penalty term was not squared, and thus attempted to use alternate methods to solve the problem"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: GPT-5 (thinking) on HW2",
            "author": "Kevin Tseng",
            "url": "https://edstem.org/us/courses/84647/discussion/7424589",
            "snippet": "In this special participation, I interactively engage GPT-5 on the non-coding parts of Homework 2. My experience using it was boring and it one-shotted every question. I did not have to use any specia..."
          },
          {
            "title": "Special Participation A: Mistral on HW2",
            "author": "Xi Cheng",
            "url": "https://edstem.org/us/courses/84647/discussion/7266065",
            "snippet": "I tested Mistral on the non-coding parts of HW2\n\nChat history link: https://chat.mistral.ai/chat/678d9106-0d96-45c6-83e1-2c0ac7a7384a\n\nAnnotated Log: \n\nExecutive Summary:\n\nI found that it could one-sh..."
          },
          {
            "title": "Special Participation A: Kimi K2 on HW2",
            "author": "Rohan Gulati",
            "url": "https://edstem.org/us/courses/84647/discussion/7409772",
            "snippet": "Here, I looked at how well Kimi K2 could solve the written questions on Homework 2. Overall, Kimi was able to handle the questions well with minimal nudges or corrections. For my approach, I provided ..."
          }
        ]
      },
      "Gemini Pro": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0505
          },
          {
            "term": "correct",
            "score": 0.0404
          },
          {
            "term": "questions",
            "score": 0.0303
          },
          {
            "term": "one",
            "score": 0.0303
          },
          {
            "term": "problems",
            "score": 0.0202
          },
          {
            "term": "prompt",
            "score": 0.0202
          },
          {
            "term": "instead",
            "score": 0.0202
          },
          {
            "term": "doesn",
            "score": 0.0202
          },
          {
            "term": "answers",
            "score": 0.0202
          },
          {
            "term": "answer",
            "score": 0.0202
          },
          {
            "term": "key",
            "score": 0.0202
          },
          {
            "term": "even",
            "score": 0.0202
          },
          {
            "term": "particular",
            "score": 0.0202
          },
          {
            "term": "question",
            "score": 0.0202
          },
          {
            "term": "special",
            "score": 0.0101
          }
        ],
        "strengths": [
          "Gemini did it great as expected, all questions were correct on the first-shot prompt",
          "Summary: Gemini 3 is one of the models for math questions and it doesn't disappoint, every procedure was at least acceptable, understandable and most important correct (doesn't hallucinate with these ",
          "The answers were correct and also using different notations",
          "Explanations were correct, concise, and directly matched the answer key",
          "Even in places where the answer key did not explain (Q5 distributed training), Gemini had a clear way to arrive at the answers"
        ],
        "weaknesses": [
          "Summary: Gemini 3 is one of the models for math questions and it doesn't disappoint, every procedure was at least acceptable, understandable and most important correct (doesn't hallucinate with these ",
          "Even then, Gemini identified that the problem is unconstrained and suggested that a particular solution exists with a squared penalty instead"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Hw2 with Gemini Pro 3 Thinking Mode",
            "author": "Gustavo Jose Ortiz Zepeda",
            "url": "https://edstem.org/us/courses/84647/discussion/7397298",
            "snippet": "For the special participation A on HW2, I use Grok to address the non-coding analytical problems 1, 2 and 7. Gemini did it great as expected, all questions were correct on the first-shot prompt. I use..."
          },
          {
            "title": "Special Participation A: Gemini Pro 3 on Homework 2",
            "author": "Aryan Bansal",
            "url": "https://edstem.org/us/courses/84647/discussion/7431042",
            "snippet": "Gemini easily one-shotted all the homework problems with ease. Explanations were correct, concise, and directly matched the answer key. Even in places where the answer key did not explain (Q5 distribu..."
          }
        ]
      },
      "Claude": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0549
          },
          {
            "term": "mathematical",
            "score": 0.033
          },
          {
            "term": "problems",
            "score": 0.033
          },
          {
            "term": "solution",
            "score": 0.033
          },
          {
            "term": "reasoning",
            "score": 0.022
          },
          {
            "term": "solutions",
            "score": 0.022
          },
          {
            "term": "guidance",
            "score": 0.022
          },
          {
            "term": "problem",
            "score": 0.022
          },
          {
            "term": "demonstrates",
            "score": 0.011
          },
          {
            "term": "strong",
            "score": 0.011
          },
          {
            "term": "capabilities",
            "score": 0.011
          },
          {
            "term": "correctly",
            "score": 0.011
          },
          {
            "term": "derived",
            "score": 0.011
          },
          {
            "term": "analytical",
            "score": 0.011
          },
          {
            "term": "without",
            "score": 0.011
          }
        ],
        "strengths": [
          "Claude demonstrates strong mathematical reasoning capabilities and correctly derived analytical solutions without any mathematical hallucinations or false claims"
        ],
        "weaknesses": [
          "The two problems requiring guidance (Problem 1b and Problem 2a) revealed a consistent pattern: Claude tends to solve problems in an over-complicated way before recognizing simpler approaches"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude on HW2 written part",
            "author": "Yaqi Su",
            "url": "https://edstem.org/us/courses/84647/discussion/7267427",
            "snippet": "Claude demonstrates strong mathematical reasoning capabilities and correctly derived analytical solutions without any mathematical hallucinations or false claims. Across all problems, Claude never mak..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "one",
            "score": 0.0355
          },
          {
            "term": "its",
            "score": 0.0284
          },
          {
            "term": "shot",
            "score": 0.0213
          },
          {
            "term": "written",
            "score": 0.0213
          },
          {
            "term": "solution",
            "score": 0.0213
          },
          {
            "term": "homework",
            "score": 0.0213
          },
          {
            "term": "questions",
            "score": 0.0213
          },
          {
            "term": "parsing",
            "score": 0.0213
          },
          {
            "term": "penalty",
            "score": 0.0213
          },
          {
            "term": "problems",
            "score": 0.0142
          },
          {
            "term": "evaluate",
            "score": 0.0142
          },
          {
            "term": "because",
            "score": 0.0142
          },
          {
            "term": "correct",
            "score": 0.0142
          },
          {
            "term": "answer",
            "score": 0.0142
          },
          {
            "term": "question",
            "score": 0.0142
          }
        ],
        "strengths": [
          "I will guide you towards the correct answer should you make a mistake",
          "\"\n\nOverall, the model performed surprisingly well one-shot on each of the questions",
          "However, it wasn't perfect",
          "However, the final result was still correct"
        ],
        "weaknesses": [
          "Also, it had a LaTeX error when generating the result for q2)",
          "I think it is useful as a \"pocket-TA\", but because of its imperfections, particularly with the L1 penalty parsing error, I would say it still requires a fundamental understanding of the concepts to ve"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT 5.1 Extended Thinking on HW2 Written",
            "author": "Anjo Pagdanganan",
            "url": "https://edstem.org/us/courses/84647/discussion/7451058",
            "snippet": "I evaluated ChatGPT 5.1 Extended Thinking's one-shot capability on HW2's written problems - 1, 2, and 5. I try to evaluate its reasoning in addition to solution correctness by ensuring it explains the..."
          }
        ]
      }
    },
    "HW10": {
      "ChatGPT": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0231
          },
          {
            "term": "questions",
            "score": 0.0128
          },
          {
            "term": "paper",
            "score": 0.0128
          },
          {
            "term": "chatgpt",
            "score": 0.0103
          },
          {
            "term": "derivations",
            "score": 0.0103
          },
          {
            "term": "conceptual",
            "score": 0.0103
          },
          {
            "term": "complexity",
            "score": 0.0103
          },
          {
            "term": "attention",
            "score": 0.0103
          },
          {
            "term": "math",
            "score": 0.0077
          },
          {
            "term": "theory",
            "score": 0.0077
          },
          {
            "term": "here",
            "score": 0.0077
          },
          {
            "term": "high",
            "score": 0.0077
          },
          {
            "term": "details",
            "score": 0.0077
          },
          {
            "term": "data",
            "score": 0.0077
          },
          {
            "term": "asked",
            "score": 0.0077
          }
        ],
        "strengths": [
          "It was able to one-shot the conceptual questions (Q4) but failed the math (Q1) and the paper reading (Q5) until I stepped in to correct it",
          "I had to force it to re-read Table 1 to get the correct FLOP count (1",
          "Conclusion: ChatGPT 4o works best as a collaborative peer you need to double-check, rather than an oracle",
          "It requires active \"dragging\" to get precise derivations right, but once corrected, it holds onto that context well",
          ", rewriting softmax with a Gaussian kernel, deriving the linear attention complexity, causal recurrences), the LLM: Got the structure right on the first try"
        ],
        "weaknesses": [
          "It was able to one-shot the conceptual questions (Q4) but failed the math (Q1) and the paper reading (Q5) until I stepped in to correct it",
          "But, it learned from its mistakes (in Q1b)\n\nAfter I corrected the complexity error in Part A, we moved on to Part B (Causal Masking)",
          "It hallucinates data tables (Q5 - FaceNet Paper)\n\nThis was the biggest failure mode",
          "It outperformed the solution key on design (Q4 - Example Difficulty)\n\nOn the flip side, the model excelled at the \"Early Exit\" conceptual questions",
          "For high-level conceptual questions (example difficulty, why early exit helps, what hooks do, when to use early exit vs a smaller model): It also mostly one-shotted reasonable, coherent answers"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT 4o on HW10",
            "author": "Shreyes Sridhara",
            "url": "https://edstem.org/us/courses/84647/discussion/7423926",
            "snippet": "For my special participation A, I put ChatGPT 4o to the test on the non-coding questions of Homework 10. My goal was to see if the model could handle a mix of complex math derivations, conceptual deep..."
          },
          {
            "title": "Special Participation A: Homework 10 ChatGPT 5.1 Thinking",
            "author": "Shoumik Roychowdhury",
            "url": "https://edstem.org/us/courses/84647/discussion/7429282",
            "snippet": "For any question that depended on my own training runs / plots / metrics, I explicitly asked it to: State what it couldn’t know, and then ell me what I needed to fill in from my own notebook (accuracy..."
          }
        ]
      },
      "Unknown": {
        "post_count": 5,
        "top_terms": [
          {
            "term": "analysis",
            "score": 0.0131
          },
          {
            "term": "its",
            "score": 0.0131
          },
          {
            "term": "model",
            "score": 0.0131
          },
          {
            "term": "reasoning",
            "score": 0.0131
          },
          {
            "term": "question",
            "score": 0.0113
          },
          {
            "term": "here",
            "score": 0.0113
          },
          {
            "term": "chat",
            "score": 0.0113
          },
          {
            "term": "deepseek",
            "score": 0.0113
          },
          {
            "term": "problems",
            "score": 0.0113
          },
          {
            "term": "grok",
            "score": 0.0094
          },
          {
            "term": "notebook",
            "score": 0.0094
          },
          {
            "term": "https",
            "score": 0.0094
          },
          {
            "term": "com",
            "score": 0.0094
          },
          {
            "term": "share",
            "score": 0.0094
          },
          {
            "term": "solve",
            "score": 0.0075
          }
        ],
        "strengths": [
          "Question 1: Linearized Attention Derivation The model handled the mathematical derivation pretty well",
          "It was particularly strong in analyzing the computational complexity, correctly identifying the reduction from quadratic O(N2) to linear O(N) by leveraging the recursive cumulative sum trick",
          "This was a one shot success",
          "Question 2: FaceNet Paper Analysis This section was handled well as expected since it was a reading assignment with context given",
          "It provided correct facts as needed for the question/\n\nQuestion 3: Example Difficulty (Notebook Analysis) This was the most revealing interaction"
        ],
        "weaknesses": [
          "It provided correct facts as needed for the question/\n\nQuestion 3: Example Difficulty (Notebook Analysis) This was the most revealing interaction",
          "It abandoned its generic answers and provided a more accurate analysis of the bimodal exit distributions and the specific geometric properties (elongation/noise) that caused difficulty",
          "However, it exhibited a tendency to \"coast\" on general knowledge when specific data was missing (as seen in the notebook section)",
          "Overall, the model did quite well on the conceptual and algebraic parts, but struggled on the subtle complexity analysis:\n\nFor the math derivations (e",
          "For the more delicate algorithmic complexity question (kernelized attention with random features), it gave a plausible but wrong answer, keeping an unnecessary (N^2) term and missing the whole “linear"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: DeepSeek on HW 10",
            "author": "Rudy Colato",
            "url": "https://edstem.org/us/courses/84647/discussion/7452189",
            "snippet": "Link: https://chat.deepseek.com/share/phkiu5eh6bi8i6i02j\n\nFor my special participation, I used DeepSeek to solve the written problems from HW 10.\n\nIn general, I find DeepSeek's chain-of-thought reason..."
          },
          {
            "title": "Special Participation A: Deepseek v3.2 on HW10",
            "author": "Kelvin Li",
            "url": "https://edstem.org/us/courses/84647/discussion/7405742",
            "snippet": "Executive Summary\n\nI used the newly released DeepSeek v3.2 on HW10.\n\nOverall, this tests the model's \n1. OCR capabilities (reading the fine equations in the screenshots of the problems and also findin..."
          },
          {
            "title": "Special Participation A: GPT 5 Thinking on HW 10",
            "author": "Sanjay Adhikesaven",
            "url": "https://edstem.org/us/courses/84647/discussion/7430749",
            "snippet": "I used ChatGPT 5 (Thinking) on HW 10 (all non-coding parts).\n\nHere is the conversation log. Here is the annotated conversation.\n\nSummary:  Across my interaction, ChatGPT was able to one-shot solve eac..."
          }
        ]
      },
      "GPT-4o": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0548
          },
          {
            "term": "one",
            "score": 0.0274
          },
          {
            "term": "perform",
            "score": 0.0274
          },
          {
            "term": "since",
            "score": 0.0274
          },
          {
            "term": "exercise",
            "score": 0.0137
          },
          {
            "term": "used",
            "score": 0.0137
          },
          {
            "term": "legacy",
            "score": 0.0137
          },
          {
            "term": "chatgpt",
            "score": 0.0137
          },
          {
            "term": "models",
            "score": 0.0137
          },
          {
            "term": "gpt",
            "score": 0.0137
          },
          {
            "term": "analyzed",
            "score": 0.0137
          },
          {
            "term": "non",
            "score": 0.0137
          },
          {
            "term": "coding",
            "score": 0.0137
          },
          {
            "term": "portions",
            "score": 0.0137
          },
          {
            "term": "homework",
            "score": 0.0137
          }
        ],
        "strengths": [
          "Initially I expected that this model wouldn't perform so well since it's an older model and I've previously experienced hallucinations with it",
          "It also had a pretty hand-wavy explanation for the runtime of softmax approximated-attention but got the correct answer nonetheless"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A: GPT-4o on HW10 Noncoding",
            "author": "John Chang",
            "url": "https://edstem.org/us/courses/84647/discussion/7405450",
            "snippet": "For this exercise, I used one of the legacy ChatGPT models (GPT-4o) and analyzed how it would perform on the non-coding portions of Homework 10, i.e. questions 1 and 5. \n\nInitially I expected that thi..."
          }
        ]
      },
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0273
          },
          {
            "term": "model",
            "score": 0.0191
          },
          {
            "term": "questions",
            "score": 0.0164
          },
          {
            "term": "correctly",
            "score": 0.0137
          },
          {
            "term": "paper",
            "score": 0.0137
          },
          {
            "term": "conceptual",
            "score": 0.0109
          },
          {
            "term": "used",
            "score": 0.0082
          },
          {
            "term": "deep",
            "score": 0.0082
          },
          {
            "term": "fourier",
            "score": 0.0082
          },
          {
            "term": "style",
            "score": 0.0082
          },
          {
            "term": "facenet",
            "score": 0.0082
          },
          {
            "term": "incorrect",
            "score": 0.0082
          },
          {
            "term": "correct",
            "score": 0.0082
          },
          {
            "term": "data",
            "score": 0.0082
          },
          {
            "term": "chat",
            "score": 0.0082
          }
        ],
        "strengths": [
          "After the model provided initial correct answers, I explicitly challenged it with conflicting data (High Params vs Low FLOPs) to test if it truly understood the underlying architecture or was just ret",
          "When given directional feedback, Gemini generally moved toward the correct answer rather than defending incorrect positions indefinitely",
          "This was a clear hallucination since the paper does discuss harmonic embeddings in Section 3",
          "I steered Gemini to the correct answer in 4 ways:\n\nAbstract conceptual questions: often led to defensive elaboration of the same point\n\nHypothetical scenarios: better success at identifying missteps\n\n"
        ],
        "weaknesses": [
          "For the problem 5, when challenged on the counter-intuitive discrepancy between FaceNet NN1’s high parameter count (140M) vs",
          "It did not hallucinate incorrect numbers and correctly referenced standard architecture traits (VGG-style vs",
          "When given directional feedback, Gemini generally moved toward the correct answer rather than defending incorrect positions indefinitely",
          "But it did anchor on incorrect interpretation sometimes",
          "This was a clear hallucination since the paper does discuss harmonic embeddings in Section 3"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A:  Gemini 3 Pro on the written part of HW 10",
            "author": "Zhengwei Fan",
            "url": "https://edstem.org/us/courses/84647/discussion/7424271",
            "snippet": "\n\nModel Used: Gemini 3 Pro \n\nOverall Performance: The model demonstrated exceptional proficiency in both advanced mathematical derivations (kernel methods) and deep learning architectural analysis. It..."
          },
          {
            "title": "Special Participation A: Gemini 2.5 Flash on HW10",
            "author": "Imra Dawoodani",
            "url": "https://edstem.org/us/courses/84647/discussion/7404071",
            "snippet": "I evaluated Gemini 2.5 Flash on the non coding portions of Homework 10, covering Kernelized Linear Attention and the FaceNet paper reading questions. Approximately 60-65% of questions were answered co..."
          }
        ]
      },
      "Gemini Pro": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "questions",
            "score": 0.0602
          },
          {
            "term": "gemini",
            "score": 0.0301
          },
          {
            "term": "question",
            "score": 0.0301
          },
          {
            "term": "coding",
            "score": 0.0226
          },
          {
            "term": "answers",
            "score": 0.0226
          },
          {
            "term": "part",
            "score": 0.0226
          },
          {
            "term": "responses",
            "score": 0.0226
          },
          {
            "term": "pro",
            "score": 0.015
          },
          {
            "term": "answer",
            "score": 0.015
          },
          {
            "term": "non",
            "score": 0.015
          },
          {
            "term": "one",
            "score": 0.015
          },
          {
            "term": "attention",
            "score": 0.015
          },
          {
            "term": "about",
            "score": 0.015
          },
          {
            "term": "facenet",
            "score": 0.015
          },
          {
            "term": "paper",
            "score": 0.015
          }
        ],
        "strengths": [
          "However, for the second question on the FaceNet paper, Gemini oneshots it and provides detailed and accurate responses to all the questions that match the provided solutions",
          "Summarization and querying key details from dense articles and papers appears to be its strength because it generates answers fairly quickly and with high accuracy; questions that are a little bit mor"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A -- Gemini Pro 3 Thinking on HW 10 , Arvind Kruthiventy",
            "author": "Arvind Kruthiventy",
            "url": "https://edstem.org/us/courses/84647/discussion/7447290",
            "snippet": "In this post, I use Gemini Pro 3 on the HW 10 to answer the non-coding portions which were two questions: one question on kernelized linear attention for efficient attention computation over long sequ..."
          }
        ]
      },
      "Claude Opus": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0357
          },
          {
            "term": "problem",
            "score": 0.0238
          },
          {
            "term": "paper",
            "score": 0.0238
          },
          {
            "term": "pdf",
            "score": 0.0238
          },
          {
            "term": "step",
            "score": 0.0238
          },
          {
            "term": "its",
            "score": 0.0238
          },
          {
            "term": "here",
            "score": 0.0238
          },
          {
            "term": "experimented",
            "score": 0.0119
          },
          {
            "term": "opus",
            "score": 0.0119
          },
          {
            "term": "using",
            "score": 0.0119
          },
          {
            "term": "extended",
            "score": 0.0119
          },
          {
            "term": "thinking",
            "score": 0.0119
          },
          {
            "term": "experience",
            "score": 0.0119
          },
          {
            "term": "surprisingly",
            "score": 0.0119
          },
          {
            "term": "great",
            "score": 0.0119
          }
        ],
        "strengths": [
          "Something impressive was its ability to parse the research paper correctly and ground its answers in the actual content rather than hallucinating details or making unsupported claims",
          "While a few of the explanations could have been more detailed or expanded with additional intuition, the overall responses were coherent, well structured, and factually correct",
          "In the end, it solved every problem in a single attempt"
        ],
        "weaknesses": [
          "I provided it with screenshots of each problem, along with the full FaceNet paper PDF from arXiv for reference",
          "In the end, it solved every problem in a single attempt"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Opus 4.5 with Extended Thinking on HW10",
            "author": "Keshab Agarwal",
            "url": "https://edstem.org/us/courses/84647/discussion/7427672",
            "snippet": "I experimented with Claude Opus 4.5 using Extended Thinking on HW10, and the experience was, not surprisingly, great. I provided it with screenshots of each problem, along with the full FaceNet paper ..."
          }
        ]
      },
      "Claude Sonnet": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0635
          },
          {
            "term": "problems",
            "score": 0.0476
          },
          {
            "term": "sonnet",
            "score": 0.0317
          },
          {
            "term": "pdf",
            "score": 0.0317
          },
          {
            "term": "its",
            "score": 0.0317
          },
          {
            "term": "here",
            "score": 0.0317
          },
          {
            "term": "experimented",
            "score": 0.0159
          },
          {
            "term": "written",
            "score": 0.0159
          },
          {
            "term": "portions",
            "score": 0.0159
          },
          {
            "term": "specifically",
            "score": 0.0159
          },
          {
            "term": "overall",
            "score": 0.0159
          },
          {
            "term": "even",
            "score": 0.0159
          },
          {
            "term": "though",
            "score": 0.0159
          },
          {
            "term": "basic",
            "score": 0.0159
          },
          {
            "term": "version",
            "score": 0.0159
          }
        ],
        "strengths": [
          "5 solved all of the problems on the first attempt, and although some of its explanations were a bit verbose, its answers were consistently correct and well-grounded"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Sonnet 4.5 on HW10",
            "author": "Swetha Rajkumar",
            "url": "https://edstem.org/us/courses/84647/discussion/7436873",
            "snippet": "I experimented with Claude Sonnet 4.5 on the written portions of HW10, specifically problems 1 and 5. Overall, even though this is the basic version of Claude, it was able to answer all of my question..."
          }
        ]
      }
    },
    "HW8": {
      "Unknown": {
        "post_count": 6,
        "top_terms": [
          {
            "term": "problem",
            "score": 0.0132
          },
          {
            "term": "its",
            "score": 0.0132
          },
          {
            "term": "model",
            "score": 0.0099
          },
          {
            "term": "solution",
            "score": 0.0099
          },
          {
            "term": "one",
            "score": 0.0086
          },
          {
            "term": "deepseek",
            "score": 0.0086
          },
          {
            "term": "complexity",
            "score": 0.0079
          },
          {
            "term": "ridge",
            "score": 0.0073
          },
          {
            "term": "answer",
            "score": 0.0073
          },
          {
            "term": "correctly",
            "score": 0.0066
          },
          {
            "term": "derivation",
            "score": 0.0066
          },
          {
            "term": "questions",
            "score": 0.0066
          },
          {
            "term": "step",
            "score": 0.0066
          },
          {
            "term": "first",
            "score": 0.0059
          },
          {
            "term": "kernel",
            "score": 0.0059
          }
        ],
        "strengths": [
          "The main failure was the first attempt at the diagonal-plus-low-rank (DPLR) SSM kernel (Problem 1(f)): the model produced an incorrect, hand-wavy spectral argument with invented “perturbative terms,” ",
          "The interaction felt less like getting final answers from an oracle and more like supervising a strong but occasionally overconfident collaborator who needs spot checks on nontrivial linear-algebra st",
          "Overall, for this session the LLM “one-shot” most sub-questions, but required human skepticism and targeted follow-up prompts to avoid accepting a superficially impressive but wrong derivation",
          "Problem 1: SSM Convolution Kernel\n\nParts (a)–(e): One-Shot Success\n\n(a) Convolution kernel derivation: It correctly unrolled the SSM, derived xk​=∑ℓ=0k−1​AℓBuk−1−ℓ​, substituted into yk​, and reindexe",
          "Behavior: Good at routine linear algebra, explicit about missing details / assumptions rather than hallucinating them as facts"
        ],
        "weaknesses": [
          "The main failure was the first attempt at the diagonal-plus-low-rank (DPLR) SSM kernel (Problem 1(f)): the model produced an incorrect, hand-wavy spectral argument with invented “perturbative terms,” ",
          "When pushed, it was able to self-diagnose its previous mistakes, explicitly list what it had gotten wrong, and then re-derive the result more rigorously",
          "Overall, for this session the LLM “one-shot” most sub-questions, but required human skepticism and targeted follow-up prompts to avoid accepting a superficially impressive but wrong derivation",
          "The transcript in Problem 1_ SSM Convolution Kernel",
          "Scope: I asked it to solve the non-coding parts of:\n\nProblem 1: SSM convolution kernel (parts (a)–(f), including diagonal and DPLR structure and complexity)"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Kimi K2 on hw8",
            "author": "Nils Selte",
            "url": "https://edstem.org/us/courses/84647/discussion/7401923",
            "snippet": "I used kimi k2 on hw9 and observed it giving correct answers zero shot on all questions. (even without \"thinking\" tokens) very impressed.\n\n..."
          },
          {
            "title": "Special Participation A: Qwen on HW8",
            "author": "Hanna Roed",
            "url": "https://edstem.org/us/courses/84647/discussion/7322058",
            "snippet": "Below is my report on using Qwen3-Max on the written part of homework 8.\n\nOverall, I'm very impressed by Qwen3-Max's performance on this homework. It seems like it really does well on the questions wh..."
          },
          {
            "title": "Special Participation A: Deepseek v3.2 on HW 8",
            "author": "Justin Li",
            "url": "https://edstem.org/us/courses/84647/discussion/7405582",
            "snippet": "I used DeepSeek v3.2 to solve the written portions of HW8, where it performed quite well and one shotted almost all of the problems. \n\nOne interesting point was that DeepSeek struggled significantly w..."
          }
        ]
      },
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "prompt",
            "score": 0.0347
          },
          {
            "term": "gemini",
            "score": 0.0231
          },
          {
            "term": "its",
            "score": 0.0231
          },
          {
            "term": "used",
            "score": 0.0173
          },
          {
            "term": "lazy",
            "score": 0.0173
          },
          {
            "term": "detailed",
            "score": 0.0173
          },
          {
            "term": "solve",
            "score": 0.0116
          },
          {
            "term": "non",
            "score": 0.0116
          },
          {
            "term": "coding",
            "score": 0.0116
          },
          {
            "term": "overall",
            "score": 0.0116
          },
          {
            "term": "mathematical",
            "score": 0.0116
          },
          {
            "term": "derivations",
            "score": 0.0116
          },
          {
            "term": "often",
            "score": 0.0116
          },
          {
            "term": "reasoning",
            "score": 0.0116
          },
          {
            "term": "results",
            "score": 0.0116
          }
        ],
        "strengths": [
          "Overall, Gemini did an excellent job producing clear and well‑structured mathematical derivations, often matching the logical flow of the official solutions",
          "I ran an A/B test using two distinct prompts: a \"Lazy\" prompt (minimal instruction) and a \"Rigorous\" prompt (detailed constraints, persona setting, and formatting rules)",
          "My conclusion: Attempting to engineer the perfect pedagogical prompt often yields diminishing returns",
          "Unless your prompt is extremely specific and detailed, you will likely waste more time trying to \"program\" the AI to teach you than you would by simply struggling through the problem yourself",
          "While my sophisticated prompt was significantly more detailed than the lazy one, I couldn't find a strong justification for the extra setup time"
        ],
        "weaknesses": [
          "Unless your prompt is extremely specific and detailed, you will likely waste more time trying to \"program\" the AI to teach you than you would by simply struggling through the problem yourself",
          "Disclaimer: This interaction was not conducted in \"Study Mode,\" I have not tested/used this mode in the past so i cannot speak to the abilities in this regard"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 3 pro on HW 8",
            "author": "Tin Yau",
            "url": "https://edstem.org/us/courses/84647/discussion/7397226",
            "snippet": "I used Gemini 3 Pro to solve the non‑coding portion of HW 8. Overall, Gemini did an excellent job producing clear and well‑structured mathematical derivations, often matching the logical flow of the o..."
          },
          {
            "title": "Special Participation A: Gemini 3 (Thinking) on HW 8",
            "author": "Andrew Choy",
            "url": "https://edstem.org/us/courses/84647/discussion/7369656",
            "snippet": "For this assignment, I used Gemini to tackle the non-coding/theory portions of Homework 8. Beyond simply checking if the model could solve the math, I wanted to investigate whether crafting an \"ideal\"..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0328
          },
          {
            "term": "correct",
            "score": 0.0262
          },
          {
            "term": "step",
            "score": 0.0262
          },
          {
            "term": "answer",
            "score": 0.0197
          },
          {
            "term": "its",
            "score": 0.0164
          },
          {
            "term": "chatgpt",
            "score": 0.0131
          },
          {
            "term": "solve",
            "score": 0.0098
          },
          {
            "term": "questions",
            "score": 0.0098
          },
          {
            "term": "chat",
            "score": 0.0098
          },
          {
            "term": "gpt",
            "score": 0.0098
          },
          {
            "term": "question",
            "score": 0.0098
          },
          {
            "term": "gave",
            "score": 0.0098
          },
          {
            "term": "follow",
            "score": 0.0098
          },
          {
            "term": "annotations",
            "score": 0.0098
          },
          {
            "term": "indicate",
            "score": 0.0098
          }
        ],
        "strengths": [
          "Below, I outlined the strengths and weaknesses of the model that I noticed, which included the types of questions Chat GPT tended to do well on versus needed more guidance on",
          "Strengths:\n\nConsistently correct on direct mathematical derivations (unrolling recurrences, computing kernels, linear algebra, etc",
          ")\n\nProduced clear step-by-step reasoning with limited guidance\n\nHandled numerical examples and matrix calculations well\n\nAble to refine and reorganize its thoughts effectively when prompted\n\nWeaknesse",
          "Green annotations / highlights indicate the response was correct",
          "Orange annotations indicate that the answer was partially correct"
        ],
        "weaknesses": [
          ")\n\nProduced clear step-by-step reasoning with limited guidance\n\nHandled numerical examples and matrix calculations well\n\nAble to refine and reorganize its thoughts effectively when prompted\n\nWeaknesse",
          "Red annotations / highlights indicate that the response was incorrect",
          "However, the main problem I ran into was that the model did not follow the format instructions",
          "It struggled to properly use the input size variable in its final answer",
          "But it is not a perfect teaching assistant because it often ignores specific instructions on how to explain the answer, and it can struggle with abstract, symbolic math like fully parameterized time c"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT on HW 8",
            "author": "Dagny Streit",
            "url": "https://edstem.org/us/courses/84647/discussion/7408067",
            "snippet": "I used ChatGPT 5.1 (Auto) to solve the written portions of Homework 8 (Questions 1, 3, and 4). For most of the problems, Chat GPT was able to correctly solve them on the first try. Below, I outlined t..."
          },
          {
            "title": "Special Participation A: ChatGPT 4o on HW 8",
            "author": "Jermaine Lei",
            "url": "https://edstem.org/us/courses/84647/discussion/7427518",
            "snippet": "For this special participation, I used the ChatGPT-4o model to solve the non-coding parts of Homework 8. To start the conversation, I gave the model the full assignment and asked it to act as a \"Deep ..."
          }
        ]
      },
      "Claude Sonnet": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0615
          },
          {
            "term": "problems",
            "score": 0.0462
          },
          {
            "term": "part",
            "score": 0.0462
          },
          {
            "term": "quite",
            "score": 0.0462
          },
          {
            "term": "attempt",
            "score": 0.0308
          },
          {
            "term": "one",
            "score": 0.0308
          },
          {
            "term": "problem",
            "score": 0.0308
          },
          {
            "term": "wise",
            "score": 0.0308
          },
          {
            "term": "correct",
            "score": 0.0308
          },
          {
            "term": "below",
            "score": 0.0154
          },
          {
            "term": "report",
            "score": 0.0154
          },
          {
            "term": "written",
            "score": 0.0154
          },
          {
            "term": "went",
            "score": 0.0154
          },
          {
            "term": "through",
            "score": 0.0154
          },
          {
            "term": "provide",
            "score": 0.0154
          }
        ],
        "strengths": [
          "The answers were for the most part correct, but Claude struggled greatly to reach the correct answer for the path length problems in part 1, and I had to give quite a few hints to guide Claude into th",
          "Some explanations could have been more detailed"
        ],
        "weaknesses": [
          "It may have been a mistake to prompt Claude problem-wise rather than part-wise, since I noticed that it repeatedly encountered the same mistakes on its first attempt at problem 1",
          "The answers were for the most part correct, but Claude struggled greatly to reach the correct answer for the path length problems in part 1, and I had to give quite a few hints to guide Claude into th"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Sonnet 4.5 on HW 8",
            "author": "Celine Tan",
            "url": "https://edstem.org/us/courses/84647/discussion/7451347",
            "snippet": "Below is my report for Claude's attempt at HW 8 (written). I went through the problems one-by-one and did not provide much guidance other than when it got stuck. It may have been a mistake to prompt C..."
          }
        ]
      }
    },
    "HW12": {
      "Gemini": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "model",
            "score": 0.041
          },
          {
            "term": "correctly",
            "score": 0.0187
          },
          {
            "term": "theoretical",
            "score": 0.0187
          },
          {
            "term": "visual",
            "score": 0.0187
          },
          {
            "term": "divergence",
            "score": 0.0149
          },
          {
            "term": "question",
            "score": 0.0149
          },
          {
            "term": "performance",
            "score": 0.0112
          },
          {
            "term": "one",
            "score": 0.0112
          },
          {
            "term": "theory",
            "score": 0.0112
          },
          {
            "term": "vib",
            "score": 0.0112
          },
          {
            "term": "code",
            "score": 0.0112
          },
          {
            "term": "plots",
            "score": 0.0112
          },
          {
            "term": "reasoning",
            "score": 0.0112
          },
          {
            "term": "part",
            "score": 0.0112
          },
          {
            "term": "gemini",
            "score": 0.0075
          }
        ],
        "strengths": [
          "Model Tested: Gemini 3 Pro\n\nOverall Performance: Very good: 100% One-shot\n\nPerformance Overview\n\nThe model was tasked with solving 3 deep learning problems involving debugging neural network initializ",
          "It provided the correct theoretical justification (variance scaling) and the exact code fix (Xavier/Glorot scaling) without needing iterative prompting",
          "Behavioral Observations & Strategies\n\nMultimodal Reasoning: The model demonstrated strong vision capabilities, accurately interpreting trend lines in plots (Question 3, Part 1) and distribution shapes",
          "This likely helped the model maintain focus, though its strong performance suggests it might have handled the full context in one go"
        ],
        "weaknesses": [
          "Code Debugging (Transformers): The model correctly identified a \"peaked softmax\" issue caused by improper weight initialization in a Transformer implementation",
          "It also correctly interpreted unlabeled validation error curves by reasoning about the regularization coefficient",
          "Hallucinations & Accuracy\n\nHallucination Rate: 0%",
          "The model did not hallucinate",
          "Notably, in Question 3 (Part 2), the model explicitly noted that \"Figure 4 was not included\" but proceeded to solve the problem by deriving what the plots must look like based on VIB theory"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 3 pro on Hw 12",
            "author": "Gabriel Han",
            "url": "https://edstem.org/us/courses/84647/discussion/7398141",
            "snippet": "Model Tested: Gemini 3 Pro\n\nOverall Performance: Very good: 100% One-shot\n\nPerformance Overview\n\nThe model was tasked with solving 3 deep learning problems involving debugging neural network initializ..."
          }
        ]
      },
      "Claude Opus": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0261
          },
          {
            "term": "correct",
            "score": 0.0261
          },
          {
            "term": "reasoning",
            "score": 0.0187
          },
          {
            "term": "explanation",
            "score": 0.0187
          },
          {
            "term": "problem",
            "score": 0.0187
          },
          {
            "term": "answer",
            "score": 0.0149
          },
          {
            "term": "didn",
            "score": 0.0149
          },
          {
            "term": "one",
            "score": 0.0112
          },
          {
            "term": "intuition",
            "score": 0.0112
          },
          {
            "term": "beta",
            "score": 0.0112
          },
          {
            "term": "prompting",
            "score": 0.0112
          },
          {
            "term": "clean",
            "score": 0.0112
          },
          {
            "term": "mistakes",
            "score": 0.0075
          },
          {
            "term": "pretty",
            "score": 0.0075
          },
          {
            "term": "shot",
            "score": 0.0075
          }
        ],
        "strengths": [
          "On algebraic or mechanical reasoning, it was very strong and made no mistakes and pretty much one shot all questions",
          "So it can get to the right answer, but sometimes needs prompting to avoid shallow intuition",
          "When guided, it produces very solid reasoning; when left alone, it feels more lazy and usually settles for a simplified story; however, seems like everything can be solved by better prompting",
          "ai/share/72ff4a16-11f6-436e-925b-163c5ce94835 \n\n\n\nProblem 1\n\nClaude handled this one pretty well",
          "It didn’t really explore whether hidden state variance or LayerNorm behavior could change the picture, but the core reasoning was correct and it stayed on track the whole time"
        ],
        "weaknesses": [
          "ai/share/72ff4a16-11f6-436e-925b-163c5ce94835 \n\n\n\nProblem 1\n\nClaude handled this one pretty well",
          "Problem 2\n\nThe KL example was correct, but Claude felt a bit on autopilot here",
          "Problem 3\n\nThis was where Claude struggled the most",
          "But when we got into the beta values and how they map to the latent plots, it started with the wrong intuition, saying small beta should make the latent “spread more",
          "Problem 5\n\nClaude did extremely well here"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Opus 4.5 with extended thinking on HW12",
            "author": "Will Cai",
            "url": "https://edstem.org/us/courses/84647/discussion/7451901",
            "snippet": "Summary: Overall Claude was reliable but with a specific pattern on answer quality. On algebraic or mechanical reasoning, it was very strong and made no mistakes and pretty much one shot all questions..."
          }
        ]
      },
      "Unknown": {
        "post_count": 5,
        "top_terms": [
          {
            "term": "question",
            "score": 0.0196
          },
          {
            "term": "model",
            "score": 0.0153
          },
          {
            "term": "questions",
            "score": 0.0131
          },
          {
            "term": "answer",
            "score": 0.0094
          },
          {
            "term": "one",
            "score": 0.0087
          },
          {
            "term": "encoder",
            "score": 0.0087
          },
          {
            "term": "correct",
            "score": 0.008
          },
          {
            "term": "right",
            "score": 0.008
          },
          {
            "term": "chat",
            "score": 0.0073
          },
          {
            "term": "graphs",
            "score": 0.0073
          },
          {
            "term": "like",
            "score": 0.0065
          },
          {
            "term": "correctly",
            "score": 0.0065
          },
          {
            "term": "able",
            "score": 0.0065
          },
          {
            "term": "used",
            "score": 0.0058
          },
          {
            "term": "homework",
            "score": 0.0058
          }
        ],
        "strengths": [
          "Per-Question Breakdown:\n\nQuestion 1: Debugging Transformers (Initialization)\n\nResult: Perfect / One-Shot\n\nAnalysis: Grok exhibited \"Global Code Awareness",
          "It provided the correct fix ($1/\\sqrt{d_{model}}$) and the correct causal explanation",
          "Question 2: Comparing Distributions (KL Divergence)\n\nResult: Perfect / One-Shot\n\nAnalysis: The model demonstrated deep intuition for Information Theory",
          "It also generated a mathematically valid counter-example (Nested Uniforms) to prove the finiteness condition D_KL(P||Q) < infinity vs D_KL(Q||P) = infinity without any prompting",
          "Question 3: Variational Information Bottleneck (VIB)\n\nResult: Perfect / One-Shot\n\nAnalysis: This was the highlight of the session"
        ],
        "weaknesses": [
          "Unlike previous generations of models that often hallucinate on complex derivations or misinterpret geometric relationships, Grok acted like a competent graduate-level tutor, correctly handling everyt",
          "\" It didn't just flag the std=1 initialization as a heuristic error; it explicitly linked it to Line 23 (weight tying), reasoning that sharing large-variance weights between input and output heads wou",
          "It matched every plot and error curve correctly based purely on theoretical physics-style reasoning",
          "Question 5: Meta-Learning Derivations\n\nResult: Correct\n\nAnalysis: I tasked the model with a multi-step derivation for the expected test error of a minimum-norm solution",
          "Grok correctly set up the constrained optimization problem, applied the orthonormality properties of the test features to cancel cross-terms, and derived the final analytical loss function"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Getting GPT 5.1 to answer Homework 12",
            "author": "Sriram Srivatsan",
            "url": "https://edstem.org/us/courses/84647/discussion/7425035",
            "snippet": "I got OpenAI's GPT 5.1 model to answer questions 1, 2, and 4 in homework 12. Overall, it seems that this model is able to answer questions about the material extremely accurately, and sometimes it eve..."
          },
          {
            "title": "Special Participation A: Mistral AI's Le Chat on HW12 Written Portion",
            "author": "Devan Perkash",
            "url": "https://edstem.org/us/courses/84647/discussion/7424734",
            "snippet": "I used Mistral AI's Le Chat on the written portion of HW 12. \n\n\n\nExecutive Summary:\n\nMistral's Le Chat had high variance with regard to its success on HW 12. For the first several questions, it was ac..."
          },
          {
            "title": "Special Participation A: Qwen on HW12 Non-coding parts",
            "author": "Tiffany Dang",
            "url": "https://edstem.org/us/courses/84647/discussion/7450064",
            "snippet": "For Special Participation A, I used Qwen to solve non-coding questions of HW12. Overall, the accuracy and performance was outstanding. I attached the txt file of the conversation because I couldn't fi..."
          }
        ]
      },
      "Claude": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0408
          },
          {
            "term": "one",
            "score": 0.0408
          },
          {
            "term": "about",
            "score": 0.0408
          },
          {
            "term": "below",
            "score": 0.0204
          },
          {
            "term": "report",
            "score": 0.0204
          },
          {
            "term": "using",
            "score": 0.0204
          },
          {
            "term": "sonnet",
            "score": 0.0204
          },
          {
            "term": "model",
            "score": 0.0204
          },
          {
            "term": "solve",
            "score": 0.0204
          },
          {
            "term": "written",
            "score": 0.0204
          },
          {
            "term": "questions",
            "score": 0.0204
          },
          {
            "term": "homework",
            "score": 0.0204
          },
          {
            "term": "attached",
            "score": 0.0204
          },
          {
            "term": "pdf",
            "score": 0.0204
          },
          {
            "term": "annotated",
            "score": 0.0204
          }
        ],
        "strengths": [
          "Summary:\n\nOverall Claude correctly one-shots every question\n\nGenerally, it's reasoning is correct, except for one slightly informal/imprecise statement that it makes about distributions in the second "
        ],
        "weaknesses": [
          "Summary:\n\nOverall Claude correctly one-shots every question\n\nGenerally, it's reasoning is correct, except for one slightly informal/imprecise statement that it makes about distributions in the second ",
          "In general, I felt that its explanations lacked pedagogical value and would not be maximally helpful to a student who was confused about the class material"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude (Sonnet 4.5) on HW 12",
            "author": "Ishir Garg",
            "url": "https://edstem.org/us/courses/84647/discussion/7393256",
            "snippet": "Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 12. I have also attached a PDF of the annotated transcript.\n\nSummary:\n\nOverall Claude correctly one-sho..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "question",
            "score": 0.0505
          },
          {
            "term": "chatgpt",
            "score": 0.0404
          },
          {
            "term": "diagram",
            "score": 0.0404
          },
          {
            "term": "questions",
            "score": 0.0303
          },
          {
            "term": "post",
            "score": 0.0303
          },
          {
            "term": "pdf",
            "score": 0.0303
          },
          {
            "term": "annotated",
            "score": 0.0202
          },
          {
            "term": "chat",
            "score": 0.0202
          },
          {
            "term": "below",
            "score": 0.0202
          },
          {
            "term": "here",
            "score": 0.0202
          },
          {
            "term": "actually",
            "score": 0.0202
          },
          {
            "term": "first",
            "score": 0.0202
          },
          {
            "term": "code",
            "score": 0.0202
          },
          {
            "term": "worked",
            "score": 0.0202
          },
          {
            "term": "solve",
            "score": 0.0202
          }
        ],
        "strengths": [
          "com/share/6938f111-c4b4-800d-90fd-000f7b0fa644\n\nQuestion (2): ChatGPT solved this question without too much difficulty",
          "Here was the 3(a) ASCII diagram and annotated chatlog PDF:\n\n\n\nEDIT: Attached a better PDF"
        ],
        "weaknesses": [
          "com/share/6938f111-c4b4-800d-90fd-000f7b0fa644\n\nQuestion (2): ChatGPT solved this question without too much difficulty",
          "Question (5)(a)-(b): ChatGPT actually failed to solve this one at first, having \"forgotten\" Homework 12's full contents"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT-5 (Regular) on Homework 12",
            "author": "Evan Davis",
            "url": "https://edstem.org/us/courses/84647/discussion/7445083",
            "snippet": "Done as reflected on the deconflict sheet.\n\nNote that I did Questions (1) and 5(c) on my other Special Participation B post, because I treated them as coding Questions. For this post, I do questions 2..."
          }
        ]
      },
      "Gemini Flash": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0278
          },
          {
            "term": "model",
            "score": 0.0278
          },
          {
            "term": "correctly",
            "score": 0.0278
          },
          {
            "term": "about",
            "score": 0.0185
          },
          {
            "term": "deep",
            "score": 0.0185
          },
          {
            "term": "learning",
            "score": 0.0185
          },
          {
            "term": "found",
            "score": 0.0185
          },
          {
            "term": "interpreted",
            "score": 0.0185
          },
          {
            "term": "visual",
            "score": 0.0185
          },
          {
            "term": "plots",
            "score": 0.0185
          },
          {
            "term": "hallucinations",
            "score": 0.0185
          },
          {
            "term": "flash",
            "score": 0.0093
          },
          {
            "term": "demonstrated",
            "score": 0.0093
          },
          {
            "term": "perfect",
            "score": 0.0093
          },
          {
            "term": "one",
            "score": 0.0093
          }
        ],
        "strengths": [
          "Gemini Flash demonstrated a perfect one-shot performance on the non-coding written portions of Homework 12",
          "The model exhibited strong domain knowledge about deep learning, specifically in Transformer initialization stability, KL Divergence, and Variational Information Bottlenecks"
        ],
        "weaknesses": [
          "For instance, in Q1, it explicitly reasoned about the expected squared norm of the embeddings, and in Q3, it correctly interpreted the trade-off between task loss and regularization strength to analyz"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini Flash on HW12",
            "author": "Jincheng Ou",
            "url": "https://edstem.org/us/courses/84647/discussion/7445419",
            "snippet": "Gemini Flash demonstrated a perfect one-shot performance on the non-coding written portions of Homework 12. The model exhibited strong domain knowledge about deep learning, specifically in Transformer..."
          }
        ]
      }
    },
    "HW3": {
      "Gemini Pro": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0259
          },
          {
            "term": "notation",
            "score": 0.0172
          },
          {
            "term": "ask",
            "score": 0.0172
          },
          {
            "term": "analogy",
            "score": 0.0172
          },
          {
            "term": "first",
            "score": 0.0129
          },
          {
            "term": "equalizer",
            "score": 0.0129
          },
          {
            "term": "symbols",
            "score": 0.0129
          },
          {
            "term": "correct",
            "score": 0.0129
          },
          {
            "term": "used",
            "score": 0.0086
          },
          {
            "term": "check",
            "score": 0.0086
          },
          {
            "term": "clarify",
            "score": 0.0086
          },
          {
            "term": "exact",
            "score": 0.0086
          },
          {
            "term": "question",
            "score": 0.0086
          },
          {
            "term": "step",
            "score": 0.0086
          },
          {
            "term": "equation",
            "score": 0.0086
          }
        ],
        "strengths": [
          "Positives (What Worked Well):\n\nOne-shot on standard rewrites: Correctly gave correct answers straight from around 30% of the questions\n\nRight methodology even when imperfect: When wrong/incomplete, it",
          "“Is my thought correct",
          "” Good for catching subtle mistakes and forcing a counterexample or fix"
        ],
        "weaknesses": [
          "Check Gemini’s output against my derivation; ask for clarifications or for the missing step",
          "Inconsistent symbols at times, likely from generic training patterns; needs nudging to match the homework question’s notation",
          "Positives (What Worked Well):\n\nOne-shot on standard rewrites: Correctly gave correct answers straight from around 30% of the questions\n\nRight methodology even when imperfect: When wrong/incomplete, it"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini Pro on HW 3",
            "author": "Ben Yu",
            "url": "https://edstem.org/us/courses/84647/discussion/7250444",
            "snippet": "What I did:\n\nI ran an interactive, non-coding walkthrough of HW 3 using Gemini Pro. Full trace (screenshots + chat excerpts) is in my doc: https://docs.google.com/document/d/1P6yTAFO4GR4W4a_l02kAFN2mY..."
          }
        ]
      },
      "Unknown": {
        "post_count": 4,
        "top_terms": [
          {
            "term": "questions",
            "score": 0.0187
          },
          {
            "term": "its",
            "score": 0.0187
          },
          {
            "term": "grok",
            "score": 0.0164
          },
          {
            "term": "model",
            "score": 0.014
          },
          {
            "term": "homework",
            "score": 0.0117
          },
          {
            "term": "providing",
            "score": 0.0117
          },
          {
            "term": "problem",
            "score": 0.0117
          },
          {
            "term": "one",
            "score": 0.0093
          },
          {
            "term": "correct",
            "score": 0.0093
          },
          {
            "term": "answer",
            "score": 0.0093
          },
          {
            "term": "responses",
            "score": 0.0093
          },
          {
            "term": "chat",
            "score": 0.0093
          },
          {
            "term": "original",
            "score": 0.0093
          },
          {
            "term": "question",
            "score": 0.0093
          },
          {
            "term": "problems",
            "score": 0.0093
          }
        ],
        "strengths": [
          "Hints proved extremely useful, as providing a small nudge almost always led to a correct answer, while leaving them out sometimes still allowed Grok to succeed, such as on questions 4C and 4D",
          "When I gave feedback—acknowledging correct answers or asking for more details—it seemed to “learn” from the previous interaction, adjusting its responses accordingly",
          "It consistently produced correct answers for most questions, and strategic prompting and providing feedback helped guide it effectively",
          "Conclusion:\n\nThe evaluation of DeepSeek's capabilities for homework 3 has demonstrated :\n\nStrong Mathematical Problem-Solving: DeepSeek reliably handles the mathematical problems, including linear alg",
          "Both the calculation and proof problems were well done"
        ],
        "weaknesses": [
          "At the same time, its verbosity, occasional hallucinations, and overzealous problem-solving emphasized the need for careful interaction and moderation",
          "Despite these limitations, Grok was a highly capable tool for this homework, and the process offered valuable insights into how modern LLMs balance reasoning, responsiveness, and conciseness in an int",
          "Conclusion:\n\nThe evaluation of DeepSeek's capabilities for homework 3 has demonstrated :\n\nStrong Mathematical Problem-Solving: DeepSeek reliably handles the mathematical problems, including linear alg",
          "However, for questions that reference an external research paper, it could misunderstand the problem statement and draw something tangent to what the question is asking",
          "Overall, the model was very effective at solving each problem and explaining its reasoning"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Deepseek Chat on HW3",
            "author": "Zhuangzhe Wu",
            "url": "https://edstem.org/us/courses/84647/discussion/7227387",
            "snippet": "Conclusion:\n\nThe evaluation of DeepSeek's capabilities for homework 3 has demonstrated :\n\nStrong Mathematical Problem-Solving: DeepSeek reliably handles the mathematical problems, including linear alg..."
          },
          {
            "title": "Special Participation A: Mistral AI's Le Chat on HW3",
            "author": "Jeffrey Cheng",
            "url": "https://edstem.org/us/courses/84647/discussion/7212131",
            "snippet": "Here is the online link: https://chat.mistral.ai/chat/8c72d241-dd44-41a0-b8fc-a0469d84ff1d\n\nHere is the annotated log:\n\nExecutive Summary:\n\nFrom my observation, Le Chat was able to answer most written..."
          },
          {
            "title": "Special Participation A: GPT 5.1 Thinking (Extended) on HW3",
            "author": "Paul Struble",
            "url": "https://edstem.org/us/courses/84647/discussion/7450077",
            "snippet": "I used GPT 5.1 Thinking (Extended) to solve the non-coding parts of Homework 3. Overall, the model was very effective at solving each problem and explaining its reasoning. It was able to one-shot all ..."
          }
        ]
      },
      "Claude Sonnet": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0652
          },
          {
            "term": "questions",
            "score": 0.029
          },
          {
            "term": "conversation",
            "score": 0.0217
          },
          {
            "term": "answers",
            "score": 0.0217
          },
          {
            "term": "answer",
            "score": 0.0217
          },
          {
            "term": "model",
            "score": 0.0145
          },
          {
            "term": "solve",
            "score": 0.0145
          },
          {
            "term": "provided",
            "score": 0.0145
          },
          {
            "term": "question",
            "score": 0.0145
          },
          {
            "term": "specific",
            "score": 0.0145
          },
          {
            "term": "comments",
            "score": 0.0145
          },
          {
            "term": "found",
            "score": 0.0145
          },
          {
            "term": "pdf",
            "score": 0.0145
          },
          {
            "term": "additional",
            "score": 0.0145
          },
          {
            "term": "correct",
            "score": 0.0145
          }
        ],
        "strengths": [
          "ai/share/9305bd53-16e3-423c-b2de-143974dab634\n\nOverall Summary:\n\nAccuracy: Claude was able to get all but 2 of the questions correct on the first try",
          "Despite these mistakes, it only took an additional comment from me to steer the model to the correct response",
          "Explanations: Claude made their process of solving the questions very clear, and in my opinion explained its answers better than the answer key"
        ],
        "weaknesses": [
          "The questions it got incorrect were Q1b and Q5b",
          "For Q1b, it made an error when doing some math calculations, and for Q5b, it did not consider loading the activations for layers 5 and 10",
          "A notable case of this is Q4c, where Claude identifies the need to use the chain rule to solve the question, which the answer key glosses over since the gradient of mu in that problem was just the ide"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Sonnet 4.5 on HW3",
            "author": "E Harrison",
            "url": "https://edstem.org/us/courses/84647/discussion/7353091",
            "snippet": "Below is my report on using Claude's Sonnet 4.5 model to solve the written questions to Homework 3. I have also provided a link to the original conversation I had with Claude. Question-specific commen..."
          }
        ]
      },
      "GPT-4o": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "homework",
            "score": 0.0376
          },
          {
            "term": "them",
            "score": 0.0301
          },
          {
            "term": "understand",
            "score": 0.0301
          },
          {
            "term": "answer",
            "score": 0.0226
          },
          {
            "term": "solutions",
            "score": 0.0226
          },
          {
            "term": "interesting",
            "score": 0.0226
          },
          {
            "term": "images",
            "score": 0.0226
          },
          {
            "term": "mathematical",
            "score": 0.015
          },
          {
            "term": "see",
            "score": 0.015
          },
          {
            "term": "one",
            "score": 0.015
          },
          {
            "term": "then",
            "score": 0.015
          },
          {
            "term": "question",
            "score": 0.015
          },
          {
            "term": "ask",
            "score": 0.015
          },
          {
            "term": "previously",
            "score": 0.015
          },
          {
            "term": "models",
            "score": 0.015
          }
        ],
        "strengths": [
          "For Homework 3, this including both mathematical solutions as well as text answers, so it was interesting to see how 4o handled them",
          "Then, I give it the image myself and ask it if the answer it provided previously is still good, or needs changing",
          "With each trial that goes wrong, they need more and more handholding to get on the right track"
        ],
        "weaknesses": [
          "I wanted to test this in a few different ways, so I initially ask 4o to just solve the problem",
          "Similar to what I've experienced previously, I find that these models get worse and worse if they fail the first time",
          "With each trial that goes wrong, they need more and more handholding to get on the right track",
          "In fact, the question it got wrong the first try was one of the simpler ones"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: GPT-4o on HW3",
            "author": "Mihir Rao",
            "url": "https://edstem.org/us/courses/84647/discussion/7419069",
            "snippet": "I worked on getting GPT-4o to answer non-coding parts of the homework. For Homework 3, this including both mathematical solutions as well as text answers, so it was interesting to see how 4o handled t..."
          }
        ]
      },
      "Gemini": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0408
          },
          {
            "term": "fast",
            "score": 0.0272
          },
          {
            "term": "question",
            "score": 0.0272
          },
          {
            "term": "questions",
            "score": 0.0204
          },
          {
            "term": "interpretation",
            "score": 0.0204
          },
          {
            "term": "research",
            "score": 0.0204
          },
          {
            "term": "step",
            "score": 0.0136
          },
          {
            "term": "solution",
            "score": 0.0136
          },
          {
            "term": "annotated",
            "score": 0.0136
          },
          {
            "term": "produced",
            "score": 0.0136
          },
          {
            "term": "incorrect",
            "score": 0.0136
          },
          {
            "term": "assumptions",
            "score": 0.0136
          },
          {
            "term": "answers",
            "score": 0.0136
          },
          {
            "term": "figure",
            "score": 0.0136
          },
          {
            "term": "figures",
            "score": 0.0136
          }
        ],
        "strengths": [
          "I used Gemini Fast to complete the written questions for Homework 3",
          "Overall, Gemini Fast performed well",
          "It solved most derivations on the first attempt, produced clean LaTeX, and often gave explanations as good as or better than the staff solution",
          "Thus, Gemini Fast is excellent for computational and mathematical deep-learning questions, but less reliable for conceptual reasoning that depends on precise assumptions or figure-based interpretation"
        ],
        "weaknesses": [
          "However, I observed two consistent weaknesses:\n\nIncorrect assumptions leading to wrong solutions",
          "Gemini sometimes committed early to an interpretation that wasn’t implied by the problem, and the resulting chain-of-thought led to confident but incorrect answers (e",
          "Hallucination on research-figure interpretation (Question 3)"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini Fast on Homework 3",
            "author": "Nazar Ospanov",
            "url": "https://edstem.org/us/courses/84647/discussion/7427400",
            "snippet": "I used Gemini Fast to complete the written questions for Homework 3. As in earlier evaluations, I instructed the model to restate each question, give a step-by-step solution, and identify uncertaintie..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "rms",
            "score": 0.0347
          },
          {
            "term": "chatgpt",
            "score": 0.027
          },
          {
            "term": "matrix",
            "score": 0.027
          },
          {
            "term": "update",
            "score": 0.0232
          },
          {
            "term": "norm",
            "score": 0.0232
          },
          {
            "term": "able",
            "score": 0.0232
          },
          {
            "term": "correct",
            "score": 0.0154
          },
          {
            "term": "correctly",
            "score": 0.0154
          },
          {
            "term": "reasoning",
            "score": 0.0154
          },
          {
            "term": "layer",
            "score": 0.0154
          },
          {
            "term": "seemed",
            "score": 0.0154
          },
          {
            "term": "still",
            "score": 0.0116
          },
          {
            "term": "order",
            "score": 0.0116
          },
          {
            "term": "vector",
            "score": 0.0116
          },
          {
            "term": "derive",
            "score": 0.0116
          }
        ],
        "strengths": [
          "Executive Summary\n\nI used ChatGPT 5 to interactively engage with HW 3's problems and get it to get to the correct answer",
          "1: Maximal Update Parameterization: \n\nSuccessfully one shotted, reasoning good",
          "It defined it as if the matrix was unrolled into a vector and then the RMS norm of that flattened vector was taken, \n\n$$\\frac{\\left\\lVert W\\right\\rVert_F}{\\sqrt{n_{l-1}n_l}}$$\n\nAs a result, it used th",
          "4: Policy Gradient and the Reparameterization Gradient Estimator:\n\n Overall, ChatGPT5 seemed to provide comprehensive reasoning (and more additional information/context) and got to the correct solutio",
          "Other than that, it was able to provide sound reasoning and the correct answers for the rest of the problem\n\nOverall:\nChatGPT seemed to provide comprehensive reasoning for every problem (sometimes an "
        ],
        "weaknesses": [
          "It defined it as if the matrix was unrolled into a vector and then the RMS norm of that flattened vector was taken, \n\n$$\\frac{\\left\\lVert W\\right\\rVert_F}{\\sqrt{n_{l-1}n_l}}$$\n\nAs a result, it used th",
          "While it made this error, ChatGPT 5 was still able to correctly derive the upper bounds in Desideratum 1 (because it didn't use the RMS norms of hidden layer vector/update or weight matrix/update for ",
          "After I reiterated the problem statement, it was easily able to answer and explain the total number of loadmems as 10",
          "Other than that, it was able to provide sound reasoning and the correct answers for the rest of the problem\n\nOverall:\nChatGPT seemed to provide comprehensive reasoning for every problem (sometimes an "
        ],
        "representative_posts": [
          {
            "title": "Special Participation A - HW 3 ChatGPT 5",
            "author": "Iana Lin",
            "url": "https://edstem.org/us/courses/84647/discussion/7111658",
            "snippet": "Executive Summary\n\nI used ChatGPT 5 to interactively engage with HW 3's problems and get it to get to the correct answer. While this wasn't the first time I've interacted with ChatGPT 5 for Deep Learn..."
          }
        ]
      },
      "Claude Opus": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0275
          },
          {
            "term": "problem",
            "score": 0.0275
          },
          {
            "term": "https",
            "score": 0.0183
          },
          {
            "term": "claude",
            "score": 0.0183
          },
          {
            "term": "problems",
            "score": 0.0183
          },
          {
            "term": "solution",
            "score": 0.0183
          },
          {
            "term": "without",
            "score": 0.0183
          },
          {
            "term": "derived",
            "score": 0.0183
          },
          {
            "term": "general",
            "score": 0.0183
          },
          {
            "term": "matrix",
            "score": 0.0183
          },
          {
            "term": "paper",
            "score": 0.0183
          },
          {
            "term": "llm",
            "score": 0.0092
          },
          {
            "term": "trace",
            "score": 0.0092
          },
          {
            "term": "share",
            "score": 0.0092
          },
          {
            "term": "annotated",
            "score": 0.0092
          }
        ],
        "strengths": [
          "Performance Overview: The model demonstrated a 100% Zero-Shot Success Rate across all problems, consistently matching or exceeding the rigor of the solution manual without hallucinations and without a"
        ],
        "weaknesses": [
          "Problem 1, Derived the full matrix math for SignGD rather than relying on the scalar hint",
          "Autonomous Context Synthesis: It successfully identified and interpreted external research contexts (referencing specific literature)\n\nProblem 3, referenced Tensor Programs paper, Spectral Condition P"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Opus 4.5 on HW 3",
            "author": "Anshul Verma",
            "url": "https://edstem.org/us/courses/84647/discussion/7428265",
            "snippet": "LLM Trace: https://claude.ai/share/b45ee84e-7009-436d-9b72-47ec844d083c\n\nAnnotated Log: https://drive.google.com/file/d/1TCL7ETF4Z27TknURe5f0fIOvSesCb2G/view?usp=sharing\n\nI audited Claude Opus 4.5 on ..."
          }
        ]
      }
    },
    "HW0": {
      "Unknown": {
        "post_count": 8,
        "top_terms": [
          {
            "term": "reasoning",
            "score": 0.0183
          },
          {
            "term": "model",
            "score": 0.0172
          },
          {
            "term": "correct",
            "score": 0.0137
          },
          {
            "term": "deepseek",
            "score": 0.0126
          },
          {
            "term": "question",
            "score": 0.0103
          },
          {
            "term": "steps",
            "score": 0.0103
          },
          {
            "term": "answer",
            "score": 0.008
          },
          {
            "term": "step",
            "score": 0.008
          },
          {
            "term": "solution",
            "score": 0.008
          },
          {
            "term": "here",
            "score": 0.008
          },
          {
            "term": "https",
            "score": 0.008
          },
          {
            "term": "prompt",
            "score": 0.0069
          },
          {
            "term": "its",
            "score": 0.0069
          },
          {
            "term": "chat",
            "score": 0.0069
          },
          {
            "term": "com",
            "score": 0.0069
          }
        ],
        "strengths": [
          "Initial prompt: \n\n\"You are being evaluated on how well a modern LLM can solve questions 2, 3, 4, and 5 of the attached homework assignment",
          "Always show your complete reasoning process",
          "Executive summary:\n\nDeepseek produced correct mathematical derivations across all early questions (Q2–Q3)\n\nIn the initial responses, it followed the required reasoning-step structure I defined in the ",
          "After receiving a follow-up prompt with explicit reminders, Deepseek returned to the correct structured format and successfully completed the rest of the assignment",
          "For the parts of Q5 where a graph would illustrate the point better, I asked it to describe the graph to reasonable success"
        ],
        "weaknesses": [
          "The omissions did not lead to incorrect results, so this might indicate the model’s tendency to compress reasoning as output length increases",
          "usp=sharing\n\nExecutive Summary:\nFrom my observations, at least for DeepSeek, the model tends to capture most of the details of the problem-solving process within its internal reasoning",
          "This issue is visible later in the annotated log, where the model fails to consider multiple possible cases for a single question",
          "The main issue is that it often skips steps, which is especially problematic for problems where you ask the model to show that A = B rather than just solve a problem, since the key is in the detailed ",
          "Finally, it struggled greatly with “5"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Grok on HW0",
            "author": "Tianyu Gu",
            "url": "https://edstem.org/us/courses/84647/discussion/7162279",
            "snippet": "For the special participation A on HW0, I use Grok to address the non-coding analytical components (problems 2–5). The performance of Grok really impressed me, almost all questions are one-shot correc..."
          },
          {
            "title": "Special Participation A: HW 0 non-coding solution from GPT5-Think",
            "author": "Zimu Wang",
            "url": "https://edstem.org/us/courses/84647/discussion/7409877",
            "snippet": "I guided GPT5-Think for the solutions of non-coding part of HW0. Aspired by the tech report from DeepSeek, when guiding super powerful thinking model, we should use zero-shot prompt with no examples b..."
          },
          {
            "title": "Special participation A: Kimi on HW0",
            "author": "ZhaoRui Qu",
            "url": "https://edstem.org/us/courses/84647/discussion/7399196",
            "snippet": "For Special participation A, I used Kimi on the writing part of HW0. Overall, it was useful, but it also revealed several limitations. Kimi is generally good at recognizing high-level patterns and giv..."
          }
        ]
      },
      "GPT-4o": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "wrong",
            "score": 0.0353
          },
          {
            "term": "part",
            "score": 0.0353
          },
          {
            "term": "solving",
            "score": 0.0235
          },
          {
            "term": "using",
            "score": 0.0235
          },
          {
            "term": "getting",
            "score": 0.0235
          },
          {
            "term": "right",
            "score": 0.0235
          },
          {
            "term": "solution",
            "score": 0.0235
          },
          {
            "term": "instead",
            "score": 0.0235
          },
          {
            "term": "question",
            "score": 0.0235
          },
          {
            "term": "get",
            "score": 0.0235
          },
          {
            "term": "correct",
            "score": 0.0235
          },
          {
            "term": "intuition",
            "score": 0.0235
          },
          {
            "term": "even",
            "score": 0.0235
          },
          {
            "term": "problem",
            "score": 0.0235
          },
          {
            "term": "sometimes",
            "score": 0.0235
          }
        ],
        "strengths": [
          "getting the right expression but solving for the wrong term, such as the full least squares solution instead of just the transformation matrix",
          "It struggled very hard on the subparts of part (b) of question 5, struggling to get the correct direction for the slope shift correctly",
          "(for example part (ii), relationship between b and w/x is correct instead of b and wx)",
          "It also seems to sometimes get the solution right but omit detailed justification (unless prompted), such as for part (d) of question 5"
        ],
        "weaknesses": [
          "getting the right expression but solving for the wrong term, such as the full least squares solution instead of just the transformation matrix",
          "It struggled very hard on the subparts of part (b) of question 5, struggling to get the correct direction for the slope shift correctly",
          "It keeps getting it wrong even after few shot tips, and even when I tell it the answer of the problem it sometimes using the wrong intuition to justify the problem"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: GPT-4o on Hw0",
            "author": "Aaron Zheng",
            "url": "https://edstem.org/us/courses/84647/discussion/7424515",
            "snippet": "Below is my report on solving non-coding related problems of Homework 0 using GPT4o. This is the pdf of the transcript. \n\n\n\nThere are some situations when GPT4o made some small minor syntactical mista..."
          }
        ]
      },
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0564
          },
          {
            "term": "able",
            "score": 0.0251
          },
          {
            "term": "elbow",
            "score": 0.0219
          },
          {
            "term": "question",
            "score": 0.0188
          },
          {
            "term": "algebra",
            "score": 0.0157
          },
          {
            "term": "right",
            "score": 0.0157
          },
          {
            "term": "example",
            "score": 0.0125
          },
          {
            "term": "one",
            "score": 0.0125
          },
          {
            "term": "questions",
            "score": 0.0125
          },
          {
            "term": "here",
            "score": 0.0094
          },
          {
            "term": "its",
            "score": 0.0094
          },
          {
            "term": "sign",
            "score": 0.0094
          },
          {
            "term": "instead",
            "score": 0.0094
          },
          {
            "term": "cases",
            "score": 0.0094
          },
          {
            "term": "left",
            "score": 0.0094
          }
        ],
        "strengths": [
          "TL;DR: Gemini was strong on the “mechanical” math (clean chain rule + indicator notation, solid ridge/SVD manipulations, decent linear-algebra bookkeeping), but its main weakness was qualitative/sign ",
          "What it did well: used the 1ϕ(x)>0​ indicator instead of messy piecewise cases, and matched the staff-style update notation w′,b′,e′",
          "Where it slipped: for case (ii), it said “elbow shifts right,” but the elbow can move left or right depending on bias + step size",
          "Clear wrong answer: for (iii) it concluded the elbow “moves left” as the expression gets larger, but the expression is negative so becoming “less negative” means it actually moves right",
          "Even when it got the final direction right: its proof sketch ended with a bogus condition like w>bx"
        ],
        "weaknesses": [
          "Clear wrong answer: for (iii) it concluded the elbow “moves left” as the expression gets larger, but the expression is negative so becoming “less negative” means it actually moves right",
          "Questions where gemini went wrong:\n\nGemini went wrong in 5 b ii"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 2.5 Flash on HW0",
            "author": "Jason Trinh",
            "url": "https://edstem.org/us/courses/84647/discussion/7374016",
            "snippet": "Hey guys — I used Gemini Flash 2.5 for the non-coding parts of HW0, and here’s the quick verdict.\n\nTL;DR: Gemini was strong on the “mechanical” math (clean chain rule + indicator notation, solid ridge..."
          },
          {
            "title": "Special Participation A: Gemini 3 Pro on HW 0",
            "author": "Ayush Goel",
            "url": "https://edstem.org/us/courses/84647/discussion/7407894",
            "snippet": "Link to the chat: https://gemini.google.com/share/89b0a83f691b\n\n\nI ran HW 0 through gemini and it was able to one-shot most of the homework. The PDF is annotated with my thoughts about specific questi..."
          }
        ]
      },
      "Claude Sonnet": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0462
          },
          {
            "term": "used",
            "score": 0.0308
          },
          {
            "term": "correctly",
            "score": 0.0308
          },
          {
            "term": "basic",
            "score": 0.0308
          },
          {
            "term": "plots",
            "score": 0.0308
          },
          {
            "term": "solution",
            "score": 0.0308
          },
          {
            "term": "conversation",
            "score": 0.0154
          },
          {
            "term": "https",
            "score": 0.0154
          },
          {
            "term": "share",
            "score": 0.0154
          },
          {
            "term": "sonnet",
            "score": 0.0154
          },
          {
            "term": "solve",
            "score": 0.0154
          },
          {
            "term": "problems",
            "score": 0.0154
          },
          {
            "term": "written",
            "score": 0.0154
          },
          {
            "term": "summary",
            "score": 0.0154
          },
          {
            "term": "able",
            "score": 0.0154
          }
        ],
        "strengths": [],
        "weaknesses": [
          "But I don't think this is a big issue, since specific forms for these were not specified"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Sonnet 4.5 on HW0",
            "author": "Justin Yang",
            "url": "https://edstem.org/us/courses/84647/discussion/7446043",
            "snippet": "Conversation: https://claude.ai/share/dd45cf31-778b-4d1c-9096-a304ad8c8247 \n\nI used Claude 4.5 sonnet to solve problems 2-5 for HW0 written. \n\nSummary: \n\nClaude was able to answer all the questions fu..."
          }
        ]
      },
      "Claude Opus": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.087
          },
          {
            "term": "markdown",
            "score": 0.029
          },
          {
            "term": "file",
            "score": 0.029
          },
          {
            "term": "generated",
            "score": 0.029
          },
          {
            "term": "its",
            "score": 0.029
          },
          {
            "term": "special",
            "score": 0.0145
          },
          {
            "term": "participation",
            "score": 0.0145
          },
          {
            "term": "used",
            "score": 0.0145
          },
          {
            "term": "opus",
            "score": 0.0145
          },
          {
            "term": "extended",
            "score": 0.0145
          },
          {
            "term": "thinking",
            "score": 0.0145
          },
          {
            "term": "enabled",
            "score": 0.0145
          },
          {
            "term": "overall",
            "score": 0.0145
          },
          {
            "term": "impressed",
            "score": 0.0145
          },
          {
            "term": "work",
            "score": 0.0145
          }
        ],
        "strengths": [
          "I initiated the task with a simple prompt and fully expected that I would have to nudge and prod Claude in the right direction, but Claude picked up on my intention easily and was able to correctly on"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A HW 0 with Claude Opus 4.5 (Extended Thinking)",
            "author": "Talon Meyer",
            "url": "https://edstem.org/us/courses/84647/discussion/7451517",
            "snippet": "For Special Participation A, I used Claude Opus 4.5 with Extended Thinking enabled on HW 0. Overall, I was very impressed with Claude's work. I initiated the task with a simple prompt and fully expect..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "problems",
            "score": 0.0909
          },
          {
            "term": "tested",
            "score": 0.0455
          },
          {
            "term": "chatgpt",
            "score": 0.0455
          },
          {
            "term": "ability",
            "score": 0.0455
          },
          {
            "term": "solve",
            "score": 0.0455
          },
          {
            "term": "questions",
            "score": 0.0455
          },
          {
            "term": "general",
            "score": 0.0455
          },
          {
            "term": "able",
            "score": 0.0455
          },
          {
            "term": "one",
            "score": 0.0455
          },
          {
            "term": "shot",
            "score": 0.0455
          },
          {
            "term": "while",
            "score": 0.0455
          },
          {
            "term": "explaining",
            "score": 0.0455
          },
          {
            "term": "its",
            "score": 0.0455
          },
          {
            "term": "reasoning",
            "score": 0.0455
          },
          {
            "term": "likely",
            "score": 0.0455
          }
        ],
        "strengths": [],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT-5.1 on HW0",
            "author": "Alena Chao",
            "url": "https://edstem.org/us/courses/84647/discussion/7414931",
            "snippet": "I tested ChatGPT's ability to solve HW0 questions 2-5. In general, it was able to one-shot the problems while explaining its reasoning, most likely because many of the problems review fundamental ML/m..."
          }
        ]
      }
    },
    "HW11": {
      "Unknown": {
        "post_count": 7,
        "top_terms": [
          {
            "term": "model",
            "score": 0.031
          },
          {
            "term": "correct",
            "score": 0.0168
          },
          {
            "term": "questions",
            "score": 0.0162
          },
          {
            "term": "question",
            "score": 0.0162
          },
          {
            "term": "one",
            "score": 0.0148
          },
          {
            "term": "answer",
            "score": 0.0115
          },
          {
            "term": "answers",
            "score": 0.0101
          },
          {
            "term": "problem",
            "score": 0.0094
          },
          {
            "term": "its",
            "score": 0.0088
          },
          {
            "term": "prompt",
            "score": 0.0074
          },
          {
            "term": "problems",
            "score": 0.0061
          },
          {
            "term": "conceptual",
            "score": 0.0061
          },
          {
            "term": "part",
            "score": 0.0061
          },
          {
            "term": "prompting",
            "score": 0.0061
          },
          {
            "term": "llm",
            "score": 0.0061
          }
        ],
        "strengths": [
          "shareId=99965dbe-7cb8-4b68-a181-f66d29022d0f\n\nThe model was reasonably good at one-shotting the problems, but it’s still far from trustworthy",
          "It was quite good at high-level conceptual explanations and even symbolic derivations, but it made small but fatal mathematical and numerical mistakes, and almost never flagged its own uncertainty",
          "Getting to fully correct answers usually required some active steering",
          "One-shot correctness:\n About 70% of individual sub-questions were essentially correct on the first try, especially the more conceptual ones",
          "Throughout the process, it often gave good reasoning behind its answers"
        ],
        "weaknesses": [
          "After the incorrect portion was pointed out, the model did a good job refocusing on that specific portion and fixing the issue, whether it was a mathematical error or a conceptual one",
          "However this required the user to know the ground truth answer to be able to tell the model where things went wrong, which was not feasible sometimes",
          "When something was incorrect, I would point out a specific part where things went wrong and ask it to redo that portion or rewrite the answer with that constraint in mind",
          "Soft-Prompting Language Models, where the model one-shot all the sub-questions with clear, coherent explanations)\n\nErrors and hallucinations:\n\nThe main issue is the model gives confident, mostly corre",
          "For example, in 2(a), the model mentioned similarity with a bag-of-words model, while the problem was asking about a token-by-token model"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A:  HW11 using GPT 5.1 Thinking (Extended)",
            "author": "Yu-Jen Lin",
            "url": "https://edstem.org/us/courses/84647/discussion/7452161",
            "snippet": "Executive Summary\n\nChatGPT did very well on all of these homework questions. It gave correct answers with clear math steps and simple explanations. For the LoRA, transformer interpretability, and soft..."
          },
          {
            "title": "Special Participation A: Mistral Le Chat on HW11 (Without Reasoning or Thinking Mode)",
            "author": "Akshaan Ahuja",
            "url": "https://edstem.org/us/courses/84647/discussion/7445765",
            "snippet": "I worked through HW 11 Problems 1, 2, 5, and 6, using Mistral’s Le Chat model. After introducing the assignment by outlining the deep learning themes it would focus on (LoRA, soft prompting, transform..."
          },
          {
            "title": "Special Participation A: KIMI K2 on HW 11 Written Questions",
            "author": "Qicheng Zhu",
            "url": "https://edstem.org/us/courses/84647/discussion/7408383",
            "snippet": "Model Tested: KIMI K2\n\nDomain: Homework11 -- LORA & Transformer & Mechanistic Interpretability\n\nPerformance Overview\n\nFor most question, KIMI K2 answers perfectly. However, there are some errors becau..."
          }
        ]
      },
      "Gemini Flash": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0628
          },
          {
            "term": "question",
            "score": 0.029
          },
          {
            "term": "questions",
            "score": 0.0242
          },
          {
            "term": "step",
            "score": 0.0193
          },
          {
            "term": "better",
            "score": 0.0193
          },
          {
            "term": "then",
            "score": 0.0193
          },
          {
            "term": "gemini",
            "score": 0.0145
          },
          {
            "term": "performance",
            "score": 0.0145
          },
          {
            "term": "told",
            "score": 0.0145
          },
          {
            "term": "context",
            "score": 0.0145
          },
          {
            "term": "solutions",
            "score": 0.0145
          },
          {
            "term": "asked",
            "score": 0.0145
          },
          {
            "term": "got",
            "score": 0.0145
          },
          {
            "term": "wrong",
            "score": 0.0145
          },
          {
            "term": "prompt",
            "score": 0.0097
          }
        ],
        "strengths": [
          "I also told the model that it should be able to correct itself when needed",
          "Analysis: The model starts off a bit slow on the very first question (about ideas to adjust LoRA to get better performance)",
          "The solutions given by the model are arguably better than the actual HW solutions in some cases",
          "Interestingly, towards the end, the model starts trying to reattempt questions that it had already solved",
          "5 Flash may be better leveraged for individual questions, rather than for the entire HW assignment, as context length seems to be an issue here"
        ],
        "weaknesses": [
          "When I asked it do then do 6a, it initially got the question slightly wrong due to some sort of apparent reading issue (I provided it with an image that it seemed to mis-extract the text of)",
          "5 Flash may be better leveraged for individual questions, rather than for the entire HW assignment, as context length seems to be an issue here",
          "It did the same thing when asked to solve 6b with an image, as it tried to then solve problem 2",
          "On 6c, it actually got two of the T/F questions wrong",
          "Even when re-prompted with the context again (just in case this was the cause once again, as with 6a), it got them wrong"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Using Gemini Flash 2.5 on HW11",
            "author": "Aaryan Chandna",
            "url": "https://edstem.org/us/courses/84647/discussion/7380526",
            "snippet": "Trace: https://gemini.google.com/share/2e206d7da648\n\nMath + T/F Question Zero-Shot Performance: 13/15.\n\nPrompt Structure: I told the model that it was a DL assistant for me. I gave the model the conce..."
          }
        ]
      },
      "Gemini Pro": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "mathematical",
            "score": 0.0296
          },
          {
            "term": "model",
            "score": 0.0296
          },
          {
            "term": "gemini",
            "score": 0.0222
          },
          {
            "term": "pro",
            "score": 0.0222
          },
          {
            "term": "questions",
            "score": 0.0222
          },
          {
            "term": "answers",
            "score": 0.0222
          },
          {
            "term": "provided",
            "score": 0.0222
          },
          {
            "term": "solve",
            "score": 0.0148
          },
          {
            "term": "task",
            "score": 0.0148
          },
          {
            "term": "since",
            "score": 0.0148
          },
          {
            "term": "intuitive",
            "score": 0.0148
          },
          {
            "term": "calculations",
            "score": 0.0148
          },
          {
            "term": "performance",
            "score": 0.0148
          },
          {
            "term": "satisfactory",
            "score": 0.0148
          },
          {
            "term": "easy",
            "score": 0.0148
          }
        ],
        "strengths": [
          "After comparing with the standard answers provided by the teaching assistant, we judged that Gemini 3 Pro achieved a 100% zero-shot accuracy",
          "All qualitative answers and intuitive understandings were reasonable and correct, the mathematical calculations were error-free, and for the few proof-based questions, it provided sufficient formula s",
          "The formatting was clear and the language was easy to understand"
        ],
        "weaknesses": [
          "All qualitative answers and intuitive understandings were reasonable and correct, the mathematical calculations were error-free, and for the few proof-based questions, it provided sufficient formula s"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini Pro 3 on HW 11",
            "author": "Xuanlin Mao",
            "url": "https://edstem.org/us/courses/84647/discussion/7403245",
            "snippet": "For this special participation, I used Gemini Pro 3 to solve the written portions of homework 11.\n\nIn this task, since there were no complex mathematical proofs or derivations involved, the vast major..."
          }
        ]
      },
      "Llama 4": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "problem",
            "score": 0.0566
          },
          {
            "term": "model",
            "score": 0.0314
          },
          {
            "term": "part",
            "score": 0.0314
          },
          {
            "term": "answer",
            "score": 0.0252
          },
          {
            "term": "llama",
            "score": 0.0189
          },
          {
            "term": "solve",
            "score": 0.0189
          },
          {
            "term": "questions",
            "score": 0.0189
          },
          {
            "term": "pdf",
            "score": 0.0157
          },
          {
            "term": "question",
            "score": 0.0157
          },
          {
            "term": "calculation",
            "score": 0.0126
          },
          {
            "term": "performed",
            "score": 0.0126
          },
          {
            "term": "correct",
            "score": 0.0126
          },
          {
            "term": "solutions",
            "score": 0.0126
          },
          {
            "term": "step",
            "score": 0.0126
          },
          {
            "term": "context",
            "score": 0.0126
          }
        ],
        "strengths": [
          "\"\n\nOverall, the model performed well on the proof-based questions",
          "Problem 1: In this problem, Llama was tasked with analyzing LoRA and possible adjustments, as well as how they would affect training",
          "For the rest of the parts, it generally gave the correct answer",
          "For part i, it got the correct answer, but I noticed that it did not solve the questions in the same manner a student would",
          "It generally gave correct answers, but they were not as verbose as the solutions"
        ],
        "weaknesses": [
          "First, output the exact problem statement from the PDF, then give a full explanation of how you reached your answer, along with the answer",
          "I was surprised to find that it did not hallucinate the problem question, giving it back to me accurately word for word",
          "Problem 1: In this problem, Llama was tasked with analyzing LoRA and possible adjustments, as well as how they would affect training",
          "Problem 2: For this problem, I asked the model to solve each question part by part",
          "It would also not highlight what is the most important takeaway from the problem/what to do know generally as the solutions pdf does (ex"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Llama 4 Maverick on HW 11",
            "author": "Hiya Shah",
            "url": "https://edstem.org/us/courses/84647/discussion/7427874",
            "snippet": "I guided Llama 4 Maverick to solve the non-coding questions for Homework 11, which was largely about model finetuning, LoRA, and Fermi Estimation and memory calculation for large-scale DL models. I pr..."
          }
        ]
      }
    },
    "HW9": {
      "Gemini Pro": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "one",
            "score": 0.0381
          },
          {
            "term": "gemini",
            "score": 0.0323
          },
          {
            "term": "model",
            "score": 0.0293
          },
          {
            "term": "shot",
            "score": 0.0264
          },
          {
            "term": "questions",
            "score": 0.0235
          },
          {
            "term": "question",
            "score": 0.0235
          },
          {
            "term": "able",
            "score": 0.0176
          },
          {
            "term": "homework",
            "score": 0.0147
          },
          {
            "term": "instead",
            "score": 0.0147
          },
          {
            "term": "correct",
            "score": 0.0147
          },
          {
            "term": "coding",
            "score": 0.0117
          },
          {
            "term": "problems",
            "score": 0.0117
          },
          {
            "term": "well",
            "score": 0.0117
          },
          {
            "term": "transformer",
            "score": 0.0117
          },
          {
            "term": "everything",
            "score": 0.0088
          }
        ],
        "strengths": [
          "I used Gemini Pro 3 (With Thinking) to complete the non-coding portion of Homework 9",
          "My reasoning for doing this is so that when the model makes a mistake early on, I can correct it, and that mistake won't carry on to future problems",
          "Overall, the model did very well on this homework, and I was surprised by how accurate it answered everything",
          "I didn't have too much prompting in each one of my prompts (I just told Gemini to complete the problem), but it gives a lot of explanation, even for the simple problems, without me asking it to",
          "The vast majority of times, it was able to one-shot the question and get it correct"
        ],
        "weaknesses": [
          "I didn't have too much prompting in each one of my prompts (I just told Gemini to complete the problem), but it gives a lot of explanation, even for the simple problems, without me asking it to",
          "Q1) This problem was relatively straightforward, and the model basically got everything correct",
          "Many times when I thought it was wrong, its answer was actually correct, just in a different form",
          "Perhaps the result would be slightly worse if I instead just gave it the pdf and asked it to solve all the problems directly, but that can be left for future testing"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini Pro 3 (With Thinking) on HW 9",
            "author": "Joshua Lu",
            "url": "https://edstem.org/us/courses/84647/discussion/7424085",
            "snippet": "I used Gemini Pro 3 (With Thinking) to complete the non-coding portion of Homework 9.\n\nHere is the trace (without annotations): https://gemini.google.com/share/deb95c933e37\n\nHere is the trace with ann..."
          }
        ]
      },
      "Unknown": {
        "post_count": 8,
        "top_terms": [
          {
            "term": "question",
            "score": 0.0252
          },
          {
            "term": "model",
            "score": 0.0181
          },
          {
            "term": "problem",
            "score": 0.0181
          },
          {
            "term": "able",
            "score": 0.0129
          },
          {
            "term": "grok",
            "score": 0.0116
          },
          {
            "term": "one",
            "score": 0.011
          },
          {
            "term": "answer",
            "score": 0.011
          },
          {
            "term": "correct",
            "score": 0.011
          },
          {
            "term": "questions",
            "score": 0.011
          },
          {
            "term": "its",
            "score": 0.0103
          },
          {
            "term": "problems",
            "score": 0.009
          },
          {
            "term": "shot",
            "score": 0.0084
          },
          {
            "term": "coding",
            "score": 0.0077
          },
          {
            "term": "prompt",
            "score": 0.0077
          },
          {
            "term": "parts",
            "score": 0.0077
          }
        ],
        "strengths": [
          "I used Mistral Le Chat on HW9 (non-coding), and it achieved 99% accuracy, solving everything correctly on the first try with only one minor notation error",
          "This was done on the second prompt, as the first one tried to be a pedagogical aide by providing explanations without complete answers",
          "Prompts\nPrompt 1: \"Please complete all of the problems in this homework assignment",
          "\"\nMistral tried to be pedagogical, encouraging me to work through problems myself\n\nPrompt 2: \"Give me the detailed answers and explanations",
          "\"\nThis resulted in complete solutions with full derivations"
        ],
        "weaknesses": [
          "I used Mistral Le Chat on HW9 (non-coding), and it achieved 99% accuracy, solving everything correctly on the first try with only one minor notation error",
          "For this specific problem set, two things matter the most for accuracy: 1",
          "Prompt and context are very important because if instructions and context are not explicitly told, the model will make certain assumptions, and thus give incorrect answers",
          "For example in question 4, when not stated explicitly, the model thinks it is completing some code function instead of filling blanks for a written question, thus identifying the wrong blanks to fill",
          "I included screenshots of the problem statements, and my prompt for each problem followed the structure:\n\nFor each subpart, please: (1) restate the problem in your own words, (2) explain the main idea"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Qwen on HW9",
            "author": "Oliver Chen",
            "url": "https://edstem.org/us/courses/84647/discussion/7302906",
            "snippet": "For the special participation A on HW9, I use Qwen to solve the non-coding analytical components (problems 2–5). The performance was very strong -- almost all questions were very quickly solved by dir..."
          },
          {
            "title": "Special Participation A: Mistral on HW9 (non-coding)",
            "author": "Subhash Prasad",
            "url": "https://edstem.org/us/courses/84647/discussion/7450819",
            "snippet": "I used Mistral Le Chat on HW9 (non-coding), and it achieved 99% accuracy, solving everything correctly on the first try with only one minor notation error. This was done on the second prompt, as the f..."
          },
          {
            "title": "Special Participation A: DeepSeek-V3.2 on HW9 Non-Coding",
            "author": "Tyler Pham",
            "url": "https://edstem.org/us/courses/84647/discussion/7424852",
            "snippet": "I used DeepSeek-V3.2 without DeepThink mode (this was released Dec 1, and is not the same as DeepSeek-V3.2-Exp) on HW9 (Non-Coding). Overall, DeepSeek was able to one-shot most of the problems even wi..."
          }
        ]
      },
      "Gemini-Pro": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0833
          },
          {
            "term": "pro",
            "score": 0.0833
          },
          {
            "term": "used",
            "score": 0.0417
          },
          {
            "term": "performed",
            "score": 0.0417
          },
          {
            "term": "well",
            "score": 0.0417
          },
          {
            "term": "main",
            "score": 0.0417
          },
          {
            "term": "issues",
            "score": 0.0417
          },
          {
            "term": "small",
            "score": 0.0417
          },
          {
            "term": "details",
            "score": 0.0417
          },
          {
            "term": "arithmetics",
            "score": 0.0417
          },
          {
            "term": "needed",
            "score": 0.0417
          },
          {
            "term": "correction",
            "score": 0.0417
          },
          {
            "term": "especially",
            "score": 0.0417
          },
          {
            "term": "linear",
            "score": 0.0417
          },
          {
            "term": "algebra",
            "score": 0.0417
          }
        ],
        "strengths": [
          "I used Gemini-pro 3 on HW 9 and it performed very well",
          "The main issues with Gemini-pro 3 was the small details in arithmetics that needed correction especially in linear algebra and time/space complexity; however, the solutions were overwhelmingly correct"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini-Pro 3 on HW9",
            "author": "Shervin Goudarzi",
            "url": "https://edstem.org/us/courses/84647/discussion/7373861",
            "snippet": "I used Gemini-pro 3 on HW 9 and it performed very well. The main issues with Gemini-pro 3 was the small details in arithmetics that needed correction especially in linear algebra and time/space comple..."
          }
        ]
      },
      "Gemini": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0437
          },
          {
            "term": "question",
            "score": 0.0273
          },
          {
            "term": "think",
            "score": 0.0219
          },
          {
            "term": "solution",
            "score": 0.0219
          },
          {
            "term": "didn",
            "score": 0.0164
          },
          {
            "term": "one",
            "score": 0.0164
          },
          {
            "term": "paste",
            "score": 0.0164
          },
          {
            "term": "helped",
            "score": 0.0164
          },
          {
            "term": "used",
            "score": 0.0109
          },
          {
            "term": "coding",
            "score": 0.0109
          },
          {
            "term": "well",
            "score": 0.0109
          },
          {
            "term": "noticed",
            "score": 0.0109
          },
          {
            "term": "questions",
            "score": 0.0109
          },
          {
            "term": "code",
            "score": 0.0109
          },
          {
            "term": "formatting",
            "score": 0.0109
          }
        ],
        "strengths": [
          "Executive Summary:\n\nI used Gemini on the non-coding parts of HW 9, and evaluated where it did well and where it didn't",
          "Even with the formatting problems though, when it was a coding analysis question or there was a lot of context, Gemini still did very well, one shotting almost all the questions",
          "I think the only case where the solution was incorrect showed inconsistency within its own solution, providing the incorrect answer as the header and then showing work to derive the correct answer",
          "This was confusing, but it ultimately did come up with the correct solution",
          "I also noticed Gemini provided detailed explanations for each question, serving as a great conceptual recap on the topic"
        ],
        "weaknesses": [
          "I think it mainly failed where the formatting of the question was an issue, and that was just because of LaTeX copy/paste error",
          "I think the only case where the solution was incorrect showed inconsistency within its own solution, providing the incorrect answer as the header and then showing work to derive the correct answer",
          "This could also be, however, that I fed each problem in individually, allowing it to see all context for each question as it came up"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini (Fast) on HW 9",
            "author": "Divya Ramesh",
            "url": "https://edstem.org/us/courses/84647/discussion/7375514",
            "snippet": "Executive Summary:\n\nI used Gemini on the non-coding parts of HW 9, and evaluated where it did well and where it didn't. I noticed Gemini could mostly one-shot these questions, especially sections that..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "question",
            "score": 0.029
          },
          {
            "term": "model",
            "score": 0.0178
          },
          {
            "term": "homework",
            "score": 0.0156
          },
          {
            "term": "assignment",
            "score": 0.0156
          },
          {
            "term": "questions",
            "score": 0.0134
          },
          {
            "term": "one",
            "score": 0.0134
          },
          {
            "term": "chatgpt",
            "score": 0.0111
          },
          {
            "term": "solve",
            "score": 0.0111
          },
          {
            "term": "first",
            "score": 0.0111
          },
          {
            "term": "its",
            "score": 0.0111
          },
          {
            "term": "correct",
            "score": 0.0111
          },
          {
            "term": "them",
            "score": 0.0111
          },
          {
            "term": "solutions",
            "score": 0.0089
          },
          {
            "term": "into",
            "score": 0.0089
          },
          {
            "term": "didn",
            "score": 0.0089
          }
        ],
        "strengths": [
          "Executive Summary\n\nFor Homework 9 (all but prob 5), I worked through the written parts with ChatGPT to see how well it could solve the questions on the first try, how often it drifted or hallucinated,",
          "For most of the assignment, especially Questions 1 through 4, it did surprisingly well",
          "One thing that stood out was how consistently the model could jump straight into the right structure of the problem",
          "When it was right, it was very right, and the explanations were easy to follow",
          "Because of that, its first attempt at the feature map was missing the constant term and the linear terms, so that part drifted from the correct answer"
        ],
        "weaknesses": [
          "One thing that stood out was how consistently the model could jump straight into the right structure of the problem",
          "The main issue came up in Question 6",
          "Because of that, its first attempt at the feature map was missing the constant term and the linear terms, so that part drifted from the correct answer",
          "So the error wasn’t really a hallucination—more like it defaulted to a familiar formula without checking whether it matched what the homework meant",
          "Aside from that one slip, the model didn’t really hallucinate or spiral"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT o3 on HW 9",
            "author": "Tamzid Razzaque",
            "url": "https://edstem.org/us/courses/84647/discussion/7397817",
            "snippet": "Executive Summary\n\nFor Homework 9 (all but prob 5), I worked through the written parts with ChatGPT to see how well it could solve the questions on the first try, how often it drifted or hallucinated,..."
          },
          {
            "title": "Special Participation A: ChatGPT 5.1 Thinking for HW9",
            "author": "Carolyn Liu",
            "url": "https://edstem.org/us/courses/84647/discussion/7423454",
            "snippet": "I used ChatGPT’s 5.1 Thinking Model to do all the non-coding questions on HW9. I first told the model I was completing an assignment except question 5 (since that was a coding question) and wanted the..."
          }
        ]
      },
      "Claude Opus": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "question",
            "score": 0.0374
          },
          {
            "term": "claude",
            "score": 0.0267
          },
          {
            "term": "its",
            "score": 0.0267
          },
          {
            "term": "well",
            "score": 0.016
          },
          {
            "term": "answer",
            "score": 0.016
          },
          {
            "term": "work",
            "score": 0.016
          },
          {
            "term": "staff",
            "score": 0.016
          },
          {
            "term": "modifications",
            "score": 0.016
          },
          {
            "term": "function",
            "score": 0.016
          },
          {
            "term": "correct",
            "score": 0.016
          },
          {
            "term": "prompt",
            "score": 0.0107
          },
          {
            "term": "through",
            "score": 0.0107
          },
          {
            "term": "deep",
            "score": 0.0107
          },
          {
            "term": "about",
            "score": 0.0107
          },
          {
            "term": "single",
            "score": 0.0107
          }
        ],
        "strengths": [
          "To do so, I started by attaching the entire problem PDF, as well as the following initial prompt:  \n\n”Hi Claude",
          "” \n\nClaude was incredibly strong, one-shotting nearly every question",
          "I expected it to struggle with deep chains of algebra, and the occasional numerical calculation, but it did very well",
          "After re-prompting it to examine the split head function, Claude was able to make the correct modifications, finally resulting in what I believe is a fully correct answer",
          "Overall, Claude is very strong at all types of questions in this homework (whether it be filling in code, algebra, arithmetic, etc"
        ],
        "weaknesses": [
          "To do so, I started by attaching the entire problem PDF, as well as the following initial prompt:  \n\n”Hi Claude",
          "I expected it to struggle with deep chains of algebra, and the occasional numerical calculation, but it did very well",
          "It organized its thoughts well, answered each question thoroughly, and required a minimal hint to converge at the correct answer in the single case where it was incorrect"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Opus 4.5 on HW 9",
            "author": "Athul Krishnan",
            "url": "https://edstem.org/us/courses/84647/discussion/7423757",
            "snippet": "Hi everyone! \n\nFor Special Participation A, I evaluated Claude Opus 4.5 (Extended Thinking) on the non-coding parts of HW9! To do so, I started by attaching the entire problem PDF, as well as the foll..."
          }
        ]
      }
    },
    "HW07": {
      "Unknown": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0533
          },
          {
            "term": "part",
            "score": 0.0266
          },
          {
            "term": "thinking",
            "score": 0.0242
          },
          {
            "term": "able",
            "score": 0.0194
          },
          {
            "term": "problems",
            "score": 0.0194
          },
          {
            "term": "one",
            "score": 0.0169
          },
          {
            "term": "seconds",
            "score": 0.0169
          },
          {
            "term": "shot",
            "score": 0.0145
          },
          {
            "term": "question",
            "score": 0.0121
          },
          {
            "term": "its",
            "score": 0.0121
          },
          {
            "term": "solution",
            "score": 0.0121
          },
          {
            "term": "problem",
            "score": 0.0097
          },
          {
            "term": "staff",
            "score": 0.0097
          },
          {
            "term": "utilized",
            "score": 0.0073
          },
          {
            "term": "surprised",
            "score": 0.0073
          }
        ],
        "strengths": [
          "There was a minor question I had because the model didn’t fully clarify one step in its working, so I wanted to confirm that I was correct on this",
          "Q4\n\n(33 seconds of thinking) For part (a), the model got the correct accuracy numbers but wrong times, so I’m thinking it actually didn’t reference the blog post and instead just answered based on its",
          "maybe these numbers were part of its pretraining corpus)\n\nFor part (b), the model had a good takeaway and also explains with great specificity\n\nFor part (c), the model gave an answer that made me also",
          "Starting with the full context of the assignment, I prompted the model to solve each sub‑question sequentially, occasionally providing clarifications or corrections when needed—though such interventio",
          "I also noticed occasional inconsistencies in variable naming across steps, but the logical flow remained clear and correct"
        ],
        "weaknesses": [
          "Here are my specific notes per subquestion:\n\nQ3\n\n(1 minute 43 seconds of thinking + 13 seconds of thinking for clarification) The model was able to one shot the problem successfully, in a slightly dif",
          "Q4\n\n(33 seconds of thinking) For part (a), the model got the correct accuracy numbers but wrong times, so I’m thinking it actually didn’t reference the blog post and instead just answered based on its",
          "Q7\n\n(36 seconds of thinking) The model was able to zero shot all the problems, and was able to get both reasonable answers for part (a) even though the problem only asked for one",
          "Finally, when I once mistakenly referenced the wrong part of a problem, the model did not simply guess; instead, it recognized the inconsistency, inferred the likely intended question, and answered ac",
          "This demonstrated a welcome resistance to hallucination and an ability to maintain coherence even when the prompt was imperfect"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: GPT 5.1 Thinking on HW07",
            "author": "Jaewon Chang",
            "url": "https://edstem.org/us/courses/84647/discussion/7428314",
            "snippet": "I utilized GPT 5.1 Thinking on homework 7 (the written questions), and overall I was surprised by how easily the model was able to one shot all the problems. Here are my specific notes per subquestion..."
          },
          {
            "title": "Special Participation A: DeepSeek on HW07",
            "author": "Edward Zhang",
            "url": "https://edstem.org/us/courses/84647/discussion/7427939",
            "snippet": "In this chat, I utilized DeepSeek 3.2 in its default reasoning mode to work through the machine learning homework problems step by step. Starting with the full context of the assignment, I prompted th..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "chatgpt",
            "score": 0.0505
          },
          {
            "term": "non",
            "score": 0.0202
          },
          {
            "term": "theory",
            "score": 0.0202
          },
          {
            "term": "however",
            "score": 0.0202
          },
          {
            "term": "sometimes",
            "score": 0.0202
          },
          {
            "term": "understand",
            "score": 0.0202
          },
          {
            "term": "answer",
            "score": 0.0202
          },
          {
            "term": "https",
            "score": 0.0202
          },
          {
            "term": "com",
            "score": 0.0202
          },
          {
            "term": "assignment",
            "score": 0.0101
          },
          {
            "term": "tested",
            "score": 0.0101
          },
          {
            "term": "coding",
            "score": 0.0101
          },
          {
            "term": "questions",
            "score": 0.0101
          },
          {
            "term": "consistently",
            "score": 0.0101
          },
          {
            "term": "one",
            "score": 0.0101
          }
        ],
        "strengths": [
          "ChatGPT seemed to understand all the concepts very well"
        ],
        "weaknesses": [
          "It consistently one-shotted every problem",
          "I also attempted to input an incorrect answer into it about orthogonal initialization in RNNs guaranteeing non-vanishing gradients"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT 5 on HW07",
            "author": "Peyton Schales",
            "url": "https://edstem.org/us/courses/84647/discussion/7428812",
            "snippet": "For this assignment, I tested ChatGPT 5 on the non-coding theory questions. It consistently one-shotted every problem. However, its initial responses sometimes skipped important steps and  derivations..."
          }
        ]
      }
    },
    "HW5": {
      "ChatGPT": {
        "post_count": 4,
        "top_terms": [
          {
            "term": "chatgpt",
            "score": 0.0241
          },
          {
            "term": "problem",
            "score": 0.0206
          },
          {
            "term": "chat",
            "score": 0.0137
          },
          {
            "term": "dropout",
            "score": 0.0137
          },
          {
            "term": "https",
            "score": 0.0137
          },
          {
            "term": "com",
            "score": 0.0137
          },
          {
            "term": "gpt",
            "score": 0.0137
          },
          {
            "term": "input",
            "score": 0.0137
          },
          {
            "term": "powerful",
            "score": 0.0103
          },
          {
            "term": "question",
            "score": 0.0103
          },
          {
            "term": "text",
            "score": 0.0103
          },
          {
            "term": "solve",
            "score": 0.0103
          },
          {
            "term": "problems",
            "score": 0.0103
          },
          {
            "term": "use",
            "score": 0.0103
          },
          {
            "term": "set",
            "score": 0.0103
          }
        ],
        "strengths": [
          "The reasoning and explanation were also clear and easy to understand",
          "This might be influenced by the fact that CNN is already a well studied topic, and the model is well trained on similar problems",
          "I asked questions in a structured way—first “understand → summarize → derive → implement,” then “complete the code” with strict guardrails (no new helpers, inverted dropout, 0",
          ", offering high-level guidance instead of finished cells/files, or being imprecise about BN mode keys, I nudged the GPT (“complete the notebook/file,” “set bn_param['mode'],” “use inverted dropout and",
          "Edit: Detailed noted on the chat: https://drive"
        ],
        "weaknesses": [
          "It can also identify the difficulty of the problem, and use Chain of Thoughts to incrementally solve the problem when the problem is more difficult or requires multiple stages of calculations",
          "1 have made in this problem set",
          "1 masters most, if not all, of the CNN topics within the problem set",
          "Then, I observed the problem-solving capabilities of the large language model under different inputs"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT-5.1 Pro on HW5",
            "author": "Eric Wang",
            "url": "https://edstem.org/us/courses/84647/discussion/7450396",
            "snippet": "One-shots all of HW5 Q1-4 (non coding) which I was quite impressed by. ..."
          },
          {
            "title": "Special Participation A: Exploration of Different Input Forms on HW5 (ChatGPT 5.1 Auto)",
            "author": "WeiYi Zhang",
            "url": "https://edstem.org/us/courses/84647/discussion/7429448",
            "snippet": "When we use large language models to solve knowledge-based problems, we may encounter the input of images/formulas. Taking hw5 as an example, I tried：\n\n\n1) text input only (without any formulas), \n2) ..."
          },
          {
            "title": "Special Participation A: ChatGPT 5.1 on HW 5",
            "author": "Jiayi Zhang",
            "url": "https://edstem.org/us/courses/84647/discussion/7423443",
            "snippet": "I am using ChatGPT 5.1 to answer the questions in Homework 5. ChatGPT 5.1 seems to be very powerful. I used a simple prompt and pasted the screenshots question by question, part by part, into the chat..."
          }
        ]
      },
      "Unknown": {
        "post_count": 5,
        "top_terms": [
          {
            "term": "one",
            "score": 0.0183
          },
          {
            "term": "deepseek",
            "score": 0.0165
          },
          {
            "term": "correct",
            "score": 0.0147
          },
          {
            "term": "shot",
            "score": 0.0128
          },
          {
            "term": "questions",
            "score": 0.0128
          },
          {
            "term": "problem",
            "score": 0.0128
          },
          {
            "term": "question",
            "score": 0.0128
          },
          {
            "term": "grok",
            "score": 0.011
          },
          {
            "term": "chat",
            "score": 0.011
          },
          {
            "term": "reasoning",
            "score": 0.0092
          },
          {
            "term": "solve",
            "score": 0.0092
          },
          {
            "term": "conceptual",
            "score": 0.0092
          },
          {
            "term": "learning",
            "score": 0.0092
          },
          {
            "term": "its",
            "score": 0.0092
          },
          {
            "term": "calculation",
            "score": 0.0092
          }
        ],
        "strengths": [
          "Accuracy: 9 / 11 one-shot (82 %), 2 / 11 minor-nudge (18 %)\n\nMain errors: one ASCII matrix mis-parse and one BatchNorm vs LayerNorm confusion\n\nNo hallucinations: all final answers matched the official",
          "The report includes:\n\nAn overview of Grok’s performance, where I categorized each task into one of four levels: One-Shot Correct, Minor Misconception, Larger Error, or Did Not Solve",
          "A reflection on Grok’s strengths and weaknesses, where I highlight its strong conceptual intuition, clarity, and adaptability, while noting occasional over-explanation and moments where certain steps ",
          "My recommendations for effectively using Grok as a learning tool, emphasizing the importance of clear prompting, iterative clarification, and maintaining focus on conceptual understanding",
          "Overall, I conclude that Grok performs really well as a conceptual teaching and problem-solving assistant for theoretical deep learning"
        ],
        "weaknesses": [
          "The report includes:\n\nAn overview of Grok’s performance, where I categorized each task into one of four levels: One-Shot Correct, Minor Misconception, Larger Error, or Did Not Solve",
          "Overall, I conclude that Grok performs really well as a conceptual teaching and problem-solving assistant for theoretical deep learning",
          "For the HW5, I try to use Deepseek to solve the problem sets and all the questions and answers are documented in the above files",
          "The strategies I am using are simple, just putting the transcription of the problem to Deepseek and see how it can solve the problems",
          "Deepseek does not support Multi-modal input, therefore when the problems rely on image input, then the model cannot really give a proper answer"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Mistral AI's Le Chat on HW5 Written Portion",
            "author": "Kian Hekmatnejad",
            "url": "https://edstem.org/us/courses/84647/discussion/7382863",
            "snippet": "For Special Participation A, I used Mistral AI's Le Chat on the written portion of HW5. Overall, it performed averagely - mostly arriving at correct answers in one shot, but in a couple of cases requi..."
          },
          {
            "title": "Special Participation A: GPT-Oss on HW5",
            "author": "Noah Lund Syrdal",
            "url": "https://edstem.org/us/courses/84647/discussion/7151370",
            "snippet": "\n\nFor this special participation, I used gpt-oss-120b (Reasoning = High) to solve all non-coding analytical parts of HW5 (Q1–Q4).\nThe model was tested on symbolic derivations and conceptual reasoning ..."
          },
          {
            "title": "Special Participation A: Grok on HW5",
            "author": "Anders Vestrum",
            "url": "https://edstem.org/us/courses/84647/discussion/7148413",
            "snippet": "This document is my report for the HW5 - written part. I evaluate the performance of Grok. It is tested across a series of theoretical deep learning questions covering topics such as convolutional net..."
          }
        ]
      },
      "Claude": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "made",
            "score": 0.0345
          },
          {
            "term": "claude",
            "score": 0.0345
          },
          {
            "term": "walk",
            "score": 0.0345
          },
          {
            "term": "through",
            "score": 0.0345
          },
          {
            "term": "homework",
            "score": 0.0345
          },
          {
            "term": "complex",
            "score": 0.0345
          },
          {
            "term": "script",
            "score": 0.0172
          },
          {
            "term": "documenting",
            "score": 0.0172
          },
          {
            "term": "guided",
            "score": 0.0172
          },
          {
            "term": "acknowledge",
            "score": 0.0172
          },
          {
            "term": "powerful",
            "score": 0.0172
          },
          {
            "term": "tool",
            "score": 0.0172
          },
          {
            "term": "help",
            "score": 0.0172
          },
          {
            "term": "rarely",
            "score": 0.0172
          },
          {
            "term": "see",
            "score": 0.0172
          }
        ],
        "strengths": [
          "Most of the arguments it made are consistent with the right solution especially when the questions are fairly straightforward"
        ],
        "weaknesses": [
          "However, when it is asked to derive something that requires many intermediate steps, it will sometimes fail to recognize the most obvious thing to do at some point"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: HW5 With the Help of Claude AI",
            "author": "Yuxiang Liu",
            "url": "https://edstem.org/us/courses/84647/discussion/7243310",
            "snippet": "Hi, I just made a script documenting how I guided Claude AI to walk through homework 5. I have to acknowledge that Claude AI is a very powerful tool that can help us walk through the homework. I rarel..."
          }
        ]
      },
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0247
          },
          {
            "term": "problem",
            "score": 0.0219
          },
          {
            "term": "solve",
            "score": 0.0164
          },
          {
            "term": "correctly",
            "score": 0.0164
          },
          {
            "term": "convolution",
            "score": 0.0164
          },
          {
            "term": "used",
            "score": 0.0137
          },
          {
            "term": "mathematical",
            "score": 0.0137
          },
          {
            "term": "its",
            "score": 0.0137
          },
          {
            "term": "problems",
            "score": 0.011
          },
          {
            "term": "one",
            "score": 0.011
          },
          {
            "term": "screenshots",
            "score": 0.011
          },
          {
            "term": "even",
            "score": 0.011
          },
          {
            "term": "hint",
            "score": 0.011
          },
          {
            "term": "question",
            "score": 0.011
          },
          {
            "term": "however",
            "score": 0.011
          }
        ],
        "strengths": [
          "Notable observations: \n\nGood parsing of information - Most of the time it correctly parsed all mathematical equations, figures and text from the screenshots, even when a problem was spread across mult",
          "When I pointed this misinterpretation to Gemini, it acknowledged the mistake and solved the problem correctly using substitutions for the system of equations",
          "Incomplete reasoning - The answer to 4b) could have been more complete",
          "However, I think the answer would be complete if it mentioned what should also happen to the $\\Gamma$ matrix (even though this was not explicitly asked in the question) for the equation to achieve the",
          "5 Pro is fully capable of answering all questions correctly with good mathematical and conceptual reasoning"
        ],
        "weaknesses": [
          "I started by mentioning that I wanted to solve a problem set related to a specific topic (in this case, basics of CNNs) and went over the four problems one by one with attached screenshots of the enti",
          "5 Pro was able to extract information included within the problem from the screenshots themselves and one-shot solve almost all the problems correctly without any hallucinations",
          "Notable observations: \n\nGood parsing of information - Most of the time it correctly parsed all mathematical equations, figures and text from the screenshots, even when a problem was spread across mult",
          "This is evident because it often types back key information from the problem statement to formulate the answer",
          "Misinterpretation of a hint - In question 1b), it used the hint to solve the problem itself"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 2.5 Pro on HW5",
            "author": "Kithmini Herath",
            "url": "https://edstem.org/us/courses/84647/discussion/7297480",
            "snippet": "I used Gemini 2.5 Pro to solve the written parts of Homework 5 (Q1-4). I started by mentioning that I wanted to solve a problem set related to a specific topic (in this case, basics of CNNs) and went ..."
          },
          {
            "title": "Special Participation A: Gemini 2.5 Flash on HW 5",
            "author": "Katie Wang",
            "url": "https://edstem.org/us/courses/84647/discussion/7451918",
            "snippet": "I used Gemini 2.5 Flash to solve questions 1, 2, 3, and 4 on HW 5. Gemini performed well overall on the homework problems, giving mostly correct mathematical results and generally clear, step-by-step ..."
          }
        ]
      }
    },
    "HW6": {
      "Unknown": {
        "post_count": 6,
        "top_terms": [
          {
            "term": "answer",
            "score": 0.0177
          },
          {
            "term": "part",
            "score": 0.0116
          },
          {
            "term": "its",
            "score": 0.01
          },
          {
            "term": "graph",
            "score": 0.01
          },
          {
            "term": "correctly",
            "score": 0.01
          },
          {
            "term": "log",
            "score": 0.0085
          },
          {
            "term": "reasoning",
            "score": 0.0085
          },
          {
            "term": "model",
            "score": 0.0069
          },
          {
            "term": "answers",
            "score": 0.0069
          },
          {
            "term": "problem",
            "score": 0.0069
          },
          {
            "term": "questions",
            "score": 0.0062
          },
          {
            "term": "satisfactory",
            "score": 0.0062
          },
          {
            "term": "analysis",
            "score": 0.0054
          },
          {
            "term": "one",
            "score": 0.0054
          },
          {
            "term": "qwen",
            "score": 0.0054
          }
        ],
        "strengths": [
          "Assignments that include filling out information or extracting information from diagrams to complete the assignment are very likely to be incorrect, as the model will hallucinate information",
          "It did well in demonstrating its mathematical analysis, making it clear to figure out any mishap in its derivation",
          "Mistral also did a good job of allowing for an open interaction with the user by making sure to end with, ‘Ready for any further questions or clarification",
          "While reasoning line by line to find the most likely way to move forward, the model also occasionally stops and assesses the most logical thing to do, doing brief sanity checks",
          "The performance was surprisingly good for a open-source model from a company with flagship proprietary models"
        ],
        "weaknesses": [
          "Assignments that include filling out information or extracting information from diagrams to complete the assignment are very likely to be incorrect, as the model will hallucinate information",
          "Here are some highlights:\n\nQwen often uses problem re-stating to find viable solutions to the problem",
          "It establishes the context for the problem by looking at the homework problem, taking into account external sources that are being referenced, and if necessary, feedback, specifically whether to expan",
          "When solving the problem Qwen is able to balance a mixture of possible perspectives, alternately considering the homework author's intent as mentioned in the homework, standard deep learning or linear",
          "Finally, Qwen is unable to read images and so completely hallucinates an answer to question 3) b) (iii)"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: HW6, gpt-oss-120b",
            "author": "Alex Luu",
            "url": "https://edstem.org/us/courses/84647/discussion/7263386",
            "snippet": "I used gpt-oss-120b with thinking on HW 6 non coding questions. The performance was surprisingly good for a open-source model from a company with flagship proprietary models. It was able to one-shot a..."
          },
          {
            "title": "Special Participation A: Kimi on HW6",
            "author": "Moxin Tang",
            "url": "https://edstem.org/us/courses/84647/discussion/7412832",
            "snippet": "Summary of Kimi Performance on HW6\n\nI tested Kimi AI’s ability to solve problems from hw6 focusing on GNN architectures. \n\nOverall, Kimi demonstrated strong reasoning capability across most questions...."
          },
          {
            "title": "Special Participation A: Mistral on HW6",
            "author": "Heidy Hernandez Juan",
            "url": "https://edstem.org/us/courses/84647/discussion/7250482",
            "snippet": "Link: https://chat.mistral.ai/chat/6cd62931-f284-4c4b-9ba3-c5b97943fd28\n\nAnnotated Log: https://drive.google.com/file/d/1mUgllouQGxVA_m5tmlVrq6Dt46HSf0hd/view?usp=sharing\n\nExecutive Summary:\n\nI observ..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "chatgpt",
            "score": 0.0203
          },
          {
            "term": "one",
            "score": 0.0203
          },
          {
            "term": "questions",
            "score": 0.0163
          },
          {
            "term": "model",
            "score": 0.0163
          },
          {
            "term": "homework",
            "score": 0.0122
          },
          {
            "term": "its",
            "score": 0.0122
          },
          {
            "term": "solutions",
            "score": 0.0122
          },
          {
            "term": "question",
            "score": 0.0122
          },
          {
            "term": "subpart",
            "score": 0.0122
          },
          {
            "term": "make",
            "score": 0.0122
          },
          {
            "term": "found",
            "score": 0.0122
          },
          {
            "term": "small",
            "score": 0.0122
          },
          {
            "term": "used",
            "score": 0.0081
          },
          {
            "term": "non",
            "score": 0.0081
          },
          {
            "term": "coding",
            "score": 0.0081
          }
        ],
        "strengths": [
          "Overall, I would say ChatGPT did extremely well on this homework, requiring only a few clarifications, especially for visual aspects",
          "Overall, I would most accurately compare it to a TA in the sense that it generally provides correct reasoning and explanations, but can go on small side tangents, be more long-winded than necessary, a",
          "A link to my detailed annotations can be found here: \n\n\n\nAnd a link to the raw conversation can be found here: https://chatgpt"
        ],
        "weaknesses": [
          "Another interesting limitation was that ChatGPT was unable to parse the image of the graph for 3b",
          "Even after I screenshotted the graph and fed it as input again, it got one of the connections wrong"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT on HW6",
            "author": "Jameson Liu",
            "url": "https://edstem.org/us/courses/84647/discussion/7283953",
            "snippet": "I used ChatGPT (5) on the non-coding parts of homework 6 (#2, #3). I prompted it by attaching the entire homework pdf and asking it to answer them as an expert in deep learning. In its first response,..."
          },
          {
            "title": "Special Participation A: ChatGPT 5.1 Extended Thinking Time on HW 6",
            "author": "Ethan Stone",
            "url": "https://edstem.org/us/courses/84647/discussion/7444253",
            "snippet": "Executive Summary:\n\nI used ChatGPT 5.1 Extended Think to solve HW 6 Questions 2 and 3 (The Non-Coding Questions). To test the advanced reasoning and memory capabilities of the thinking model, I submit..."
          }
        ]
      },
      "Claude": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0462
          },
          {
            "term": "non",
            "score": 0.0231
          },
          {
            "term": "coding",
            "score": 0.0231
          },
          {
            "term": "questions",
            "score": 0.0231
          },
          {
            "term": "answers",
            "score": 0.0231
          },
          {
            "term": "homework",
            "score": 0.0154
          },
          {
            "term": "step",
            "score": 0.0154
          },
          {
            "term": "gnn",
            "score": 0.0154
          },
          {
            "term": "handling",
            "score": 0.0154
          },
          {
            "term": "missing",
            "score": 0.0154
          },
          {
            "term": "accurate",
            "score": 0.0154
          },
          {
            "term": "looked",
            "score": 0.0077
          },
          {
            "term": "well",
            "score": 0.0077
          },
          {
            "term": "solve",
            "score": 0.0077
          },
          {
            "term": "provided",
            "score": 0.0077
          }
        ],
        "strengths": [
          "I looked at how well Claude AI could solve the non-coding questions on Homework 6",
          "For the application/intuition questions (molecular graphs, CNN–GNN analogies, handling missing node values, and scaling/computation of GNNs), Claude’s answers were detailed, on-topic, and made sensibl",
          "I did not observe clear hallucinations or places where Claude invented nonexistent assumptions; when it extended beyond the literal question (e",
          "Overall, Claude’s non-coding HW6 answers show strong reliability on both formal reasoning and conceptual explanation, and would be usable as high-quality solutions or study notes with only light editi"
        ],
        "weaknesses": [
          "For the application/intuition questions (molecular graphs, CNN–GNN analogies, handling missing node values, and scaling/computation of GNNs), Claude’s answers were detailed, on-topic, and made sensibl",
          ", suggesting multiple practical strategies for missing-feature handling), those additions were still consistent with standard GNN practice"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude on HW6",
            "author": "Guohao Lv",
            "url": "https://edstem.org/us/courses/84647/discussion/7412632",
            "snippet": "I looked at how well Claude AI could solve the non-coding questions on Homework 6. I provided Claude with the prompts and context from the HW6 PDF, asking it to work through the problems step-by-step...."
          }
        ]
      },
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "its",
            "score": 0.0321
          },
          {
            "term": "gemini",
            "score": 0.0179
          },
          {
            "term": "questions",
            "score": 0.0179
          },
          {
            "term": "able",
            "score": 0.0179
          },
          {
            "term": "about",
            "score": 0.0179
          },
          {
            "term": "answer",
            "score": 0.0179
          },
          {
            "term": "one",
            "score": 0.0143
          },
          {
            "term": "prompt",
            "score": 0.0143
          },
          {
            "term": "question",
            "score": 0.0143
          },
          {
            "term": "subpart",
            "score": 0.0143
          },
          {
            "term": "correct",
            "score": 0.0143
          },
          {
            "term": "model",
            "score": 0.0143
          },
          {
            "term": "subparts",
            "score": 0.0107
          },
          {
            "term": "problem",
            "score": 0.0107
          },
          {
            "term": "error",
            "score": 0.0107
          }
        ],
        "strengths": [
          "Summary: \n\nGemini performed mostly well, one-shotting most subparts even with minimal guidance beyond the problem setup and accompanying graphs/figures",
          "Overall, this is extremely impressive",
          "Think carefully about each question and answer each subpart with a detailed explanation",
          "” I hoped that this would encourage better results and avoid shallow answers",
          "This subpart took a couple rounds of clarifications and hints to correct"
        ],
        "weaknesses": [
          "Summary: \n\nGemini performed mostly well, one-shotting most subparts even with minimal guidance beyond the problem setup and accompanying graphs/figures",
          "When it made a mistake, it was usually able to reason about its process to identify/clarify the error within a couple of turns",
          "Recap:\n\nI started off by giving it a prefacing prompt: “I will give you problem set questions about deep learning",
          "As a baseline, I fed the entire problem (Q2) into the prompt and attached the relevant figures",
          "I was expecting that given the length and density of the prompt, it would struggle to answer correctly, but surprisingly it was able to one-shot most of the subparts correctly, only being slightly wro"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 3 Pro Thinking on HW 6 Non-Coding",
            "author": "Grant Yang",
            "url": "https://edstem.org/us/courses/84647/discussion/7416689",
            "snippet": "Using Gemini 3 Pro Thinking on HW 6 non-coding questions, I was able to observe the following results.\n\nSummary: \n\nGemini performed mostly well, one-shotting most subparts even with minimal guidance b..."
          },
          {
            "title": "Special Participation A: Gemini on HW6 Non-Coding problems",
            "author": "Arnav Dalal",
            "url": "https://edstem.org/us/courses/84647/discussion/7451705",
            "snippet": "I used Gemini on the HW 6 problems focused on the intuition behind GNNs and their update rules. The model was very good with zero-shot prompting, getting most of the questions right with a few excepti..."
          }
        ]
      },
      "Claude Opus": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0364
          },
          {
            "term": "well",
            "score": 0.0364
          },
          {
            "term": "step",
            "score": 0.0364
          },
          {
            "term": "equations",
            "score": 0.0364
          },
          {
            "term": "wrong",
            "score": 0.0364
          },
          {
            "term": "overall",
            "score": 0.0182
          },
          {
            "term": "performed",
            "score": 0.0182
          },
          {
            "term": "questions",
            "score": 0.0182
          },
          {
            "term": "correct",
            "score": 0.0182
          },
          {
            "term": "strong",
            "score": 0.0182
          },
          {
            "term": "mathematical",
            "score": 0.0182
          },
          {
            "term": "derivations",
            "score": 0.0182
          },
          {
            "term": "path",
            "score": 0.0182
          },
          {
            "term": "counting",
            "score": 0.0182
          },
          {
            "term": "induction",
            "score": 0.0182
          }
        ],
        "strengths": [
          "Overall, Claude performed well, 13/14 questions correct",
          "It was strong at mathematical derivations(path counting induction proof), residual connections, and Newton Schulz convergence analysis",
          "It provided a step by step reasoning which was clean and the explanations were well structured, and easy to follow"
        ],
        "weaknesses": [
          "The one failure was Q3c(iii), where Claude had to write update equations for specific nodes in a graph",
          "Wrong neighbors led to wrong equations"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Testing Claude Opus 4.5 (Extended Thinking) on HW6",
            "author": "Manan Roongta",
            "url": "https://edstem.org/us/courses/84647/discussion/7431312",
            "snippet": "Overall, Claude performed well, 13/14 questions correct. It was strong at mathematical derivations(path counting induction proof), residual connections, and Newton Schulz convergence analysis. It prov..."
          }
        ]
      }
    },
    "HW7": {
      "Claude": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "well",
            "score": 0.025
          },
          {
            "term": "claude",
            "score": 0.025
          },
          {
            "term": "opus",
            "score": 0.025
          },
          {
            "term": "non",
            "score": 0.025
          },
          {
            "term": "coding",
            "score": 0.025
          },
          {
            "term": "one",
            "score": 0.025
          },
          {
            "term": "see",
            "score": 0.025
          },
          {
            "term": "problems",
            "score": 0.0167
          },
          {
            "term": "model",
            "score": 0.0167
          },
          {
            "term": "without",
            "score": 0.0167
          },
          {
            "term": "any",
            "score": 0.0167
          },
          {
            "term": "logs",
            "score": 0.0167
          },
          {
            "term": "stable",
            "score": 0.0167
          },
          {
            "term": "reasoning",
            "score": 0.0167
          },
          {
            "term": "much",
            "score": 0.0167
          }
        ],
        "strengths": [
          "Executive Summary\n\nFor this assignment, I looked at how well Claude Opus can handle the non-coding, conceptual parts of CS282 Homework 7",
          "My main goal was to see (1) how accurate it is, (2) how stable its reasoning is, and (3) how much I need to steer it to get the right answer",
          "Overall, Claude Opus did really well",
          "It got everything right on the first try and produced clean, well-structured derivations without me having to nudge it much",
          "For the four non-coding problems (3b, 4, 7, and 8), it basically one-shot the correct solution every time"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude on HW7",
            "author": "Vongani Maluleke",
            "url": "https://edstem.org/us/courses/84647/discussion/7404515",
            "snippet": "Executive Summary\n\nFor this assignment, I looked at how well Claude Opus can handle the non-coding, conceptual parts of CS282 Homework 7. I went through the problems one by one, interacted with the mo..."
          }
        ]
      },
      "Claude Opus": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "use",
            "score": 0.0526
          },
          {
            "term": "claude",
            "score": 0.0526
          },
          {
            "term": "opus",
            "score": 0.0526
          },
          {
            "term": "extended",
            "score": 0.0526
          },
          {
            "term": "thinking",
            "score": 0.0526
          },
          {
            "term": "solve",
            "score": 0.0526
          },
          {
            "term": "non",
            "score": 0.0526
          },
          {
            "term": "coding",
            "score": 0.0526
          },
          {
            "term": "problems",
            "score": 0.0526
          },
          {
            "term": "homework",
            "score": 0.0526
          },
          {
            "term": "attached",
            "score": 0.0526
          },
          {
            "term": "file",
            "score": 0.0526
          },
          {
            "term": "include",
            "score": 0.0526
          },
          {
            "term": "executive",
            "score": 0.0526
          },
          {
            "term": "summary",
            "score": 0.0526
          }
        ],
        "strengths": [
          "In the attached file, I include the executive summary and annotations of the complete chat logs"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Opus 4.5 (Extended Thinking) on HW7",
            "author": "Sufjan Fana",
            "url": "https://edstem.org/us/courses/84647/discussion/7431425",
            "snippet": "I use Claude Opus 4.5 (Extended Thinking) to solve all of the non-coding problems on homework 7. In the attached file, I include the executive summary and annotations of the complete chat logs.\n\n..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "correct",
            "score": 0.0391
          },
          {
            "term": "verdict",
            "score": 0.0312
          },
          {
            "term": "linear",
            "score": 0.0234
          },
          {
            "term": "chatgpt",
            "score": 0.0156
          },
          {
            "term": "staff",
            "score": 0.0156
          },
          {
            "term": "solutions",
            "score": 0.0156
          },
          {
            "term": "exactly",
            "score": 0.0156
          },
          {
            "term": "correctly",
            "score": 0.0156
          },
          {
            "term": "sos",
            "score": 0.0156
          },
          {
            "term": "dog",
            "score": 0.0156
          },
          {
            "term": "model",
            "score": 0.0156
          },
          {
            "term": "evaluated",
            "score": 0.0078
          },
          {
            "term": "capabilities",
            "score": 0.0078
          },
          {
            "term": "non",
            "score": 0.0078
          },
          {
            "term": "coding",
            "score": 0.0078
          }
        ],
        "strengths": [
          "ChatGPT‑5’s answers are correct",
          "Verdict: correct (with extra but accurate context)",
          "(b) Clear SVD argument that the λ‑regularized optimum favors orthonormal columns in W2​ (minimizing σ2+1/σ2 at σ=1)"
        ],
        "weaknesses": [
          "Per‑problem highlights",
          "(a) Correctly explains why “vertical stacking” is flawed (breaks variable‑length handling and global conditioning)"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT on HW7",
            "author": "Faiaz Khan",
            "url": "https://edstem.org/us/courses/84647/discussion/7246769",
            "snippet": "I evaluated ChatGPT‑5's capabilities for HW7 non‑coding parts: 3(b), 4, 7, 8, using the hw7 questions and staff‑solutions as ground truth.\n\nBottom line. ChatGPT‑5’s answers are correct. \n\nPer‑problem ..."
          }
        ]
      },
      "Unknown": {
        "post_count": 5,
        "top_terms": [
          {
            "term": "model",
            "score": 0.0258
          },
          {
            "term": "questions",
            "score": 0.021
          },
          {
            "term": "question",
            "score": 0.0178
          },
          {
            "term": "grok",
            "score": 0.0113
          },
          {
            "term": "problems",
            "score": 0.0113
          },
          {
            "term": "first",
            "score": 0.0097
          },
          {
            "term": "qwen",
            "score": 0.0097
          },
          {
            "term": "correct",
            "score": 0.0097
          },
          {
            "term": "answer",
            "score": 0.0097
          },
          {
            "term": "multiple",
            "score": 0.0081
          },
          {
            "term": "response",
            "score": 0.0081
          },
          {
            "term": "one",
            "score": 0.0081
          },
          {
            "term": "blog",
            "score": 0.0081
          },
          {
            "term": "used",
            "score": 0.0065
          },
          {
            "term": "written",
            "score": 0.0065
          }
        ],
        "strengths": [
          "I used Grok to complete the written part of HW7",
          "Grok performs very well on the multiple-choice questions, often answering correctly on the first attempt",
          "For the open-ended free response problems, Grok provides reasonable points with clear explanations",
          "Qwen performs well for questions that require basic understanding of classic models like encoder-decoder, multiple choice questions, and blog summary",
          "Context retention: Surprisingly strong—Qwen consistently located the correct question inside the PDF without needing me to restate it"
        ],
        "weaknesses": [
          "I begin by clearly stating Grok’s role and the assistance I require, then provide whole HW7 file to Grok and ask it to understand the problem setups ONLY, so it does not skip ahead to solving the prob",
          "Overall Grok performs descently on the written part of the homework, with great performance on MCQ problems and open-ended free response problems, but difficulty with the proofs",
          "The goal was to see whether Qwen could (1) avoid forgetting earlier context, (2) correctly retrieve the appropriate problem statement from the embedded PDF, and (3) solve each part in a single attempt",
          "In this experiment Qwen successfully accomplished (1), (2) but failed (3) for questions that require SVD math",
          "Hallucinations: None observed\nModel failures: Qwen tends to fail questions with sophisticated mathematical operations like SVD, linear algebra"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: DeepSeek V3.2 on HW7",
            "author": "Neil Pattanaik",
            "url": "https://edstem.org/us/courses/84647/discussion/7419304",
            "snippet": "\n\nConversation link: https://chat.deepseek.com/share/hilftw4hcw8pevn9vy\n\nI used the recently-released DeepSeek V3.2 model (with thinking enabled) to solve the non-coding portion of Homework 7. As expe..."
          },
          {
            "title": "Special Participation A: HW7 with Grok",
            "author": "Ender Ji",
            "url": "https://edstem.org/us/courses/84647/discussion/7250623",
            "snippet": "I used Grok to complete the written part of HW7. I begin by clearly stating Grok’s role and the assistance I require, then provide whole HW7 file to Grok and ask it to understand the problem setups ON..."
          },
          {
            "title": "Special Participation A: Mistral AI on HW7 Written Portion",
            "author": "Tvisha Londhe",
            "url": "https://edstem.org/us/courses/84647/discussion/7452122",
            "snippet": "I used Mistral AI to work through the non-coding portions of HW7, and the results were mixed. While it managed to derive the first-order optimality conditions, it initially gave the final formulas wit..."
          }
        ]
      },
      "Gemini": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0588
          },
          {
            "term": "quite",
            "score": 0.0353
          },
          {
            "term": "math",
            "score": 0.0353
          },
          {
            "term": "official",
            "score": 0.0353
          },
          {
            "term": "pro",
            "score": 0.0235
          },
          {
            "term": "overall",
            "score": 0.0235
          },
          {
            "term": "giving",
            "score": 0.0235
          },
          {
            "term": "solutions",
            "score": 0.0235
          },
          {
            "term": "one",
            "score": 0.0235
          },
          {
            "term": "questions",
            "score": 0.0235
          },
          {
            "term": "different",
            "score": 0.0235
          },
          {
            "term": "solution",
            "score": 0.0235
          },
          {
            "term": "used",
            "score": 0.0118
          },
          {
            "term": "solve",
            "score": 0.0118
          },
          {
            "term": "non",
            "score": 0.0118
          }
        ],
        "strengths": [
          "Overall, Gemini 3 Pro did a quite good job of giving an intuitive explanation for some of the complex math found in the official HW solutions"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 3 Pro on HW 7",
            "author": "Vrushank Prakash",
            "url": "https://edstem.org/us/courses/84647/discussion/7389325",
            "snippet": "I used Gemini 3 Pro to solve the non-coding portion of HW 7, which include 3(b), 4, 7, and 8. Overall, Gemini 3 Pro did a quite good job of giving an intuitive explanation for some of the complex math..."
          }
        ]
      },
      "GPT-4o": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "answer",
            "score": 0.0317
          },
          {
            "term": "special",
            "score": 0.0159
          },
          {
            "term": "participation",
            "score": 0.0159
          },
          {
            "term": "used",
            "score": 0.0159
          },
          {
            "term": "several",
            "score": 0.0159
          },
          {
            "term": "parts",
            "score": 0.0159
          },
          {
            "term": "overall",
            "score": 0.0159
          },
          {
            "term": "helpful",
            "score": 0.0159
          },
          {
            "term": "revealed",
            "score": 0.0159
          },
          {
            "term": "important",
            "score": 0.0159
          },
          {
            "term": "limitations",
            "score": 0.0159
          },
          {
            "term": "excels",
            "score": 0.0159
          },
          {
            "term": "high",
            "score": 0.0159
          },
          {
            "term": "level",
            "score": 0.0159
          },
          {
            "term": "conceptual",
            "score": 0.0159
          }
        ],
        "strengths": [
          "ChatGPT4o excels at high-level conceptual explanations, providing clear insights on autoencoders, PCA, and sequence models",
          "and it’s also fairly strong with mathematical computations"
        ],
        "weaknesses": [
          "A notable issue occurred on Question 7(b), where ChatGPT4o initially gave an incorrect answer about decoder tokens during training and resisted correction when I first pointed out the mistake",
          "This highlighted the need to persistently challenge responses that seem wrong rather than accepting them at face value"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A, ChatGPT-4o on HW7",
            "author": "Kexin Liu",
            "url": "https://edstem.org/us/courses/84647/discussion/7424051",
            "snippet": "For Special Participation A, I used ChatGPT4o on several parts of HW7. Overall, it was helpful but revealed important limitations. ChatGPT4o excels at high-level conceptual explanations, providing cle..."
          }
        ]
      }
    },
    "Unknown": {
      "Unknown": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "chinese",
            "score": 0.0638
          },
          {
            "term": "think",
            "score": 0.0426
          },
          {
            "term": "question",
            "score": 0.0426
          },
          {
            "term": "prepending",
            "score": 0.0213
          },
          {
            "term": "one",
            "score": 0.0213
          },
          {
            "term": "sentence",
            "score": 0.0213
          },
          {
            "term": "model",
            "score": 0.0213
          },
          {
            "term": "reason",
            "score": 0.0213
          },
          {
            "term": "accelerates",
            "score": 0.0213
          },
          {
            "term": "its",
            "score": 0.0213
          },
          {
            "term": "response",
            "score": 0.0213
          },
          {
            "term": "saves",
            "score": 0.0213
          },
          {
            "term": "tokens",
            "score": 0.0213
          },
          {
            "term": "prompt",
            "score": 0.0213
          },
          {
            "term": "please",
            "score": 0.0213
          }
        ],
        "strengths": [],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A -- DeepSeek-v3.2 Overthinks Less in Chinese",
            "author": "Xueli Sun",
            "url": "https://edstem.org/us/courses/84647/discussion/7407541",
            "snippet": "TL;DR: By prepending one Chinese sentence, the model will reason / \"think\" in Chinese, which accelerates its response by 2.5x and saves 2/3 tokens!\n\nThe prompt: 请务必用中文思考，并用英文回答以下问题。 (\"Please make sure..."
          },
          {
            "title": "Special Participation A",
            "author": "Shaurya Jain",
            "url": "https://edstem.org/us/courses/84647/discussion/7451771",
            "snippet": "I have a curiosity-driven question about Deep Learning as a subject and field of human endeavor. \n\nI used GPT 5.1 Thinking on HWK 8 Non-Coding Problems.\n\nAttached below...."
          }
        ]
      }
    },
    "HW09": {
      "Unknown": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "gemma",
            "score": 0.049
          },
          {
            "term": "problems",
            "score": 0.0245
          },
          {
            "term": "correct",
            "score": 0.0175
          },
          {
            "term": "reasoning",
            "score": 0.014
          },
          {
            "term": "one",
            "score": 0.014
          },
          {
            "term": "found",
            "score": 0.014
          },
          {
            "term": "grok",
            "score": 0.0105
          },
          {
            "term": "seemed",
            "score": 0.0105
          },
          {
            "term": "start",
            "score": 0.0105
          },
          {
            "term": "incorrect",
            "score": 0.0105
          },
          {
            "term": "overall",
            "score": 0.0105
          },
          {
            "term": "explanations",
            "score": 0.0105
          },
          {
            "term": "pdf",
            "score": 0.007
          },
          {
            "term": "stuck",
            "score": 0.007
          },
          {
            "term": "move",
            "score": 0.007
          }
        ],
        "strengths": [
          "One thing I noticed was that in questions where there were many short mcq parts, the model seemed to hallucinate on the correct answer if I posted them all at once - it would reason about the correct ",
          "Overall, Gemma performed fairly well (especially given that it is an open-source model that I ran locally on my laptop)",
          "For problems 1–4e, which were largely computation problems, Gemma (mostly) produced correct solutions on the first attempt and consistently demonstrated a strong grasp of the underlying concepts",
          "Gemma's strong explanations (when correct) make it a particularly helpful learning tool, even when it gets some problems wrong",
          "I actually found myself better understanding some of the time and space-complexity arguments around attention mechanisms when trying to guide Gemma to the right solution"
        ],
        "weaknesses": [
          "One thing I noticed was that in questions where there were many short mcq parts, the model seemed to hallucinate on the correct answer if I posted them all at once - it would reason about the correct ",
          "Additionally, I noticed that most times that I corrected Grok, it would start reasoning and start searching the internet and pull up references that were barely relevant to the problem",
          "This led to very long reasoning times and incorrect assumptions about the problem",
          "Gemma's strong explanations (when correct) make it a particularly helpful learning tool, even when it gets some problems wrong",
          "However, even when it produced incorrect bounds or incorrect tensor shapes, it often identified the correct overall strategy, so using Gemma can still be instructive"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Grok 4.1 reasoning on HW09",
            "author": "Rahul Bir",
            "url": "https://edstem.org/us/courses/84647/discussion/7449875",
            "snippet": "For special participation A, I tested Grok 4.1 (beta) with reasoning capabilities on the non-coding question on hw09.\n\nThis is the pdf: \n\nin the pdf, I annotated and noted sections where Grok 4.1 seem..."
          },
          {
            "title": "Special Participation A: Gemma 3 (12b params) on HW09 Written Problems",
            "author": "Etaash Patel",
            "url": "https://edstem.org/us/courses/84647/discussion/7389909",
            "snippet": "Executive Summary:\n\nI worked with Gemma 3 on the written problems for Homework 9 (problems 1, 2, 3, 4, and 6). Overall, Gemma performed fairly well (especially given that it is an open-source model th..."
          }
        ]
      }
    },
    "HW13": {
      "gpt-4o": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gpt",
            "score": 0.0533
          },
          {
            "term": "homework",
            "score": 0.04
          },
          {
            "term": "model",
            "score": 0.0267
          },
          {
            "term": "question",
            "score": 0.0267
          },
          {
            "term": "however",
            "score": 0.0267
          },
          {
            "term": "long",
            "score": 0.0267
          },
          {
            "term": "questions",
            "score": 0.0267
          },
          {
            "term": "images",
            "score": 0.0267
          },
          {
            "term": "tokens",
            "score": 0.0267
          },
          {
            "term": "image",
            "score": 0.0267
          },
          {
            "term": "here",
            "score": 0.0267
          },
          {
            "term": "after",
            "score": 0.0133
          },
          {
            "term": "trying",
            "score": 0.0133
          },
          {
            "term": "use",
            "score": 0.0133
          },
          {
            "term": "solve",
            "score": 0.0133
          }
        ],
        "strengths": [
          "Model: GPT-4o\n\nHomework 13\n\nAfter trying to use GPT-4o to solve homework 13, I was quite surprised how quickly it solved question 1 (with 1 minor mistake which I suspect occurred because it drifted fr"
        ],
        "weaknesses": [
          "Model: GPT-4o\n\nHomework 13\n\nAfter trying to use GPT-4o to solve homework 13, I was quite surprised how quickly it solved question 1 (with 1 minor mistake which I suspect occurred because it drifted fr",
          "However, it really struggled on the second question of the homework which involved very long questions (especially f and g which it couldn’t even get 30% of the way there)",
          "Originally, I suspected some of the error may have to do with the extremely long context length since I passed the questions to the model as images instead of text"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: gpt-4o on HW13 (written)",
            "author": "Jason Lee",
            "url": "https://edstem.org/us/courses/84647/discussion/7452109",
            "snippet": "Model: GPT-4o\n\nHomework 13\n\nAfter trying to use GPT-4o to solve homework 13, I was quite surprised how quickly it solved question 1 (with 1 minor mistake which I suspect occurred because it drifted fr..."
          }
        ]
      },
      "Gemini": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.0261
          },
          {
            "term": "its",
            "score": 0.0261
          },
          {
            "term": "pro",
            "score": 0.0196
          },
          {
            "term": "step",
            "score": 0.0196
          },
          {
            "term": "model",
            "score": 0.0196
          },
          {
            "term": "homework",
            "score": 0.0131
          },
          {
            "term": "well",
            "score": 0.0131
          },
          {
            "term": "theoretical",
            "score": 0.0131
          },
          {
            "term": "derivations",
            "score": 0.0131
          },
          {
            "term": "dpo",
            "score": 0.0131
          },
          {
            "term": "problem",
            "score": 0.0131
          },
          {
            "term": "through",
            "score": 0.0131
          },
          {
            "term": "see",
            "score": 0.0131
          },
          {
            "term": "accuracy",
            "score": 0.0131
          },
          {
            "term": "algebraic",
            "score": 0.0131
          }
        ],
        "strengths": [
          "0 Pro on Homework 13\n\nFor this assignment, I evaluated how well Gemini 3",
          "Overall, Gemini performed exceptionally well",
          "0 Pro on Homework 13\n\nFor this assignment, I evaluated how well Gemini 3",
          "Overall, Gemini performed exceptionally well"
        ],
        "weaknesses": [
          "I approached the Direct Preference Optimization (DPO) problem set step-by-step, initiating the session with a specific persona prompt to establish a \"technical partner\" role, and then guided the model",
          "My main goal was to see (1) its OCR accuracy on dense mathematical problem sets, (2) its ability to perform rigorous algebraic manipulations, and (3) the clarity of its conceptual explanations",
          "I approached the Direct Preference Optimization (DPO) problem set step-by-step, initiating the session with a specific persona prompt to establish a \"technical partner\" role, and then guided the model",
          "My main goal was to see (1) its OCR accuracy on dense mathematical problem sets, (2) its ability to perform rigorous algebraic manipulations, and (3) the clarity of its conceptual explanations"
        ],
        "representative_posts": [
          {
            "title": "[Spoiler Alert] Special Participation A: Gemini 3.0 Pro on Homework 13",
            "author": "Tom Chen",
            "url": "https://edstem.org/us/courses/84647/discussion/7410078",
            "snippet": "Special Participation A: Gemini 3.0 Pro on Homework 13\n\nFor this assignment, I evaluated how well Gemini 3.0 Pro can handle the theoretical, non-coding derivations of CS182 Homework 13. I approached t..."
          },
          {
            "title": "Special Participation A: Gemini 3.0 Pro on Homework 13",
            "author": "Tom Chen",
            "url": "https://edstem.org/us/courses/84647/discussion/7433942",
            "snippet": "Special Participation A: Gemini 3.0 Pro on Homework 13\n\nFor this assignment, I evaluated how well Gemini 3.0 Pro can handle the theoretical, non-coding derivations of CS182 Homework 13. I approached t..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gpt",
            "score": 0.037
          },
          {
            "term": "problems",
            "score": 0.0185
          },
          {
            "term": "parts",
            "score": 0.0185
          },
          {
            "term": "right",
            "score": 0.0185
          },
          {
            "term": "questions",
            "score": 0.0185
          },
          {
            "term": "variables",
            "score": 0.0185
          },
          {
            "term": "through",
            "score": 0.0185
          },
          {
            "term": "circular",
            "score": 0.0185
          },
          {
            "term": "reasoning",
            "score": 0.0185
          },
          {
            "term": "thing",
            "score": 0.0185
          },
          {
            "term": "answer",
            "score": 0.0185
          },
          {
            "term": "sometimes",
            "score": 0.0185
          },
          {
            "term": "wrong",
            "score": 0.0185
          },
          {
            "term": "looking",
            "score": 0.0093
          },
          {
            "term": "attempt",
            "score": 0.0093
          }
        ],
        "strengths": [
          "Looking at GPT's attempt at these two problems, I'd say it got maybe 4 out of 10 parts completely right on the first try",
          "Sometimes that answer is right, sometimes it's totally wrong"
        ],
        "weaknesses": [
          "And further questions on this would easily lead to hallucination and circular reasoning",
          "Sometimes that answer is right, sometimes it's totally wrong",
          "For the DPO problem especially, there were some actual mathematical errors -- not just stylistic issues, but wrong coefficients and circular logic"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT 5.1 thinking on HW13",
            "author": "Jin Ying",
            "url": "https://edstem.org/us/courses/84647/discussion/7418727",
            "snippet": "Looking at GPT's attempt at these two problems, I'd say it got maybe 4 out of 10 parts completely right on the first try. The pattern I noticed is pretty consistent: GPT nails the warm-up questions wh..."
          }
        ]
      },
      "Unknown": {
        "post_count": 2,
        "top_terms": [
          {
            "term": "deepseek",
            "score": 0.0226
          },
          {
            "term": "dpo",
            "score": 0.0188
          },
          {
            "term": "correctly",
            "score": 0.015
          },
          {
            "term": "step",
            "score": 0.015
          },
          {
            "term": "reasoning",
            "score": 0.015
          },
          {
            "term": "questions",
            "score": 0.0113
          },
          {
            "term": "partition",
            "score": 0.0113
          },
          {
            "term": "responses",
            "score": 0.0113
          },
          {
            "term": "derivation",
            "score": 0.0113
          },
          {
            "term": "model",
            "score": 0.0113
          },
          {
            "term": "well",
            "score": 0.0113
          },
          {
            "term": "qwen",
            "score": 0.0075
          },
          {
            "term": "proofs",
            "score": 0.0075
          },
          {
            "term": "diffusion",
            "score": 0.0075
          },
          {
            "term": "models",
            "score": 0.0075
          }
        ],
        "strengths": [
          "The responses included rigorous mathematical detail, proper notation, and logical step-by-step reasoning",
          "Despite minimal input context (only image placeholders uploaded), the model inferred the likely content (standard theoretical ML problems) and provided complete, self-contained solutions, demonstratin",
          "Responses were well-organized, labeled by sub-question, and included boxed final answers where appropriate, enhancing readability and correctness verification",
          "DeepSeek successfully answers almost all questions on the first attempt, providing detailed derivations and correct results throughout",
          "For the DPO derivation, DeepSeek performs especially well"
        ],
        "weaknesses": [
          "However, it tooks 383 seconds to analyze the problem and reasoning"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Qwen on HW13",
            "author": "Peidong Zhang",
            "url": "https://edstem.org/us/courses/84647/discussion/7444212",
            "snippet": "I use Qwen to solve HW13 written part in this special participation A.\n\nQwen's accuracy really impressed me. All questions, including requiring proofs (e.g., induction in diffusion models), derivation..."
          },
          {
            "title": "Special Participation A: Deepseek on HW13 Non-coding",
            "author": "Shuwei Yang",
            "url": "https://edstem.org/us/courses/84647/discussion/7450012",
            "snippet": "I used DeepSeek to answer the non-coding portions of Homework 13. DeepSeek successfully answers almost all questions on the first attempt, providing detailed derivations and correct results throughout..."
          }
        ]
      },
      "Claude": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "claude",
            "score": 0.0545
          },
          {
            "term": "overall",
            "score": 0.0545
          },
          {
            "term": "able",
            "score": 0.0364
          },
          {
            "term": "well",
            "score": 0.0364
          },
          {
            "term": "parts",
            "score": 0.0364
          },
          {
            "term": "derivation",
            "score": 0.0364
          },
          {
            "term": "chat",
            "score": 0.0182
          },
          {
            "term": "used",
            "score": 0.0182
          },
          {
            "term": "sonnet",
            "score": 0.0182
          },
          {
            "term": "answer",
            "score": 0.0182
          },
          {
            "term": "prompt",
            "score": 0.0182
          },
          {
            "term": "included",
            "score": 0.0182
          },
          {
            "term": "pdf",
            "score": 0.0182
          },
          {
            "term": "pretty",
            "score": 0.0182
          },
          {
            "term": "answering",
            "score": 0.0182
          }
        ],
        "strengths": [
          "Overall Claude was able to do pretty well, answering all subproblems fully correctly (in my opinion)",
          "Claude is able to do the first parts of each question very well and provides a very good explanation for each",
          "There are some small parts where it skipped intermediate algebraic derivation steps like in 2b for the Lagrange multipliers but overall the performance was quite strong"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "[SPOILER ALERT] Special Participation A: Claude on HW13",
            "author": "Andy Peng",
            "url": "https://edstem.org/us/courses/84647/discussion/7443651",
            "snippet": "In this chat, I used Claude 4.5 Sonnet to answer HW13. My prompt is included in the pdf. Overall Claude was able to do pretty well, answering all subproblems fully correctly (in my opinion). This was ..."
          }
        ]
      }
    },
    "HW08": {
      "Unknown": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "official",
            "score": 0.0351
          },
          {
            "term": "solutions",
            "score": 0.0351
          },
          {
            "term": "used",
            "score": 0.0175
          },
          {
            "term": "grok",
            "score": 0.0175
          },
          {
            "term": "written",
            "score": 0.0175
          },
          {
            "term": "non",
            "score": 0.0175
          },
          {
            "term": "coding",
            "score": 0.0175
          },
          {
            "term": "problems",
            "score": 0.0175
          },
          {
            "term": "strong",
            "score": 0.0175
          },
          {
            "term": "algebraic",
            "score": 0.0175
          },
          {
            "term": "conceptual",
            "score": 0.0175
          },
          {
            "term": "parts",
            "score": 0.0175
          },
          {
            "term": "ssm",
            "score": 0.0175
          },
          {
            "term": "kernels",
            "score": 0.0175
          },
          {
            "term": "linear",
            "score": 0.0175
          }
        ],
        "strengths": [
          "It was very strong on the algebraic and conceptual parts (SSM kernels, linear purification, ridge attention), usually getting the correct derivations on the first try"
        ],
        "weaknesses": [
          "The main issue I saw was in complexity analysis: in a few places it mixed up total work vs"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Grok on HW 08",
            "author": "Krish Yadav",
            "url": "https://edstem.org/us/courses/84647/discussion/7401078",
            "snippet": "I used Grok on the written (non-coding) problems of HW8. It was very strong on the algebraic and conceptual parts (SSM kernels, linear purification, ridge attention), usually getting the correct deriv..."
          }
        ]
      },
      "ChatGPT": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "solutions",
            "score": 0.0282
          },
          {
            "term": "chatgpt",
            "score": 0.0282
          },
          {
            "term": "give",
            "score": 0.0226
          },
          {
            "term": "model",
            "score": 0.0169
          },
          {
            "term": "subparts",
            "score": 0.0169
          },
          {
            "term": "staff",
            "score": 0.0169
          },
          {
            "term": "solution",
            "score": 0.0169
          },
          {
            "term": "correct",
            "score": 0.0169
          },
          {
            "term": "really",
            "score": 0.0169
          },
          {
            "term": "there",
            "score": 0.0113
          },
          {
            "term": "said",
            "score": 0.0113
          },
          {
            "term": "full",
            "score": 0.0113
          },
          {
            "term": "academic",
            "score": 0.0113
          },
          {
            "term": "guardrails",
            "score": 0.0113
          },
          {
            "term": "proceeded",
            "score": 0.0113
          }
        ],
        "strengths": [
          "It was most misleading on problem 1c) where it incorrectly suggested an FFT based convolution for critical path when the correct (and more efficient) solution uses direct conv with parallel matrix ope",
          "While ChatGPT got incorrect results for time complexity analysis for question 1, it gave a fully correct (with correct and thorough steps & intuition) solution for 4c)"
        ],
        "weaknesses": [
          "These guardrails must be quite weak, or at least the model doesn't understand academic honesty, because the model proceeded to give full mathematical derivations of every subpart of the homework",
          "It was most misleading on problem 1c) where it incorrectly suggested an FFT based convolution for critical path when the correct (and more efficient) solution uses direct conv with parallel matrix ope",
          "This threw off the logic and resulted in incorrect solutions for the following subparts",
          "While ChatGPT got incorrect results for time complexity analysis for question 1, it gave a fully correct (with correct and thorough steps & intuition) solution for 4c)",
          "See this annotated conversation for more problem specific details:"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: ChatGPT 5.1 Thinking on HW08",
            "author": "Sammie Smith",
            "url": "https://edstem.org/us/courses/84647/discussion/7409308",
            "snippet": "Hi there,\n\nI asked ChatGPT5.1 Thinking model to do HW08. Interestingly, it said that it could not give me full solutions due to OpenAI's academic integrity guardrails. These guardrails must be quite w..."
          }
        ]
      },
      "Gemini": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "part",
            "score": 0.0658
          },
          {
            "term": "mathbf",
            "score": 0.0329
          },
          {
            "term": "correct",
            "score": 0.0288
          },
          {
            "term": "gemini",
            "score": 0.0206
          },
          {
            "term": "struggled",
            "score": 0.0206
          },
          {
            "term": "conceptual",
            "score": 0.0165
          },
          {
            "term": "question",
            "score": 0.0165
          },
          {
            "term": "path",
            "score": 0.0165
          },
          {
            "term": "matrix",
            "score": 0.0165
          },
          {
            "term": "first",
            "score": 0.0123
          },
          {
            "term": "problem",
            "score": 0.0123
          },
          {
            "term": "solution",
            "score": 0.0123
          },
          {
            "term": "key",
            "score": 0.0123
          },
          {
            "term": "correctly",
            "score": 0.0123
          },
          {
            "term": "optimal",
            "score": 0.0123
          }
        ],
        "strengths": [
          "I first solved Questions 1, 3 and 4 based solely on the problem description in the uploaded PDF",
          "Here are the per-question results on the first run through:\n\nQuestion 1\n\nCorrect The derivation of the kernel $K$ (part a), the concrete examples (part b), the critical path comparison (part c), and t",
          "Struggled In part (d), while the correct highly parallel method and the $\\mathbf{O(\\log L)}$ dependency on sequence length were correctly identified, the final critical path expression was $\\mathbf{O(",
          "Question 3\n\nCorrect\n\nThe mathematical derivation for the optimal weight matrix $\\hat{W}$ in part (b) ii was correct and followed the most elegant geometric invariance approach shown in the key",
          "The determined range for $\\lambda$ in part (c), $\\mathbf{1 \\le \\lambda \\le 4}$, was also correct based on the $\\mathbf{80\\%}$ preservation and $\\mathbf{50\\%}$ attenuation requirements specified in the"
        ],
        "weaknesses": [
          "I first solved Questions 1, 3 and 4 based solely on the problem description in the uploaded PDF",
          "However, it still struggled a bit in a few of the problem portions",
          "Struggled In part (d), while the correct highly parallel method and the $\\mathbf{O(\\log L)}$ dependency on sequence length were correctly identified, the final critical path expression was $\\mathbf{O(",
          "The determined range for $\\lambda$ in part (c), $\\mathbf{1 \\le \\lambda \\le 4}$, was also correct based on the $\\mathbf{80\\%}$ preservation and $\\mathbf{50\\%}$ attenuation requirements specified in the",
          "Struggled\n\nIn part (a) Gemini, was unable to fill in the missing Regularization Loss for encoder $\\mathbf{W^{(\\beta)}}$"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 2.5 Fast on Homework 08",
            "author": "Mishty Dhekial",
            "url": "https://edstem.org/us/courses/84647/discussion/7417556",
            "snippet": "I utilized the Gemini 2.5 Fast model to tackle the non-coding problems of Homework 8. I first solved Questions 1, 3 and 4 based solely on the problem description in the uploaded PDF. I then used the p..."
          }
        ]
      },
      "Claude": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "correct",
            "score": 0.0566
          },
          {
            "term": "mostly",
            "score": 0.0377
          },
          {
            "term": "one",
            "score": 0.0377
          },
          {
            "term": "shot",
            "score": 0.0377
          },
          {
            "term": "fully",
            "score": 0.0377
          },
          {
            "term": "summary",
            "score": 0.0189
          },
          {
            "term": "claude",
            "score": 0.0189
          },
          {
            "term": "opus",
            "score": 0.0189
          },
          {
            "term": "thinking",
            "score": 0.0189
          },
          {
            "term": "able",
            "score": 0.0189
          },
          {
            "term": "questions",
            "score": 0.0189
          },
          {
            "term": "however",
            "score": 0.0189
          },
          {
            "term": "interestingly",
            "score": 0.0189
          },
          {
            "term": "got",
            "score": 0.0189
          },
          {
            "term": "bit",
            "score": 0.0189
          }
        ],
        "strengths": [
          "Even with significant guidance, it kept adding in terms that weren't really necessary, and I basically needed to fully guide it to the correct answer",
          "Oddly, it did the difficult part of the question correct, which was figuring out the critical path length",
          "All other answers it gave were mostly or fully correct in one-shot"
        ],
        "weaknesses": [
          "However, interestingly, it got a bit stuck/potentially overthought on problem 1c)"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude 4.5 Opus (Extended Thinking) on HW 08",
            "author": "Atharv Sampath",
            "url": "https://edstem.org/us/courses/84647/discussion/7450685",
            "snippet": "Summary: Claude Opus 4.5 with thinking was able to mostly one-shot all of the questions. However, interestingly, it got a bit stuck/potentially overthought on problem 1c). Even with significant guidan..."
          }
        ]
      }
    },
    "HW05": {
      "Claude Opus": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "part",
            "score": 0.0474
          },
          {
            "term": "correct",
            "score": 0.0263
          },
          {
            "term": "correctly",
            "score": 0.0263
          },
          {
            "term": "batch",
            "score": 0.0211
          },
          {
            "term": "dropout",
            "score": 0.0211
          },
          {
            "term": "question",
            "score": 0.0211
          },
          {
            "term": "norm",
            "score": 0.0211
          },
          {
            "term": "claude",
            "score": 0.0158
          },
          {
            "term": "normalization",
            "score": 0.0158
          },
          {
            "term": "depthwise",
            "score": 0.0158
          },
          {
            "term": "separable",
            "score": 0.0158
          },
          {
            "term": "regularization",
            "score": 0.0158
          },
          {
            "term": "questions",
            "score": 0.0158
          },
          {
            "term": "one",
            "score": 0.0158
          },
          {
            "term": "convolution",
            "score": 0.0158
          }
        ],
        "strengths": [
          "5 achieved a 100% one-shot success rate across all 11 sub-questions with no corrective prompting needed",
          "Question 1: Convolutional Networks - All correct",
          "Part (b) solved the linear system and self-verified the filter [2, -1, 3]",
          "Part (c) computed the 2D transpose convolution output correctly with clear bookkeeping",
          "Question 2: Batch Normalization - All correct"
        ],
        "weaknesses": [],
        "representative_posts": [
          {
            "title": "Special Participation A: Claude Opus 4.5 on HW05 (Written Questions)",
            "author": "Rishi Thakar",
            "url": "https://edstem.org/us/courses/84647/discussion/7424254",
            "snippet": "I used Claude Opus 4.5 Thinking to solve the written portions of Homework 5, covering convolutional networks, batch normalization, depthwise separable convolutions, and dropout as regularization. Ques..."
          }
        ]
      }
    },
    "HW02": {
      "Unknown": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "problem",
            "score": 0.0508
          },
          {
            "term": "max",
            "score": 0.0339
          },
          {
            "term": "solve",
            "score": 0.0339
          },
          {
            "term": "math",
            "score": 0.0339
          },
          {
            "term": "problems",
            "score": 0.0339
          },
          {
            "term": "able",
            "score": 0.0339
          },
          {
            "term": "correctly",
            "score": 0.0339
          },
          {
            "term": "one",
            "score": 0.0339
          },
          {
            "term": "question",
            "score": 0.0339
          },
          {
            "term": "homework",
            "score": 0.0339
          },
          {
            "term": "correction",
            "score": 0.0339
          },
          {
            "term": "its",
            "score": 0.0339
          },
          {
            "term": "used",
            "score": 0.0169
          },
          {
            "term": "shot",
            "score": 0.0169
          },
          {
            "term": "three",
            "score": 0.0169
          }
        ],
        "strengths": [],
        "weaknesses": [
          "At first, I provided just the homework pdf without the stated correction; this caused the model to have significantly more trouble with problem 1(b), but after providing the correction it was able to ",
          "Additionally, there was only one minor hallucination, related to the \"Important Note\" that it provides in its solution to Problem 1 (b); which did not affect its ability to correctly answer the questi"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Qwen3-Max on HW02",
            "author": "Cameron Jordan",
            "url": "https://edstem.org/us/courses/84647/discussion/7423915",
            "snippet": "I used Qwen3-Max to solve the math problems on HW02 (Problems 1, 2, and 5). Qwen3-Max was able to correctly one-shot all three math question on this homework. \n\nAt first, I provided just the homework ..."
          }
        ]
      }
    },
    "HW03": {
      "Gemini": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "pro",
            "score": 0.0171
          },
          {
            "term": "parts",
            "score": 0.0171
          },
          {
            "term": "problem",
            "score": 0.0171
          },
          {
            "term": "gemini",
            "score": 0.0114
          },
          {
            "term": "gave",
            "score": 0.0114
          },
          {
            "term": "often",
            "score": 0.0114
          },
          {
            "term": "explicit",
            "score": 0.0114
          },
          {
            "term": "steps",
            "score": 0.0114
          },
          {
            "term": "questions",
            "score": 0.0114
          },
          {
            "term": "paper",
            "score": 0.0114
          },
          {
            "term": "once",
            "score": 0.0114
          },
          {
            "term": "generic",
            "score": 0.0114
          },
          {
            "term": "wrong",
            "score": 0.0114
          },
          {
            "term": "forward",
            "score": 0.0114
          },
          {
            "term": "explicitly",
            "score": 0.0114
          }
        ],
        "strengths": [
          "Most of the probability / calculus / optimization pieces came out correct on the first attempt; my role was mainly to sanity-check the algebra and occasionally nudge it to be more explicit about inter",
          "It handled the Gaussian policy-gradient and Maximal Update Parameterization questions well, with clean use of the log-derivative trick, reparameterization, and scaling arguments",
          ", interpreting figures/tables from the muP paper), once I pointed it to the specific figure or row, it gave focused, accurate summaries instead of generic “paper reviews",
          "Explanations are concise and formula-heavy; this is good for following the math, but it often skips intuition or broader context unless explicitly requested"
        ],
        "weaknesses": [
          "I gave it the full problem statements (often as screenshots) and asked it to work through each sub-question with explicit derivation steps, but no code",
          "”\n\nWhen my prompt was ambiguous or I referenced the wrong part of a problem, it tended not to invent details; it either stayed generic or adjusted once I clarified, which kept hallucinations relativel",
          "Weaknesses / failure modes",
          "For implementation-style questions (tensor rematerialization forward counts), it initially chose a reasonable but wrong cost model and confidently overcounted",
          "Overall, Gemini 3 Pro was quite usable as a step-by-step assistant for the theory parts of the homework, but still needed a human in the loop to pin down problem semantics and verify that its interpre"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini 3 Pro on HW03",
            "author": "John Wang",
            "url": "https://edstem.org/us/courses/84647/discussion/7429651",
            "snippet": "For Participation A I used Gemini 3 Pro (Thinking with 3 Pro) on the non-coding parts of HW3 (Problems 1, 3, 4, 5). I gave it the full problem statements (often as screenshots) and asked it to work th..."
          }
        ]
      }
    },
    "HW06": {
      "Gemini": {
        "post_count": 1,
        "top_terms": [
          {
            "term": "gemini",
            "score": 0.022
          },
          {
            "term": "pdf",
            "score": 0.014
          },
          {
            "term": "text",
            "score": 0.012
          },
          {
            "term": "script",
            "score": 0.012
          },
          {
            "term": "question",
            "score": 0.01
          },
          {
            "term": "hallucination",
            "score": 0.01
          },
          {
            "term": "prompt",
            "score": 0.01
          },
          {
            "term": "table",
            "score": 0.01
          },
          {
            "term": "its",
            "score": 0.01
          },
          {
            "term": "path",
            "score": 0.008
          },
          {
            "term": "graph",
            "score": 0.008
          },
          {
            "term": "high",
            "score": 0.008
          },
          {
            "term": "standard",
            "score": 0.008
          },
          {
            "term": "based",
            "score": 0.008
          },
          {
            "term": "hallucinations",
            "score": 0.008
          }
        ],
        "strengths": [
          "Here's the PDF summarizing our interaction:\n\nOverview of Performance\n\nGemini acted as a \"Teaching Assistant/Technical Solver\" under well-defined rules I designed to reduce hallucination and maximize p",
          "The model successfully solved 100% of the non-coding questions, 9/13 in the first attempt, and the remaining 4/13 on the second attempt after additional guidance or clarification from me",
          "Gemini demonstrated strong reasoning capabilities in mathematical proofs (GNN path counting) and theoretical analysis (Muon optimizer scaling), but required additional prompting for visual interpretat",
          "Outcomes\n\nOne-Shot Success Rate: ~70%\n\nHigh success: Mathematical derivations, standard Deep Learning theory (memory calculations, optimizer comparisons), text-based conceptual questions",
          "Lower success: Questions requiring visual extraction from the PDF (e"
        ],
        "weaknesses": [
          "Here's the PDF summarizing our interaction:\n\nOverview of Performance\n\nGemini acted as a \"Teaching Assistant/Technical Solver\" under well-defined rules I designed to reduce hallucination and maximize p",
          "Interestingly, when solving question 3ciii, the model did not hallucinate graph edges when they were missing from the text extraction; instead, it paused and requested me to describe the topology",
          "I believe it did this because I said it could ask me for clarification if it was confused about anything in my initial prompt",
          "Neat Observation: Gemini Meta-cognition\n\nGemini surprisingly took the initiative to write a Python script to help it read a table in the homework PDF when its built-in PDF processing subroutine failed",
          "It explained that my \"Hallucination Check\" and \"Precision\" rules provided it motivation to develop and run this script to help it solve the errors it detected"
        ],
        "representative_posts": [
          {
            "title": "Special Participation A: Gemini (Thinking With Pro 3) on HW06",
            "author": "Nicolas Rault-Wang",
            "url": "https://edstem.org/us/courses/84647/discussion/7357397",
            "snippet": "I used Gemini (Thinking with Pro 3) to solve every non-coding question of homework 6.\n\nHere's the PDF summarizing our interaction:\n\nOverview of Performance\n\nGemini acted as a \"Teaching Assistant/Techn..."
          }
        ]
      }
    }
  },
  "heatmap": {
    "homeworks": [
      "HW0",
      "HW02",
      "HW03",
      "HW05",
      "HW06",
      "HW07",
      "HW08",
      "HW09",
      "HW1",
      "HW10",
      "HW11",
      "HW12",
      "HW13",
      "HW2",
      "HW3",
      "HW4",
      "HW5",
      "HW6",
      "HW7",
      "HW8",
      "HW9",
      "Unknown"
    ],
    "models": [
      "ChatGPT",
      "Claude",
      "Claude Opus",
      "Claude Sonnet",
      "GPT-4o",
      "Gemini",
      "Gemini Flash",
      "Gemini Pro",
      "Gemini-Pro",
      "Llama 4",
      "Unknown",
      "gpt-4o"
    ],
    "matrix": {
      "HW0": {
        "ChatGPT": 1,
        "Claude": 0,
        "Claude Opus": 1,
        "Claude Sonnet": 1,
        "GPT-4o": 1,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 8,
        "gpt-4o": 0
      },
      "HW02": {
        "ChatGPT": 0,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 0,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 1,
        "gpt-4o": 0
      },
      "HW03": {
        "ChatGPT": 0,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 1,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 0,
        "gpt-4o": 0
      },
      "HW05": {
        "ChatGPT": 0,
        "Claude": 0,
        "Claude Opus": 1,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 0,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 0,
        "gpt-4o": 0
      },
      "HW06": {
        "ChatGPT": 0,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 1,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 0,
        "gpt-4o": 0
      },
      "HW07": {
        "ChatGPT": 1,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 0,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 2,
        "gpt-4o": 0
      },
      "HW08": {
        "ChatGPT": 1,
        "Claude": 1,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 1,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 1,
        "gpt-4o": 0
      },
      "HW09": {
        "ChatGPT": 0,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 0,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 2,
        "gpt-4o": 0
      },
      "HW1": {
        "ChatGPT": 2,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 1,
        "GPT-4o": 0,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 1,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 5,
        "gpt-4o": 0
      },
      "HW10": {
        "ChatGPT": 2,
        "Claude": 0,
        "Claude Opus": 1,
        "Claude Sonnet": 1,
        "GPT-4o": 1,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 1,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 5,
        "gpt-4o": 0
      },
      "HW11": {
        "ChatGPT": 0,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 0,
        "Gemini Flash": 1,
        "Gemini Pro": 1,
        "Gemini-Pro": 0,
        "Llama 4": 1,
        "Unknown": 7,
        "gpt-4o": 0
      },
      "HW12": {
        "ChatGPT": 1,
        "Claude": 1,
        "Claude Opus": 1,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 1,
        "Gemini Flash": 1,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 5,
        "gpt-4o": 0
      },
      "HW13": {
        "ChatGPT": 1,
        "Claude": 1,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 2,
        "gpt-4o": 1
      },
      "HW2": {
        "ChatGPT": 1,
        "Claude": 1,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 2,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 4,
        "gpt-4o": 0
      },
      "HW3": {
        "ChatGPT": 1,
        "Claude": 0,
        "Claude Opus": 1,
        "Claude Sonnet": 1,
        "GPT-4o": 1,
        "Gemini": 1,
        "Gemini Flash": 0,
        "Gemini Pro": 1,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 4,
        "gpt-4o": 0
      },
      "HW4": {
        "ChatGPT": 2,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 1,
        "GPT-4o": 0,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 5,
        "gpt-4o": 0
      },
      "HW5": {
        "ChatGPT": 4,
        "Claude": 1,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 5,
        "gpt-4o": 0
      },
      "HW6": {
        "ChatGPT": 2,
        "Claude": 1,
        "Claude Opus": 1,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 6,
        "gpt-4o": 0
      },
      "HW7": {
        "ChatGPT": 1,
        "Claude": 1,
        "Claude Opus": 1,
        "Claude Sonnet": 0,
        "GPT-4o": 1,
        "Gemini": 1,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 5,
        "gpt-4o": 0
      },
      "HW8": {
        "ChatGPT": 2,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 1,
        "GPT-4o": 0,
        "Gemini": 2,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 6,
        "gpt-4o": 0
      },
      "HW9": {
        "ChatGPT": 2,
        "Claude": 0,
        "Claude Opus": 1,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 1,
        "Gemini Flash": 0,
        "Gemini Pro": 1,
        "Gemini-Pro": 1,
        "Llama 4": 0,
        "Unknown": 8,
        "gpt-4o": 0
      },
      "Unknown": {
        "ChatGPT": 0,
        "Claude": 0,
        "Claude Opus": 0,
        "Claude Sonnet": 0,
        "GPT-4o": 0,
        "Gemini": 0,
        "Gemini Flash": 0,
        "Gemini Pro": 0,
        "Gemini-Pro": 0,
        "Llama 4": 0,
        "Unknown": 2,
        "gpt-4o": 0
      }
    }
  },
  "timeline": {
    "dates": [
      "2025-10-01",
      "2025-10-05",
      "2025-10-07",
      "2025-10-09",
      "2025-10-11",
      "2025-10-15",
      "2025-10-18",
      "2025-10-20",
      "2025-10-26",
      "2025-10-28",
      "2025-10-29",
      "2025-10-30",
      "2025-11-03",
      "2025-11-04",
      "2025-11-05",
      "2025-11-06",
      "2025-11-07",
      "2025-11-10",
      "2025-11-12",
      "2025-11-13",
      "2025-11-14",
      "2025-11-16",
      "2025-11-17",
      "2025-11-19",
      "2025-11-23",
      "2025-11-24",
      "2025-11-27",
      "2025-11-28",
      "2025-11-29",
      "2025-11-30",
      "2025-12-01",
      "2025-12-02",
      "2025-12-03",
      "2025-12-04",
      "2025-12-05",
      "2025-12-06",
      "2025-12-07",
      "2025-12-08",
      "2025-12-09",
      "2025-12-10",
      "2025-12-11"
    ],
    "series": {
      "HW0": [
        0,
        1,
        1,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        2,
        1,
        1,
        0,
        0,
        0,
        4
      ],
      "HW02": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0
      ],
      "HW03": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0
      ],
      "HW05": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0
      ],
      "HW06": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0
      ],
      "HW07": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        3,
        0,
        0,
        0
      ],
      "HW08": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        1,
        1,
        0,
        0,
        0,
        0,
        1
      ],
      "HW09": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1
      ],
      "HW1": [
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        3,
        0,
        0,
        4
      ],
      "HW10": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        4,
        0,
        0,
        2,
        3,
        1,
        0,
        3
      ],
      "HW11": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        1,
        2,
        0,
        0,
        3,
        0,
        2,
        1
      ],
      "HW12": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        1,
        0,
        1,
        2,
        1,
        0,
        2,
        2
      ],
      "HW13": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        1,
        0,
        0,
        1,
        2,
        2
      ],
      "HW2": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        1,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        2,
        0,
        1,
        0,
        1,
        1,
        0,
        0,
        1
      ],
      "HW3": [
        1,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        2,
        0,
        0,
        1
      ],
      "HW4": [
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        2,
        0,
        1,
        0,
        2,
        0,
        1,
        1
      ],
      "HW5": [
        0,
        0,
        0,
        0,
        0,
        0,
        2,
        0,
        1,
        1,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        1,
        1,
        0,
        0,
        2
      ],
      "HW6": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        1,
        0,
        1,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        2,
        1,
        0,
        2,
        0,
        2,
        1
      ],
      "HW7": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        1,
        1,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        1,
        0,
        1,
        0,
        1,
        1,
        1,
        0,
        0,
        1
      ],
      "HW8": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        1,
        1,
        0,
        0,
        0,
        0,
        1,
        2,
        1,
        0,
        0,
        2,
        0,
        0,
        2
      ],
      "HW9": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        1,
        2,
        0,
        0,
        1,
        1,
        0,
        1,
        4,
        0,
        0,
        0,
        2
      ],
      "Unknown": [
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        0,
        1,
        0,
        0,
        0,
        0,
        0,
        1
      ]
    }
  },
  "statistics": {
    "total_posts": 169,
    "total_homeworks": 22,
    "total_models": 12,
    "total_combinations": 86,
    "global_top_terms": [
      {
        "term": "model",
        "frequency": 0.0152
      },
      {
        "term": "question",
        "frequency": 0.0131
      },
      {
        "term": "questions",
        "frequency": 0.0108
      },
      {
        "term": "problem",
        "frequency": 0.0103
      },
      {
        "term": "one",
        "frequency": 0.0103
      },
      {
        "term": "correct",
        "frequency": 0.0095
      },
      {
        "term": "answer",
        "frequency": 0.0093
      },
      {
        "term": "its",
        "frequency": 0.0089
      },
      {
        "term": "problems",
        "frequency": 0.0067
      },
      {
        "term": "homework",
        "frequency": 0.0065
      },
      {
        "term": "gemini",
        "frequency": 0.0065
      },
      {
        "term": "reasoning",
        "frequency": 0.0063
      },
      {
        "term": "part",
        "frequency": 0.0057
      },
      {
        "term": "correctly",
        "frequency": 0.0056
      },
      {
        "term": "able",
        "frequency": 0.0052
      },
      {
        "term": "answers",
        "frequency": 0.0052
      },
      {
        "term": "shot",
        "frequency": 0.0051
      },
      {
        "term": "solution",
        "frequency": 0.005
      },
      {
        "term": "step",
        "frequency": 0.0049
      },
      {
        "term": "used",
        "frequency": 0.0047
      }
    ]
  }
}